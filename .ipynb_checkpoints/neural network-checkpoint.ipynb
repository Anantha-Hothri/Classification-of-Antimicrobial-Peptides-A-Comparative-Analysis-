{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5993f04c-f821-4b57-93b9-6ff0e68f8b6c",
   "metadata": {},
   "source": [
    "**PhysicoChemical properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22478c9f-e14f-4ae9-924e-c76c877e19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start calculating properties for posDF and negDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 921/921 [01:29<00:00, 10.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1645/1645 [02:02<00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculating properties for posDF and negDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from lime import lime_tabular\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "posDF = pd.read_csv(r\"C:\\Users\\advik\\Downloads\\bio project\\PositiveSequences.csv\")\n",
    "allDataDF = pd.read_csv(r'C:\\Users\\advik\\Downloads\\bio project\\nlp_data.csv')\n",
    "\n",
    "# Process data for Method 1 (Physicochemical Properties)\n",
    "allDataDF = allDataDF.set_index(allDataDF['DADP ID'])\n",
    "posDF = posDF.set_index(posDF['DADP ID'])\n",
    "posID_List = posDF[\"DADP ID\"].tolist()\n",
    "negDF = allDataDF.drop(posID_List)\n",
    "\n",
    "invalid_chars = \"/\"\n",
    "negDF = negDF[~negDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "posDF = posDF[~posDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "\n",
    "def calculate_physicochemical_properties(sequence):\n",
    "    analyzed_seq = ProteinAnalysis(sequence)\n",
    "    properties = {\n",
    "        'MolecularWeight': analyzed_seq.molecular_weight(),\n",
    "        'IsoelectricPoint': analyzed_seq.isoelectric_point(),\n",
    "        'Aromaticity': analyzed_seq.aromaticity(),\n",
    "        'InstabilityIndex': analyzed_seq.instability_index(),\n",
    "        'Hydrophobicity': analyzed_seq.gravy(),\n",
    "        'ChargeAtPH7': analyzed_seq.charge_at_pH(7.0)\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "def calculate_rdkit_descriptors(sequence):\n",
    "    mol = Chem.MolFromSequence(sequence)\n",
    "    properties = {\n",
    "        'MolWt': Descriptors.MolWt(mol),\n",
    "        'LogP': Descriptors.MolLogP(mol),\n",
    "        'TPSA': Descriptors.TPSA(mol),\n",
    "        'NumHDonors': Descriptors.NumHDonors(mol),\n",
    "        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "        'BalabanJ': Descriptors.BalabanJ(mol),\n",
    "        'BertzCT': Descriptors.BertzCT(mol),\n",
    "        'Chi0v': Descriptors.Chi0v(mol),\n",
    "        'Chi1v': Descriptors.Chi1v(mol),\n",
    "        'HallKierAlpha': Descriptors.HallKierAlpha(mol),\n",
    "        'Kappa1': Descriptors.Kappa1(mol),\n",
    "        'Kappa2': Descriptors.Kappa2(mol),\n",
    "        'Kappa3': Descriptors.Kappa3(mol)\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "def calculate_all_properties(sequence):\n",
    "    physico_props = calculate_physicochemical_properties(sequence)\n",
    "    rdkit_props = calculate_rdkit_descriptors(sequence)\n",
    "    all_props = {**physico_props, **rdkit_props}\n",
    "    return all_props\n",
    "print(\"Start calculating properties for posDF and negDF\")\n",
    "posDF['Properties'] = posDF['Bioactive sequence'].progress_apply(calculate_all_properties)\n",
    "negDF['Properties'] = negDF['Bioactive sequence'].progress_apply(calculate_all_properties)\n",
    "print(\"Finished calculating properties for posDF and negDF\")\n",
    "pos_properties = pd.DataFrame(posDF['Properties'].tolist(), index=posDF.index)\n",
    "neg_properties = pd.DataFrame(negDF['Properties'].tolist(), index=negDF.index)\n",
    "pos_properties['Value'] = 1\n",
    "neg_properties['Value'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec604f2a-a67a-46d8-aaad-866f58626e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319   \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875   \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542   \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154   \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848   \n",
       "...               ...            ...         ...         ...         ...   \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037   \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541   \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362   \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276   \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679   \n",
       "\n",
       "           Value  \n",
       "DADP ID           \n",
       "SP_P31107      1  \n",
       "SP_2643        1  \n",
       "SP_2644        1  \n",
       "SP_2645        1  \n",
       "SP_2646        1  \n",
       "...          ...  \n",
       "SP_2852        0  \n",
       "SP_2853        0  \n",
       "SP_2854        0  \n",
       "SP_2855        0  \n",
       "SP_Q09022      0  \n",
       "\n",
       "[2566 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n",
      "Confusion Matrix:\n",
      "[[280  60]\n",
      " [ 74 100]]\n",
      "Accuracy: 0.7393\n",
      "Precision: 0.6250\n",
      "Recall: 0.5747\n",
      "F1 Score: 0.5988\n",
      "ROC AUC Score: 0.7836\n",
      "Performance Metrics:\n",
      "       Metric     Value\n",
      "0   Accuracy  0.739300\n",
      "1  Precision  0.625000\n",
      "2     Recall  0.574713\n",
      "3   F1 Score  0.598802\n",
      "4    ROC AUC  0.783595\n",
      "\n",
      "Confusion Matrix:\n",
      " [[280  60]\n",
      " [ 74 100]]\n",
      "\n",
      "Predicting using Deep Learning Model:\n",
      "Sequence: MFTLKKSMLLLFFLGTISLSLC\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Prediction: Antimicrobial\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "all_properties = pd.concat([pos_properties, neg_properties])\n",
    "display(all_properties)\n",
    "allDataDF2=all_properties\n",
    "\n",
    "\n",
    "# Prepare data for Method 1\n",
    "X1 = all_properties.drop('Value', axis=1)\n",
    "y1 = all_properties['Value']\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1_train = scaler.fit_transform(X1_train)\n",
    "X1_test = scaler.transform(X1_test)\n",
    "# Deep Learning Model\n",
    "def create_deep_learning_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the deep learning model\n",
    "def train_and_evaluate_dl_model(X_train, y_train, X_test, y_test, epochs=500, batch_size=2000):\n",
    "    input_shape = X_train.shape[1]\n",
    "    model = create_deep_learning_model(input_shape)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    \n",
    "    return y_pred,y_pred_prob, model\n",
    "\n",
    "# Train and evaluate the deep learning model\n",
    "y_pred,y_pred_prob, dl_model = train_and_evaluate_dl_model(X1_train, y1_train, X1_test, y1_test, epochs=500, batch_size=2000)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y1_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "precision = precision_score(y1_test, y_pred)\n",
    "recall = recall_score(y1_test, y_pred)\n",
    "f1 = f1_score(y1_test, y_pred)\n",
    "roc_auc = roc_auc_score(y1_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "performance_metrics_method1 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics:\\n\", performance_metrics_method1)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_dl_model(sequence, model, scaler):\n",
    "    properties = calculate_all_properties(sequence)\n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    prediction = model.predict(prop_values)\n",
    "    result = 'Antimicrobial' if prediction[0][0] > 0.5 else 'Non-antimicrobial'\n",
    "    return result\n",
    "\n",
    "# Example prediction\n",
    "sequence = 'MFTLKKSMLLLFFLGTISLSLC'\n",
    "\n",
    "# Prediction using deep learning model with progress bar\n",
    "print(\"\\nPredicting using Deep Learning Model:\")\n",
    "\n",
    "print(f\"Sequence: {sequence}\")\n",
    "prediction_dl = predict_dl_model(sequence, dl_model, scaler)\n",
    "print(\"Prediction:\", prediction_dl)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c92b4-0005-4f30-acfd-9efc58979526",
   "metadata": {},
   "source": [
    "**One Hot Encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c1ac7f-043c-4f8f-88c7-f1053634cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DADP ID     Entry Name Uniprot code                Species  \\\n",
       "DADP ID                                                                   \n",
       "SP_P31107  SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "SP_2643      SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "SP_2644      SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "SP_2645      SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "SP_2646      SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...              ...            ...          ...                    ...   \n",
       "SP_2852      SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "SP_2853      SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "SP_2854      SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "SP_2855      SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "SP_Q09022  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "           Sequence length         signal sequence  \\\n",
       "DADP ID                                              \n",
       "SP_P31107               81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "SP_2643                 23                       /   \n",
       "SP_2644                 23                       /   \n",
       "SP_2645                 23                       /   \n",
       "SP_2646                 16                       /   \n",
       "...                    ...                     ...   \n",
       "SP_2852                 59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "SP_2853                 55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "SP_2854                 67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "SP_2855                 64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "SP_Q09022               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                          Bioactive sequence  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107                  GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "SP_2643                              GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2644                              GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2645                              GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "SP_2646                                     ILGKLLSTAAGLLSNL   \n",
       "...                                                      ...   \n",
       "SP_2852                                        VFLGNIVSMGKKI   \n",
       "SP_2853                                   DAAVEPELYHWGKVWLPN   \n",
       "SP_2854                                 CVDIGFSPTGKRPPFCPYPG   \n",
       "SP_2855                                    VADKRPYILREKKSIPY   \n",
       "SP_Q09022  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                                  Properties  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "SP_2643    {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "SP_2644    {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "SP_2645    {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "SP_2646    {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                      ...   \n",
       "SP_2852    {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "SP_2853    {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "SP_2854    {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "SP_2855    {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "SP_Q09022  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "                    TruncSequence             ModSequence  Value  \n",
       "DADP ID                                                           \n",
       "SP_P31107  GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "SP_2643    GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2644    GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2645    GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2646          ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                           ...                     ...    ...  \n",
       "SP_2852             VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "SP_2853        DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "SP_2854      CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "SP_2855         VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "SP_Q09022  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...  BalabanJ       BertzCT       Chi0v       Chi1v  \\\n",
       "DADP ID                ...                                                   \n",
       "SP_P31107          46  ...  2.195969   7612.627585  131.799954   75.452403   \n",
       "SP_2643            31  ...  2.246811   5191.774736   95.319148   54.422594   \n",
       "SP_2644            31  ...  2.254177   5211.853178   96.026255   54.922594   \n",
       "SP_2645            31  ...  2.253657   5096.168912   94.987870   54.392469   \n",
       "SP_2646            22  ...  6.186024   3100.024844   67.476541   37.978223   \n",
       "...               ...  ...       ...           ...         ...         ...   \n",
       "SP_2852            18  ...  3.276060   2766.535116   59.496327   34.969862   \n",
       "SP_2853            26  ...  0.992706   5867.905389   86.416638   50.512045   \n",
       "SP_2854            26  ...  0.884764   5233.121472   86.425115   51.704602   \n",
       "SP_2855            30  ...  1.659505   4729.413175   85.376209   50.244738   \n",
       "SP_Q09022         111  ...  2.570045  18204.737165  294.537783  172.867436   \n",
       "\n",
       "           HallKierAlpha      Kappa1      Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107         -20.68  199.359058  105.101422   75.403319      1   \n",
       "SP_2643           -15.02  142.034244   74.405793   54.137875      1   \n",
       "SP_2644           -15.02  143.033879   75.212983   54.431542      1   \n",
       "SP_2645           -14.53  141.524432   74.738555   53.425154      1   \n",
       "SP_2646            -9.21  101.790000   52.104489   40.528848      1   \n",
       "...                  ...         ...         ...         ...    ...   \n",
       "SP_2852            -8.01   88.001112   47.066173   33.026037      0   \n",
       "SP_2853           -15.99  121.218778   58.255568   37.036541      0   \n",
       "SP_2854           -13.86  121.347700   60.104035   35.961362      0   \n",
       "SP_2855           -13.69  125.600865   65.022901   43.867276      0   \n",
       "SP_Q09022         -40.18  448.837609  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "...                                                      ...  \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[2566 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 235ms/step - loss: 0.7062 - accuracy: 0.4503 - val_loss: 0.6621 - val_accuracy: 0.6576\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6622 - accuracy: 0.6452 - val_loss: 0.6349 - val_accuracy: 0.6751\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6372 - accuracy: 0.6477 - val_loss: 0.6161 - val_accuracy: 0.6751\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6198 - accuracy: 0.6472 - val_loss: 0.6011 - val_accuracy: 0.6770\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6057 - accuracy: 0.6472 - val_loss: 0.5891 - val_accuracy: 0.6907\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5933 - accuracy: 0.6647 - val_loss: 0.5791 - val_accuracy: 0.6965\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5813 - accuracy: 0.6754 - val_loss: 0.5707 - val_accuracy: 0.7179\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.5705 - accuracy: 0.6959 - val_loss: 0.5640 - val_accuracy: 0.7043\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.5610 - accuracy: 0.6993 - val_loss: 0.5584 - val_accuracy: 0.7043\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.5522 - accuracy: 0.7066 - val_loss: 0.5538 - val_accuracy: 0.7121\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.5438 - accuracy: 0.7232 - val_loss: 0.5505 - val_accuracy: 0.7082\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.5364 - accuracy: 0.7329 - val_loss: 0.5484 - val_accuracy: 0.7101\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.5303 - accuracy: 0.7407 - val_loss: 0.5479 - val_accuracy: 0.7121\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.5258 - accuracy: 0.7432 - val_loss: 0.5482 - val_accuracy: 0.7082\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.5221 - accuracy: 0.7461 - val_loss: 0.5468 - val_accuracy: 0.7140\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.5167 - accuracy: 0.7505 - val_loss: 0.5418 - val_accuracy: 0.7160\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.5081 - accuracy: 0.7534 - val_loss: 0.5338 - val_accuracy: 0.7198\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4977 - accuracy: 0.7573 - val_loss: 0.5265 - val_accuracy: 0.7257\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4883 - accuracy: 0.7612 - val_loss: 0.5209 - val_accuracy: 0.7140\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.4809 - accuracy: 0.7607 - val_loss: 0.5166 - val_accuracy: 0.7179\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4748 - accuracy: 0.7617 - val_loss: 0.5135 - val_accuracy: 0.7198\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4699 - accuracy: 0.7563 - val_loss: 0.5112 - val_accuracy: 0.7276\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4650 - accuracy: 0.7583 - val_loss: 0.5091 - val_accuracy: 0.7354\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4599 - accuracy: 0.7651 - val_loss: 0.5066 - val_accuracy: 0.7354\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4544 - accuracy: 0.7675 - val_loss: 0.5044 - val_accuracy: 0.7354\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4479 - accuracy: 0.7758 - val_loss: 0.5043 - val_accuracy: 0.7432\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4424 - accuracy: 0.7792 - val_loss: 0.5051 - val_accuracy: 0.7412\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4383 - accuracy: 0.7870 - val_loss: 0.5036 - val_accuracy: 0.7529\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4333 - accuracy: 0.7904 - val_loss: 0.5008 - val_accuracy: 0.7568\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4275 - accuracy: 0.7978 - val_loss: 0.4973 - val_accuracy: 0.7588\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4212 - accuracy: 0.8012 - val_loss: 0.4922 - val_accuracy: 0.7665\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4150 - accuracy: 0.8036 - val_loss: 0.4910 - val_accuracy: 0.7626\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4122 - accuracy: 0.8060 - val_loss: 0.4923 - val_accuracy: 0.7743\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4107 - accuracy: 0.8085 - val_loss: 0.4925 - val_accuracy: 0.7782\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4082 - accuracy: 0.8129 - val_loss: 0.4921 - val_accuracy: 0.7724\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4047 - accuracy: 0.8119 - val_loss: 0.4913 - val_accuracy: 0.7763\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.3985 - accuracy: 0.8192 - val_loss: 0.4914 - val_accuracy: 0.7529\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3922 - accuracy: 0.8207 - val_loss: 0.4916 - val_accuracy: 0.7490\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3876 - accuracy: 0.8236 - val_loss: 0.4923 - val_accuracy: 0.7451\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3839 - accuracy: 0.8212 - val_loss: 0.4938 - val_accuracy: 0.7490\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.3813 - accuracy: 0.8241 - val_loss: 0.4934 - val_accuracy: 0.7510\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3779 - accuracy: 0.8231 - val_loss: 0.4896 - val_accuracy: 0.7626\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.3735 - accuracy: 0.8280 - val_loss: 0.4851 - val_accuracy: 0.7685\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3703 - accuracy: 0.8270 - val_loss: 0.4828 - val_accuracy: 0.7685\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3686 - accuracy: 0.8231 - val_loss: 0.4807 - val_accuracy: 0.7743\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.3649 - accuracy: 0.8280 - val_loss: 0.4791 - val_accuracy: 0.7743\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3598 - accuracy: 0.8314 - val_loss: 0.4785 - val_accuracy: 0.7782\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3559 - accuracy: 0.8363 - val_loss: 0.4787 - val_accuracy: 0.7782\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3527 - accuracy: 0.8358 - val_loss: 0.4786 - val_accuracy: 0.7763\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.3495 - accuracy: 0.8416 - val_loss: 0.4787 - val_accuracy: 0.7782\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3462 - accuracy: 0.8480 - val_loss: 0.4808 - val_accuracy: 0.7704\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3409 - accuracy: 0.8445 - val_loss: 0.4881 - val_accuracy: 0.7607\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.3394 - accuracy: 0.8480 - val_loss: 0.4897 - val_accuracy: 0.7607\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3368 - accuracy: 0.8553 - val_loss: 0.4835 - val_accuracy: 0.7588\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.3309 - accuracy: 0.8548 - val_loss: 0.4771 - val_accuracy: 0.7802\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.3275 - accuracy: 0.8606 - val_loss: 0.4747 - val_accuracy: 0.7821\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.3286 - accuracy: 0.8631 - val_loss: 0.4755 - val_accuracy: 0.7938\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.3304 - accuracy: 0.8587 - val_loss: 0.4764 - val_accuracy: 0.7860\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3277 - accuracy: 0.8596 - val_loss: 0.4764 - val_accuracy: 0.7840\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.3193 - accuracy: 0.8699 - val_loss: 0.4783 - val_accuracy: 0.7782\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.3130 - accuracy: 0.8748 - val_loss: 0.4804 - val_accuracy: 0.7743\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.3089 - accuracy: 0.8733 - val_loss: 0.4869 - val_accuracy: 0.7724\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.3062 - accuracy: 0.8582 - val_loss: 0.4954 - val_accuracy: 0.7510\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.3049 - accuracy: 0.8577 - val_loss: 0.4976 - val_accuracy: 0.7510\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.3013 - accuracy: 0.8601 - val_loss: 0.4929 - val_accuracy: 0.7821\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.2964 - accuracy: 0.8738 - val_loss: 0.4933 - val_accuracy: 0.7704\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.2966 - accuracy: 0.8694 - val_loss: 0.4960 - val_accuracy: 0.7646\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.2948 - accuracy: 0.8674 - val_loss: 0.4983 - val_accuracy: 0.7646\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2905 - accuracy: 0.8718 - val_loss: 0.5025 - val_accuracy: 0.7860\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2873 - accuracy: 0.8796 - val_loss: 0.5040 - val_accuracy: 0.7821\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2836 - accuracy: 0.8869 - val_loss: 0.5020 - val_accuracy: 0.7802\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2789 - accuracy: 0.8923 - val_loss: 0.5002 - val_accuracy: 0.7918\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2749 - accuracy: 0.8957 - val_loss: 0.5013 - val_accuracy: 0.7938\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2718 - accuracy: 0.8913 - val_loss: 0.5050 - val_accuracy: 0.7763\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.2693 - accuracy: 0.8889 - val_loss: 0.5108 - val_accuracy: 0.7724\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.2677 - accuracy: 0.8923 - val_loss: 0.5218 - val_accuracy: 0.7588\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2694 - accuracy: 0.8923 - val_loss: 0.5384 - val_accuracy: 0.7471\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2755 - accuracy: 0.8821 - val_loss: 0.5494 - val_accuracy: 0.7510\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2794 - accuracy: 0.8787 - val_loss: 0.5382 - val_accuracy: 0.7685\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2692 - accuracy: 0.8860 - val_loss: 0.5201 - val_accuracy: 0.7860\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.2583 - accuracy: 0.8933 - val_loss: 0.5142 - val_accuracy: 0.7977\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.2614 - accuracy: 0.8972 - val_loss: 0.5173 - val_accuracy: 0.7977\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2696 - accuracy: 0.8894 - val_loss: 0.5133 - val_accuracy: 0.7938\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.2582 - accuracy: 0.8947 - val_loss: 0.5217 - val_accuracy: 0.7704\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.2498 - accuracy: 0.8981 - val_loss: 0.5447 - val_accuracy: 0.7432\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2587 - accuracy: 0.8884 - val_loss: 0.5429 - val_accuracy: 0.7412\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2557 - accuracy: 0.8884 - val_loss: 0.5199 - val_accuracy: 0.7860\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.2426 - accuracy: 0.8972 - val_loss: 0.5123 - val_accuracy: 0.8152\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.2431 - accuracy: 0.9089 - val_loss: 0.5137 - val_accuracy: 0.8132\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2452 - accuracy: 0.9045 - val_loss: 0.5148 - val_accuracy: 0.8093\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2419 - accuracy: 0.9055 - val_loss: 0.5178 - val_accuracy: 0.7996\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2371 - accuracy: 0.9030 - val_loss: 0.5248 - val_accuracy: 0.7860\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2350 - accuracy: 0.9050 - val_loss: 0.5279 - val_accuracy: 0.7665\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.2320 - accuracy: 0.9050 - val_loss: 0.5242 - val_accuracy: 0.7665\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2263 - accuracy: 0.9059 - val_loss: 0.5211 - val_accuracy: 0.7860\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.2229 - accuracy: 0.9108 - val_loss: 0.5230 - val_accuracy: 0.7918\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.2220 - accuracy: 0.9128 - val_loss: 0.5271 - val_accuracy: 0.7821\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.2196 - accuracy: 0.9079 - val_loss: 0.5346 - val_accuracy: 0.7704\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2160 - accuracy: 0.9118 - val_loss: 0.5455 - val_accuracy: 0.7665\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2144 - accuracy: 0.9098 - val_loss: 0.5508 - val_accuracy: 0.7763\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.2110 - accuracy: 0.9152 - val_loss: 0.5482 - val_accuracy: 0.7763\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2052 - accuracy: 0.9220 - val_loss: 0.5430 - val_accuracy: 0.7938\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.2030 - accuracy: 0.9215 - val_loss: 0.5442 - val_accuracy: 0.8054\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2071 - accuracy: 0.9191 - val_loss: 0.5487 - val_accuracy: 0.8016\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2092 - accuracy: 0.9201 - val_loss: 0.5514 - val_accuracy: 0.7957\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.2037 - accuracy: 0.9215 - val_loss: 0.5596 - val_accuracy: 0.7802\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1993 - accuracy: 0.9274 - val_loss: 0.5745 - val_accuracy: 0.7840\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.2007 - accuracy: 0.9196 - val_loss: 0.5797 - val_accuracy: 0.7918\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1975 - accuracy: 0.9220 - val_loss: 0.5732 - val_accuracy: 0.7957\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1898 - accuracy: 0.9269 - val_loss: 0.5684 - val_accuracy: 0.8035\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1877 - accuracy: 0.9347 - val_loss: 0.5735 - val_accuracy: 0.8016\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1926 - accuracy: 0.9211 - val_loss: 0.5817 - val_accuracy: 0.7938\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1963 - accuracy: 0.9206 - val_loss: 0.5840 - val_accuracy: 0.7899\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1919 - accuracy: 0.9230 - val_loss: 0.5818 - val_accuracy: 0.7860\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1828 - accuracy: 0.9288 - val_loss: 0.5818 - val_accuracy: 0.7879\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1773 - accuracy: 0.9347 - val_loss: 0.5853 - val_accuracy: 0.7899\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.1788 - accuracy: 0.9303 - val_loss: 0.5904 - val_accuracy: 0.7996\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1843 - accuracy: 0.9259 - val_loss: 0.5933 - val_accuracy: 0.7957\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1869 - accuracy: 0.9230 - val_loss: 0.5915 - val_accuracy: 0.8016\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1821 - accuracy: 0.9254 - val_loss: 0.5890 - val_accuracy: 0.7938\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1762 - accuracy: 0.9298 - val_loss: 0.5865 - val_accuracy: 0.7957\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1728 - accuracy: 0.9347 - val_loss: 0.5879 - val_accuracy: 0.8035\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1715 - accuracy: 0.9337 - val_loss: 0.5909 - val_accuracy: 0.7977\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1711 - accuracy: 0.9342 - val_loss: 0.5893 - val_accuracy: 0.7938\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.1700 - accuracy: 0.9357 - val_loss: 0.5907 - val_accuracy: 0.8054\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.1710 - accuracy: 0.9352 - val_loss: 0.5956 - val_accuracy: 0.8074\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1719 - accuracy: 0.9376 - val_loss: 0.6009 - val_accuracy: 0.8152\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1745 - accuracy: 0.9357 - val_loss: 0.6049 - val_accuracy: 0.8113\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1694 - accuracy: 0.9391 - val_loss: 0.6116 - val_accuracy: 0.7996\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.1629 - accuracy: 0.9415 - val_loss: 0.6185 - val_accuracy: 0.7782\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1613 - accuracy: 0.9386 - val_loss: 0.6270 - val_accuracy: 0.7626\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1617 - accuracy: 0.9362 - val_loss: 0.6251 - val_accuracy: 0.7840\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1586 - accuracy: 0.9386 - val_loss: 0.6137 - val_accuracy: 0.8016\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1566 - accuracy: 0.9415 - val_loss: 0.6121 - val_accuracy: 0.8054\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1576 - accuracy: 0.9410 - val_loss: 0.6178 - val_accuracy: 0.7938\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.1542 - accuracy: 0.9386 - val_loss: 0.6275 - val_accuracy: 0.7996\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.1510 - accuracy: 0.9430 - val_loss: 0.6410 - val_accuracy: 0.7899\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1530 - accuracy: 0.9425 - val_loss: 0.6515 - val_accuracy: 0.7899\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1560 - accuracy: 0.9420 - val_loss: 0.6496 - val_accuracy: 0.7996\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1536 - accuracy: 0.9449 - val_loss: 0.6401 - val_accuracy: 0.8152\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1538 - accuracy: 0.9425 - val_loss: 0.6453 - val_accuracy: 0.8152\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1626 - accuracy: 0.9410 - val_loss: 0.6414 - val_accuracy: 0.8152\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1513 - accuracy: 0.9479 - val_loss: 0.6543 - val_accuracy: 0.7879\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.1461 - accuracy: 0.9430 - val_loss: 0.6866 - val_accuracy: 0.7665\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1592 - accuracy: 0.9376 - val_loss: 0.6808 - val_accuracy: 0.7782\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1534 - accuracy: 0.9420 - val_loss: 0.6647 - val_accuracy: 0.7840\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1474 - accuracy: 0.9435 - val_loss: 0.6678 - val_accuracy: 0.7840\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1506 - accuracy: 0.9371 - val_loss: 0.6751 - val_accuracy: 0.7918\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1479 - accuracy: 0.9401 - val_loss: 0.6773 - val_accuracy: 0.7879\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1429 - accuracy: 0.9430 - val_loss: 0.6732 - val_accuracy: 0.7879\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.1402 - accuracy: 0.9483 - val_loss: 0.6742 - val_accuracy: 0.7996\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1414 - accuracy: 0.9508 - val_loss: 0.6786 - val_accuracy: 0.8074\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1421 - accuracy: 0.9518 - val_loss: 0.6850 - val_accuracy: 0.8074\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1415 - accuracy: 0.9508 - val_loss: 0.6951 - val_accuracy: 0.8016\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1412 - accuracy: 0.9474 - val_loss: 0.7045 - val_accuracy: 0.7938\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1414 - accuracy: 0.9454 - val_loss: 0.7069 - val_accuracy: 0.7938\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1378 - accuracy: 0.9474 - val_loss: 0.7018 - val_accuracy: 0.7957\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1347 - accuracy: 0.9503 - val_loss: 0.7043 - val_accuracy: 0.8054\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1400 - accuracy: 0.9420 - val_loss: 0.7041 - val_accuracy: 0.8093\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1407 - accuracy: 0.9396 - val_loss: 0.6967 - val_accuracy: 0.8113\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1360 - accuracy: 0.9420 - val_loss: 0.7001 - val_accuracy: 0.8152\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1335 - accuracy: 0.9498 - val_loss: 0.7096 - val_accuracy: 0.8054\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1354 - accuracy: 0.9503 - val_loss: 0.7214 - val_accuracy: 0.7977\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1348 - accuracy: 0.9532 - val_loss: 0.7484 - val_accuracy: 0.7802\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1451 - accuracy: 0.9469 - val_loss: 0.7572 - val_accuracy: 0.7840\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1515 - accuracy: 0.9410 - val_loss: 0.7217 - val_accuracy: 0.7938\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1371 - accuracy: 0.9464 - val_loss: 0.7017 - val_accuracy: 0.8093\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1412 - accuracy: 0.9415 - val_loss: 0.7094 - val_accuracy: 0.8035\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1599 - accuracy: 0.9342 - val_loss: 0.7062 - val_accuracy: 0.8016\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1592 - accuracy: 0.9332 - val_loss: 0.6930 - val_accuracy: 0.8152\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1414 - accuracy: 0.9396 - val_loss: 0.6930 - val_accuracy: 0.8054\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1297 - accuracy: 0.9444 - val_loss: 0.7112 - val_accuracy: 0.7879\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1342 - accuracy: 0.9474 - val_loss: 0.7174 - val_accuracy: 0.7860\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1353 - accuracy: 0.9464 - val_loss: 0.7113 - val_accuracy: 0.7840\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1293 - accuracy: 0.9464 - val_loss: 0.6967 - val_accuracy: 0.7957\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1211 - accuracy: 0.9508 - val_loss: 0.6849 - val_accuracy: 0.8113\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1180 - accuracy: 0.9542 - val_loss: 0.6854 - val_accuracy: 0.8210\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1202 - accuracy: 0.9508 - val_loss: 0.6881 - val_accuracy: 0.8210\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1193 - accuracy: 0.9561 - val_loss: 0.6896 - val_accuracy: 0.8152\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1156 - accuracy: 0.9542 - val_loss: 0.6931 - val_accuracy: 0.8152\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1132 - accuracy: 0.9586 - val_loss: 0.6991 - val_accuracy: 0.8016\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1137 - accuracy: 0.9581 - val_loss: 0.7075 - val_accuracy: 0.8074\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1186 - accuracy: 0.9561 - val_loss: 0.7181 - val_accuracy: 0.8035\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.1217 - accuracy: 0.9522 - val_loss: 0.7314 - val_accuracy: 0.7918\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.1211 - accuracy: 0.9498 - val_loss: 0.7414 - val_accuracy: 0.7977\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1195 - accuracy: 0.9508 - val_loss: 0.7434 - val_accuracy: 0.7996\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1172 - accuracy: 0.9561 - val_loss: 0.7413 - val_accuracy: 0.7996\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1179 - accuracy: 0.9566 - val_loss: 0.7318 - val_accuracy: 0.8074\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1171 - accuracy: 0.9586 - val_loss: 0.7218 - val_accuracy: 0.8268\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1147 - accuracy: 0.9605 - val_loss: 0.7194 - val_accuracy: 0.8230\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1138 - accuracy: 0.9615 - val_loss: 0.7251 - val_accuracy: 0.8171\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1153 - accuracy: 0.9571 - val_loss: 0.7366 - val_accuracy: 0.8054\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1160 - accuracy: 0.9566 - val_loss: 0.7494 - val_accuracy: 0.8016\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1165 - accuracy: 0.9561 - val_loss: 0.7559 - val_accuracy: 0.8054\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1162 - accuracy: 0.9522 - val_loss: 0.7561 - val_accuracy: 0.8035\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1151 - accuracy: 0.9552 - val_loss: 0.7561 - val_accuracy: 0.8074\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1137 - accuracy: 0.9542 - val_loss: 0.7598 - val_accuracy: 0.8054\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1141 - accuracy: 0.9532 - val_loss: 0.7666 - val_accuracy: 0.7996\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1172 - accuracy: 0.9508 - val_loss: 0.7736 - val_accuracy: 0.8054\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1210 - accuracy: 0.9474 - val_loss: 0.7781 - val_accuracy: 0.8093\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1235 - accuracy: 0.9464 - val_loss: 0.7817 - val_accuracy: 0.8132\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1224 - accuracy: 0.9498 - val_loss: 0.7868 - val_accuracy: 0.8093\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1192 - accuracy: 0.9498 - val_loss: 0.7882 - val_accuracy: 0.8152\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1133 - accuracy: 0.9498 - val_loss: 0.7825 - val_accuracy: 0.8093\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1059 - accuracy: 0.9537 - val_loss: 0.7741 - val_accuracy: 0.8191\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1010 - accuracy: 0.9615 - val_loss: 0.7674 - val_accuracy: 0.8113\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1008 - accuracy: 0.9600 - val_loss: 0.7635 - val_accuracy: 0.8054\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1076 - accuracy: 0.9586 - val_loss: 0.7653 - val_accuracy: 0.8093\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1148 - accuracy: 0.9557 - val_loss: 0.7699 - val_accuracy: 0.8093\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1139 - accuracy: 0.9552 - val_loss: 0.7785 - val_accuracy: 0.8035\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1141 - accuracy: 0.9542 - val_loss: 0.7898 - val_accuracy: 0.7996\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1153 - accuracy: 0.9503 - val_loss: 0.7976 - val_accuracy: 0.7879\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.1153 - accuracy: 0.9483 - val_loss: 0.7942 - val_accuracy: 0.7938\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1126 - accuracy: 0.9522 - val_loss: 0.7916 - val_accuracy: 0.8016\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1108 - accuracy: 0.9498 - val_loss: 0.7877 - val_accuracy: 0.8210\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.1090 - accuracy: 0.9547 - val_loss: 0.7845 - val_accuracy: 0.8191\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.1058 - accuracy: 0.9581 - val_loss: 0.7797 - val_accuracy: 0.8230\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1014 - accuracy: 0.9600 - val_loss: 0.7774 - val_accuracy: 0.8171\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0995 - accuracy: 0.9596 - val_loss: 0.7778 - val_accuracy: 0.8074\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0999 - accuracy: 0.9596 - val_loss: 0.7806 - val_accuracy: 0.8093\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1022 - accuracy: 0.9576 - val_loss: 0.7868 - val_accuracy: 0.8074\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1042 - accuracy: 0.9576 - val_loss: 0.7928 - val_accuracy: 0.8093\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1042 - accuracy: 0.9542 - val_loss: 0.8026 - val_accuracy: 0.8054\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.1034 - accuracy: 0.9586 - val_loss: 0.8114 - val_accuracy: 0.7938\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.1038 - accuracy: 0.9576 - val_loss: 0.8081 - val_accuracy: 0.7957\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.1041 - accuracy: 0.9596 - val_loss: 0.8036 - val_accuracy: 0.7977\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1043 - accuracy: 0.9591 - val_loss: 0.8011 - val_accuracy: 0.7957\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1030 - accuracy: 0.9576 - val_loss: 0.7931 - val_accuracy: 0.8113\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0994 - accuracy: 0.9600 - val_loss: 0.7897 - val_accuracy: 0.8074\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0981 - accuracy: 0.9605 - val_loss: 0.7946 - val_accuracy: 0.8054\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0998 - accuracy: 0.9630 - val_loss: 0.8004 - val_accuracy: 0.8074\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0982 - accuracy: 0.9615 - val_loss: 0.8143 - val_accuracy: 0.8093\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1012 - accuracy: 0.9605 - val_loss: 0.8216 - val_accuracy: 0.8074\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.1045 - accuracy: 0.9566 - val_loss: 0.8108 - val_accuracy: 0.8171\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.1033 - accuracy: 0.9591 - val_loss: 0.7997 - val_accuracy: 0.8132\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1042 - accuracy: 0.9610 - val_loss: 0.7943 - val_accuracy: 0.8152\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.1026 - accuracy: 0.9630 - val_loss: 0.7936 - val_accuracy: 0.8191\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.1006 - accuracy: 0.9639 - val_loss: 0.7984 - val_accuracy: 0.8152\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.0987 - accuracy: 0.9639 - val_loss: 0.8153 - val_accuracy: 0.8093\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0994 - accuracy: 0.9596 - val_loss: 0.8399 - val_accuracy: 0.7938\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1053 - accuracy: 0.9591 - val_loss: 0.8388 - val_accuracy: 0.7938\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.1036 - accuracy: 0.9586 - val_loss: 0.8204 - val_accuracy: 0.8035\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.0975 - accuracy: 0.9615 - val_loss: 0.8107 - val_accuracy: 0.8152\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0982 - accuracy: 0.9581 - val_loss: 0.8118 - val_accuracy: 0.8191\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.1034 - accuracy: 0.9557 - val_loss: 0.8127 - val_accuracy: 0.8307\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.1008 - accuracy: 0.9605 - val_loss: 0.8165 - val_accuracy: 0.8191\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0940 - accuracy: 0.9639 - val_loss: 0.8268 - val_accuracy: 0.8113\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0912 - accuracy: 0.9644 - val_loss: 0.8379 - val_accuracy: 0.8093\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0910 - accuracy: 0.9659 - val_loss: 0.8436 - val_accuracy: 0.8113\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0890 - accuracy: 0.9678 - val_loss: 0.8454 - val_accuracy: 0.8113\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0862 - accuracy: 0.9678 - val_loss: 0.8488 - val_accuracy: 0.8016\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 222ms/step - loss: 0.0856 - accuracy: 0.9673 - val_loss: 0.8539 - val_accuracy: 0.7996\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0859 - accuracy: 0.9693 - val_loss: 0.8555 - val_accuracy: 0.7977\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0857 - accuracy: 0.9669 - val_loss: 0.8426 - val_accuracy: 0.8074\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0832 - accuracy: 0.9678 - val_loss: 0.8334 - val_accuracy: 0.8113\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0842 - accuracy: 0.9664 - val_loss: 0.8326 - val_accuracy: 0.8113\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.0874 - accuracy: 0.9605 - val_loss: 0.8338 - val_accuracy: 0.8132\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0917 - accuracy: 0.9566 - val_loss: 0.8330 - val_accuracy: 0.8074\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0953 - accuracy: 0.9576 - val_loss: 0.8308 - val_accuracy: 0.8054\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0954 - accuracy: 0.9547 - val_loss: 0.8335 - val_accuracy: 0.8054\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0959 - accuracy: 0.9566 - val_loss: 0.8391 - val_accuracy: 0.7957\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0962 - accuracy: 0.9566 - val_loss: 0.8365 - val_accuracy: 0.8054\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0929 - accuracy: 0.9596 - val_loss: 0.8336 - val_accuracy: 0.8074\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0886 - accuracy: 0.9610 - val_loss: 0.8370 - val_accuracy: 0.8152\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0857 - accuracy: 0.9659 - val_loss: 0.8420 - val_accuracy: 0.8132\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.8480 - val_accuracy: 0.8191\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0845 - accuracy: 0.9673 - val_loss: 0.8576 - val_accuracy: 0.8132\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0870 - accuracy: 0.9659 - val_loss: 0.8655 - val_accuracy: 0.8054\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0883 - accuracy: 0.9664 - val_loss: 0.8701 - val_accuracy: 0.8074\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0877 - accuracy: 0.9688 - val_loss: 0.8674 - val_accuracy: 0.8132\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0852 - accuracy: 0.9688 - val_loss: 0.8564 - val_accuracy: 0.8230\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0821 - accuracy: 0.9688 - val_loss: 0.8553 - val_accuracy: 0.8152\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0842 - accuracy: 0.9673 - val_loss: 0.8606 - val_accuracy: 0.8093\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0873 - accuracy: 0.9654 - val_loss: 0.8661 - val_accuracy: 0.8054\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0893 - accuracy: 0.9615 - val_loss: 0.8698 - val_accuracy: 0.8132\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0887 - accuracy: 0.9654 - val_loss: 0.8732 - val_accuracy: 0.8054\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.0896 - accuracy: 0.9664 - val_loss: 0.8740 - val_accuracy: 0.8035\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0897 - accuracy: 0.9669 - val_loss: 0.8641 - val_accuracy: 0.8152\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0860 - accuracy: 0.9698 - val_loss: 0.8536 - val_accuracy: 0.8249\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0887 - accuracy: 0.9615 - val_loss: 0.8583 - val_accuracy: 0.8249\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0965 - accuracy: 0.9615 - val_loss: 0.8683 - val_accuracy: 0.8230\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0981 - accuracy: 0.9620 - val_loss: 0.8751 - val_accuracy: 0.8093\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0953 - accuracy: 0.9625 - val_loss: 0.8788 - val_accuracy: 0.8113\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0904 - accuracy: 0.9669 - val_loss: 0.8847 - val_accuracy: 0.8054\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0865 - accuracy: 0.9664 - val_loss: 0.8935 - val_accuracy: 0.7977\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0856 - accuracy: 0.9625 - val_loss: 0.9045 - val_accuracy: 0.7957\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0874 - accuracy: 0.9615 - val_loss: 0.9085 - val_accuracy: 0.7957\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0888 - accuracy: 0.9600 - val_loss: 0.8967 - val_accuracy: 0.8054\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0861 - accuracy: 0.9649 - val_loss: 0.8843 - val_accuracy: 0.8074\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0863 - accuracy: 0.9635 - val_loss: 0.8787 - val_accuracy: 0.8191\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0898 - accuracy: 0.9615 - val_loss: 0.8792 - val_accuracy: 0.8132\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0900 - accuracy: 0.9620 - val_loss: 0.8929 - val_accuracy: 0.8093\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0900 - accuracy: 0.9673 - val_loss: 0.9140 - val_accuracy: 0.7957\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0957 - accuracy: 0.9620 - val_loss: 0.9154 - val_accuracy: 0.7918\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0960 - accuracy: 0.9610 - val_loss: 0.8904 - val_accuracy: 0.7996\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0871 - accuracy: 0.9635 - val_loss: 0.8713 - val_accuracy: 0.8132\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0830 - accuracy: 0.9644 - val_loss: 0.8698 - val_accuracy: 0.8249\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0854 - accuracy: 0.9664 - val_loss: 0.8748 - val_accuracy: 0.8288\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0880 - accuracy: 0.9659 - val_loss: 0.8751 - val_accuracy: 0.8249\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0878 - accuracy: 0.9625 - val_loss: 0.8735 - val_accuracy: 0.8268\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0878 - accuracy: 0.9649 - val_loss: 0.8747 - val_accuracy: 0.8191\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0871 - accuracy: 0.9615 - val_loss: 0.8799 - val_accuracy: 0.8093\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0875 - accuracy: 0.9644 - val_loss: 0.8792 - val_accuracy: 0.8132\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0865 - accuracy: 0.9664 - val_loss: 0.8775 - val_accuracy: 0.8171\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0850 - accuracy: 0.9659 - val_loss: 0.8802 - val_accuracy: 0.8191\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0845 - accuracy: 0.9639 - val_loss: 0.8858 - val_accuracy: 0.8132\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0824 - accuracy: 0.9654 - val_loss: 0.8976 - val_accuracy: 0.8054\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0844 - accuracy: 0.9630 - val_loss: 0.9040 - val_accuracy: 0.8016\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0860 - accuracy: 0.9610 - val_loss: 0.8969 - val_accuracy: 0.8054\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0869 - accuracy: 0.9620 - val_loss: 0.9053 - val_accuracy: 0.8132\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1026 - accuracy: 0.9566 - val_loss: 0.9133 - val_accuracy: 0.8152\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1136 - accuracy: 0.9561 - val_loss: 0.9082 - val_accuracy: 0.8152\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.1063 - accuracy: 0.9605 - val_loss: 0.9038 - val_accuracy: 0.8035\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0942 - accuracy: 0.9644 - val_loss: 0.9087 - val_accuracy: 0.7977\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0876 - accuracy: 0.9649 - val_loss: 0.9138 - val_accuracy: 0.7977\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0855 - accuracy: 0.9659 - val_loss: 0.9116 - val_accuracy: 0.8054\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0830 - accuracy: 0.9683 - val_loss: 0.9081 - val_accuracy: 0.7977\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0832 - accuracy: 0.9639 - val_loss: 0.9142 - val_accuracy: 0.8016\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0900 - accuracy: 0.9639 - val_loss: 0.9158 - val_accuracy: 0.7996\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0950 - accuracy: 0.9615 - val_loss: 0.9003 - val_accuracy: 0.8054\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.0916 - accuracy: 0.9615 - val_loss: 0.8904 - val_accuracy: 0.8230\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.0885 - accuracy: 0.9625 - val_loss: 0.8904 - val_accuracy: 0.8152\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0881 - accuracy: 0.9654 - val_loss: 0.8951 - val_accuracy: 0.8191\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0870 - accuracy: 0.9669 - val_loss: 0.9056 - val_accuracy: 0.8113\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0883 - accuracy: 0.9673 - val_loss: 0.9185 - val_accuracy: 0.8074\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0918 - accuracy: 0.9678 - val_loss: 0.9235 - val_accuracy: 0.8054\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0930 - accuracy: 0.9669 - val_loss: 0.9162 - val_accuracy: 0.8054\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0896 - accuracy: 0.9683 - val_loss: 0.9066 - val_accuracy: 0.8171\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0862 - accuracy: 0.9659 - val_loss: 0.8986 - val_accuracy: 0.8113\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0863 - accuracy: 0.9635 - val_loss: 0.8975 - val_accuracy: 0.8054\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0868 - accuracy: 0.9615 - val_loss: 0.8984 - val_accuracy: 0.8132\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0839 - accuracy: 0.9625 - val_loss: 0.8956 - val_accuracy: 0.8152\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0803 - accuracy: 0.9610 - val_loss: 0.8949 - val_accuracy: 0.8132\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0780 - accuracy: 0.9659 - val_loss: 0.8977 - val_accuracy: 0.8152\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0767 - accuracy: 0.9664 - val_loss: 0.9000 - val_accuracy: 0.8191\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0762 - accuracy: 0.9688 - val_loss: 0.9084 - val_accuracy: 0.8093\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0774 - accuracy: 0.9654 - val_loss: 0.9214 - val_accuracy: 0.8093\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0786 - accuracy: 0.9654 - val_loss: 0.9321 - val_accuracy: 0.7977\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.0798 - accuracy: 0.9659 - val_loss: 0.9334 - val_accuracy: 0.8016\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0794 - accuracy: 0.9659 - val_loss: 0.9262 - val_accuracy: 0.7977\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0781 - accuracy: 0.9664 - val_loss: 0.9201 - val_accuracy: 0.8054\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0782 - accuracy: 0.9659 - val_loss: 0.9177 - val_accuracy: 0.8113\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0787 - accuracy: 0.9659 - val_loss: 0.9159 - val_accuracy: 0.8113\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0792 - accuracy: 0.9649 - val_loss: 0.9166 - val_accuracy: 0.8171\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0780 - accuracy: 0.9644 - val_loss: 0.9210 - val_accuracy: 0.8210\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0736 - accuracy: 0.9678 - val_loss: 0.9319 - val_accuracy: 0.8132\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0734 - accuracy: 0.9717 - val_loss: 0.9465 - val_accuracy: 0.8074\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0779 - accuracy: 0.9683 - val_loss: 0.9534 - val_accuracy: 0.8054\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0813 - accuracy: 0.9639 - val_loss: 0.9495 - val_accuracy: 0.8054\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0803 - accuracy: 0.9635 - val_loss: 0.9371 - val_accuracy: 0.8171\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0763 - accuracy: 0.9639 - val_loss: 0.9259 - val_accuracy: 0.8230\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0749 - accuracy: 0.9630 - val_loss: 0.9225 - val_accuracy: 0.8268\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0768 - accuracy: 0.9659 - val_loss: 0.9224 - val_accuracy: 0.8210\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0780 - accuracy: 0.9644 - val_loss: 0.9221 - val_accuracy: 0.8152\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0796 - accuracy: 0.9644 - val_loss: 0.9236 - val_accuracy: 0.8093\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0826 - accuracy: 0.9673 - val_loss: 0.9288 - val_accuracy: 0.8113\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0837 - accuracy: 0.9673 - val_loss: 0.9361 - val_accuracy: 0.8074\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0811 - accuracy: 0.9683 - val_loss: 0.9432 - val_accuracy: 0.7977\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0768 - accuracy: 0.9678 - val_loss: 0.9522 - val_accuracy: 0.8035\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0736 - accuracy: 0.9669 - val_loss: 0.9636 - val_accuracy: 0.8074\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0733 - accuracy: 0.9708 - val_loss: 0.9596 - val_accuracy: 0.8132\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0721 - accuracy: 0.9712 - val_loss: 0.9452 - val_accuracy: 0.8171\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0700 - accuracy: 0.9717 - val_loss: 0.9353 - val_accuracy: 0.8268\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0702 - accuracy: 0.9722 - val_loss: 0.9348 - val_accuracy: 0.8268\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0743 - accuracy: 0.9722 - val_loss: 0.9342 - val_accuracy: 0.8268\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0751 - accuracy: 0.9703 - val_loss: 0.9317 - val_accuracy: 0.8268\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0717 - accuracy: 0.9698 - val_loss: 0.9342 - val_accuracy: 0.8249\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0698 - accuracy: 0.9708 - val_loss: 0.9406 - val_accuracy: 0.8191\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0699 - accuracy: 0.9703 - val_loss: 0.9485 - val_accuracy: 0.8093\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0707 - accuracy: 0.9659 - val_loss: 0.9634 - val_accuracy: 0.7977\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0735 - accuracy: 0.9654 - val_loss: 0.9787 - val_accuracy: 0.7938\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0776 - accuracy: 0.9654 - val_loss: 0.9815 - val_accuracy: 0.7977\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.0779 - accuracy: 0.9678 - val_loss: 0.9714 - val_accuracy: 0.8093\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0740 - accuracy: 0.9708 - val_loss: 0.9594 - val_accuracy: 0.8191\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0716 - accuracy: 0.9703 - val_loss: 0.9550 - val_accuracy: 0.8210\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0723 - accuracy: 0.9678 - val_loss: 0.9620 - val_accuracy: 0.8210\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0721 - accuracy: 0.9678 - val_loss: 0.9754 - val_accuracy: 0.8152\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0724 - accuracy: 0.9693 - val_loss: 0.9911 - val_accuracy: 0.8035\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0765 - accuracy: 0.9654 - val_loss: 0.9994 - val_accuracy: 0.8035\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0795 - accuracy: 0.9620 - val_loss: 0.9973 - val_accuracy: 0.8074\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 0.0799 - accuracy: 0.9620 - val_loss: 0.9924 - val_accuracy: 0.8132\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 0.0795 - accuracy: 0.9630 - val_loss: 0.9858 - val_accuracy: 0.8210\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0774 - accuracy: 0.9644 - val_loss: 0.9775 - val_accuracy: 0.8191\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0754 - accuracy: 0.9678 - val_loss: 0.9701 - val_accuracy: 0.8191\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0766 - accuracy: 0.9683 - val_loss: 0.9692 - val_accuracy: 0.8210\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0807 - accuracy: 0.9678 - val_loss: 0.9724 - val_accuracy: 0.8191\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0838 - accuracy: 0.9664 - val_loss: 0.9754 - val_accuracy: 0.8113\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0843 - accuracy: 0.9688 - val_loss: 0.9768 - val_accuracy: 0.8132\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0832 - accuracy: 0.9683 - val_loss: 0.9682 - val_accuracy: 0.8113\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0785 - accuracy: 0.9688 - val_loss: 0.9530 - val_accuracy: 0.8249\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0726 - accuracy: 0.9708 - val_loss: 0.9516 - val_accuracy: 0.8346\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0761 - accuracy: 0.9693 - val_loss: 0.9663 - val_accuracy: 0.8268\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0867 - accuracy: 0.9649 - val_loss: 0.9797 - val_accuracy: 0.8268\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0923 - accuracy: 0.9654 - val_loss: 0.9876 - val_accuracy: 0.8171\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0923 - accuracy: 0.9615 - val_loss: 0.9943 - val_accuracy: 0.8132\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0915 - accuracy: 0.9639 - val_loss: 1.0007 - val_accuracy: 0.8074\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0907 - accuracy: 0.9669 - val_loss: 0.9933 - val_accuracy: 0.8074\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0868 - accuracy: 0.9659 - val_loss: 0.9754 - val_accuracy: 0.8054\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0824 - accuracy: 0.9678 - val_loss: 0.9566 - val_accuracy: 0.8230\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0824 - accuracy: 0.9673 - val_loss: 0.9515 - val_accuracy: 0.8327\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0865 - accuracy: 0.9698 - val_loss: 0.9526 - val_accuracy: 0.8307\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0876 - accuracy: 0.9688 - val_loss: 0.9515 - val_accuracy: 0.8210\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0820 - accuracy: 0.9688 - val_loss: 0.9576 - val_accuracy: 0.8074\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0770 - accuracy: 0.9678 - val_loss: 0.9746 - val_accuracy: 0.8054\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0771 - accuracy: 0.9669 - val_loss: 0.9917 - val_accuracy: 0.8016\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0796 - accuracy: 0.9673 - val_loss: 0.9911 - val_accuracy: 0.7996\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.0794 - accuracy: 0.9678 - val_loss: 0.9779 - val_accuracy: 0.8093\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0781 - accuracy: 0.9683 - val_loss: 0.9666 - val_accuracy: 0.8191\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0790 - accuracy: 0.9664 - val_loss: 0.9633 - val_accuracy: 0.8171\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0824 - accuracy: 0.9635 - val_loss: 0.9633 - val_accuracy: 0.8210\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0835 - accuracy: 0.9630 - val_loss: 0.9639 - val_accuracy: 0.8249\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0824 - accuracy: 0.9635 - val_loss: 0.9661 - val_accuracy: 0.8288\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0807 - accuracy: 0.9649 - val_loss: 0.9685 - val_accuracy: 0.8230\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0775 - accuracy: 0.9669 - val_loss: 0.9730 - val_accuracy: 0.8171\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0763 - accuracy: 0.9664 - val_loss: 0.9780 - val_accuracy: 0.8191\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0743 - accuracy: 0.9693 - val_loss: 0.9881 - val_accuracy: 0.8074\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0724 - accuracy: 0.9703 - val_loss: 1.0016 - val_accuracy: 0.8054\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0728 - accuracy: 0.9688 - val_loss: 1.0062 - val_accuracy: 0.8035\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0727 - accuracy: 0.9698 - val_loss: 1.0036 - val_accuracy: 0.8016\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0726 - accuracy: 0.9703 - val_loss: 0.9987 - val_accuracy: 0.8074\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0742 - accuracy: 0.9698 - val_loss: 0.9955 - val_accuracy: 0.8152\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.0764 - accuracy: 0.9693 - val_loss: 0.9978 - val_accuracy: 0.8093\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0791 - accuracy: 0.9654 - val_loss: 1.0000 - val_accuracy: 0.8093\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0796 - accuracy: 0.9630 - val_loss: 1.0023 - val_accuracy: 0.8113\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0748 - accuracy: 0.9669 - val_loss: 1.0112 - val_accuracy: 0.8093\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0723 - accuracy: 0.9712 - val_loss: 1.0133 - val_accuracy: 0.7996\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0713 - accuracy: 0.9688 - val_loss: 1.0097 - val_accuracy: 0.7977\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0706 - accuracy: 0.9678 - val_loss: 1.0003 - val_accuracy: 0.8054\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0694 - accuracy: 0.9708 - val_loss: 0.9891 - val_accuracy: 0.8054\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0692 - accuracy: 0.9717 - val_loss: 0.9835 - val_accuracy: 0.8152\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0677 - accuracy: 0.9717 - val_loss: 0.9842 - val_accuracy: 0.8191\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0669 - accuracy: 0.9722 - val_loss: 0.9862 - val_accuracy: 0.8171\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0681 - accuracy: 0.9722 - val_loss: 0.9894 - val_accuracy: 0.8171\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0699 - accuracy: 0.9708 - val_loss: 0.9971 - val_accuracy: 0.8210\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0721 - accuracy: 0.9698 - val_loss: 1.0064 - val_accuracy: 0.8171\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0737 - accuracy: 0.9683 - val_loss: 1.0183 - val_accuracy: 0.8152\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0735 - accuracy: 0.9698 - val_loss: 1.0330 - val_accuracy: 0.8074\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0759 - accuracy: 0.9659 - val_loss: 1.0331 - val_accuracy: 0.8054\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0749 - accuracy: 0.9649 - val_loss: 1.0187 - val_accuracy: 0.8016\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0712 - accuracy: 0.9683 - val_loss: 1.0040 - val_accuracy: 0.8093\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0698 - accuracy: 0.9708 - val_loss: 0.9935 - val_accuracy: 0.8171\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0705 - accuracy: 0.9698 - val_loss: 0.9883 - val_accuracy: 0.8230\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0750 - accuracy: 0.9669 - val_loss: 0.9861 - val_accuracy: 0.8210\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0766 - accuracy: 0.9683 - val_loss: 0.9876 - val_accuracy: 0.8210\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0742 - accuracy: 0.9683 - val_loss: 0.9911 - val_accuracy: 0.8132\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0736 - accuracy: 0.9698 - val_loss: 0.9883 - val_accuracy: 0.8113\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0731 - accuracy: 0.9698 - val_loss: 0.9869 - val_accuracy: 0.8132\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0725 - accuracy: 0.9669 - val_loss: 0.9894 - val_accuracy: 0.8132\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0725 - accuracy: 0.9673 - val_loss: 0.9927 - val_accuracy: 0.8152\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0722 - accuracy: 0.9664 - val_loss: 1.0001 - val_accuracy: 0.8113\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0714 - accuracy: 0.9654 - val_loss: 1.0147 - val_accuracy: 0.8054\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0733 - accuracy: 0.9654 - val_loss: 1.0298 - val_accuracy: 0.8016\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0779 - accuracy: 0.9639 - val_loss: 1.0306 - val_accuracy: 0.8016\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0767 - accuracy: 0.9630 - val_loss: 1.0215 - val_accuracy: 0.8132\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0710 - accuracy: 0.9712 - val_loss: 1.0148 - val_accuracy: 0.8093\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0671 - accuracy: 0.9693 - val_loss: 1.0135 - val_accuracy: 0.8074\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0658 - accuracy: 0.9678 - val_loss: 1.0155 - val_accuracy: 0.8074\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0669 - accuracy: 0.9683 - val_loss: 1.0191 - val_accuracy: 0.8035\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0671 - accuracy: 0.9678 - val_loss: 1.0218 - val_accuracy: 0.8054\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0675 - accuracy: 0.9688 - val_loss: 1.0150 - val_accuracy: 0.8074\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0666 - accuracy: 0.9693 - val_loss: 1.0094 - val_accuracy: 0.8074\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0660 - accuracy: 0.9688 - val_loss: 1.0081 - val_accuracy: 0.8113\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0650 - accuracy: 0.9688 - val_loss: 1.0100 - val_accuracy: 0.8152\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0646 - accuracy: 0.9688 - val_loss: 1.0139 - val_accuracy: 0.8074\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0654 - accuracy: 0.9635 - val_loss: 1.0200 - val_accuracy: 0.8054\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0665 - accuracy: 0.9635 - val_loss: 1.0265 - val_accuracy: 0.8016\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0682 - accuracy: 0.9630 - val_loss: 1.0262 - val_accuracy: 0.8035\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0719 - accuracy: 0.9600 - val_loss: 1.0234 - val_accuracy: 0.8054\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0722 - accuracy: 0.9635 - val_loss: 1.0201 - val_accuracy: 0.8016\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.0682 - accuracy: 0.9664 - val_loss: 1.0183 - val_accuracy: 0.8074\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0659 - accuracy: 0.9712 - val_loss: 1.0216 - val_accuracy: 0.8035\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0663 - accuracy: 0.9703 - val_loss: 1.0317 - val_accuracy: 0.8016\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0685 - accuracy: 0.9708 - val_loss: 1.0367 - val_accuracy: 0.7977\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0700 - accuracy: 0.9703 - val_loss: 1.0360 - val_accuracy: 0.7996\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0695 - accuracy: 0.9703 - val_loss: 1.0378 - val_accuracy: 0.7996\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0690 - accuracy: 0.9703 - val_loss: 1.0384 - val_accuracy: 0.8035\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0691 - accuracy: 0.9693 - val_loss: 1.0364 - val_accuracy: 0.8093\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 298ms/step - loss: 0.0697 - accuracy: 0.9698 - val_loss: 1.0328 - val_accuracy: 0.8152\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0701 - accuracy: 0.9708 - val_loss: 1.0310 - val_accuracy: 0.8210\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0704 - accuracy: 0.9708 - val_loss: 1.0322 - val_accuracy: 0.8230\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0689 - accuracy: 0.9708 - val_loss: 1.0338 - val_accuracy: 0.8210\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0673 - accuracy: 0.9678 - val_loss: 1.0315 - val_accuracy: 0.8210\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0664 - accuracy: 0.9698 - val_loss: 1.0299 - val_accuracy: 0.8132\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0648 - accuracy: 0.9698 - val_loss: 1.0318 - val_accuracy: 0.8113\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0642 - accuracy: 0.9698 - val_loss: 1.0345 - val_accuracy: 0.8074\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0647 - accuracy: 0.9698 - val_loss: 1.0379 - val_accuracy: 0.8054\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0655 - accuracy: 0.9708 - val_loss: 1.0435 - val_accuracy: 0.8074\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0666 - accuracy: 0.9693 - val_loss: 1.0483 - val_accuracy: 0.8035\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0667 - accuracy: 0.9708 - val_loss: 1.0456 - val_accuracy: 0.8074\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0655 - accuracy: 0.9698 - val_loss: 1.0384 - val_accuracy: 0.8035\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0649 - accuracy: 0.9688 - val_loss: 1.0316 - val_accuracy: 0.8054\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0653 - accuracy: 0.9683 - val_loss: 1.0257 - val_accuracy: 0.8132\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0658 - accuracy: 0.9683 - val_loss: 1.0218 - val_accuracy: 0.8191\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.0672 - accuracy: 0.9688 - val_loss: 1.0198 - val_accuracy: 0.8152\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0655 - accuracy: 0.9688 - val_loss: 1.0226 - val_accuracy: 0.8191\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0624 - accuracy: 0.9703 - val_loss: 1.0323 - val_accuracy: 0.8113\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0633 - accuracy: 0.9722 - val_loss: 1.0415 - val_accuracy: 0.8093\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0657 - accuracy: 0.9727 - val_loss: 1.0443 - val_accuracy: 0.8093\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.0669 - accuracy: 0.9722 - val_loss: 1.0403 - val_accuracy: 0.8171\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.0672 - accuracy: 0.9722 - val_loss: 1.0364 - val_accuracy: 0.8074\n",
      "17/17 [==============================] - 0s 3ms/step\n",
      "Performance Metrics for Method 2 (One-hot Encoding):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.807393\n",
      "1  Precision  0.709497\n",
      "2     Recall  0.729885\n",
      "3   F1 Score  0.719547\n",
      "4    ROC AUC  0.845428\n",
      "\n",
      "Confusion Matrix for Method 2 (One-hot Encoding):\n",
      " [[288  52]\n",
      " [ 47 127]]\n",
      "MFTLKKSMLLLFFLGTISLSLC\n",
      "neither\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Prediction for Method 2 (Deep Learning): Antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming you have already defined posDF, negDF, char_to_int, and alphabet as in your original code\n",
    "\n",
    "# Process data for Method 2 (One-hot Encoding)\n",
    "allDataDF = allDataDF.reset_index(drop=True)\n",
    "posDF = posDF.reset_index(drop=True)\n",
    "negDF = negDF.reset_index(drop=True)\n",
    "\n",
    "posDF['Bioactive sequence'] = (posDF['Bioactive sequence'].str.upper())\n",
    "negDF['Bioactive sequence'] = (negDF['Bioactive sequence'].str.upper())\n",
    "\n",
    "posDF['TruncSequence'] = (posDF['Bioactive sequence'].str.slice(0,22))\n",
    "negDF['TruncSequence'] = (negDF['Bioactive sequence'].str.slice(0,22))\n",
    "\n",
    "posDF['ModSequence'] = (posDF['TruncSequence'].str.pad(22,\"right\",\"#\"))\n",
    "negDF['ModSequence'] = (negDF['TruncSequence'].str.pad(22,\"right\",\"#\"))\n",
    "\n",
    "posDF['Value'] = 1\n",
    "negDF['Value'] = 0\n",
    "\n",
    "\n",
    "a=pd.concat([posDF,negDF])\n",
    "a=a.set_index(a['DADP ID'])\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY#/ '\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "# Encode sequences using one-hot encoding\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    integer_encoded = [char_to_int[char] for char in sequence]\n",
    "    onehot_encoded = np.zeros((len(sequence), len(alphabet)))\n",
    "    for i, value in enumerate(integer_encoded):\n",
    "        onehot_encoded[i, value] = 1\n",
    "    return onehot_encoded\n",
    "display(a)\n",
    "allDataDF2['EncodedX'] = a['ModSequence'].apply(one_hot_encode_sequence)\n",
    "display(allDataDF2)\n",
    "# Prepare data for training\n",
    "X = np.array(list(allDataDF2['EncodedX']))\n",
    "y = allDataDF2['Value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define a simple feedforward neural network model\n",
    "def create_nn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the neural network model\n",
    "model = create_nn_model(input_shape=X.shape[1:])\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=2000, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method2 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 2 (One-hot Encoding):\\n\", performance_metrics_method2)\n",
    "print(\"\\nConfusion Matrix for Method 2 (One-hot Encoding):\\n\", conf_matrix)\n",
    "\n",
    "# Function to predict using the trained neural network model\n",
    "def predict_method2(sequence, model):\n",
    "    sequence = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence]\n",
    "    onehot_encoded = np.zeros((len(sequence), len(alphabet)))\n",
    "    for i, value in enumerate(integer_encoded):\n",
    "        onehot_encoded[i, value] = 1\n",
    "    onehot_encoded = np.array(onehot_encoded).reshape(1, 22, len(alphabet))  # Reshape for the model\n",
    "    prediction = model.predict(onehot_encoded)\n",
    "    if prediction[0] >= 0.5:\n",
    "        return 'Antimicrobial'\n",
    "    else:\n",
    "        return 'Non-antimicrobial'\n",
    "\n",
    "# Example of making predictions\n",
    "sequence='MFTLKKSMLLLFFLGTISLSLC'\n",
    "print(sequence)\n",
    "if sequence in posDF[\"Bioactive sequence\"]:\n",
    "    print(\"positive\")\n",
    "elif sequence in negDF:\n",
    "    print(\"negative\")\n",
    "else:\n",
    "    print(\"neither\")\n",
    "    \n",
    "\n",
    "prediction = predict_method2(sequence, model)\n",
    "print(f\"Prediction for Method 2 (Deep Learning): {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f1abc-64f6-4f18-9a8e-cf9ed08a12ad",
   "metadata": {},
   "source": [
    "**Bert Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5db40e-894e-4cbf-85b8-d79fb5aec9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DADP ID     Entry Name Uniprot code                Species  \\\n",
       "0     SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "1       SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "2       SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "3       SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "4       SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...         ...            ...          ...                    ...   \n",
       "1640    SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "1641    SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "1642    SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "1643    SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "1644  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "      Sequence length         signal sequence  \\\n",
       "0                  81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "1                  23                       /   \n",
       "2                  23                       /   \n",
       "3                  23                       /   \n",
       "4                  16                       /   \n",
       "...               ...                     ...   \n",
       "1640               59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "1641               55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "1642               67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "1643               64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "1644               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                     Bioactive sequence  \\\n",
       "0                     GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "1                               GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "2                               GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "3                               GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "4                                      ILGKLLSTAAGLLSNL   \n",
       "...                                                 ...   \n",
       "1640                                      VFLGNIVSMGKKI   \n",
       "1641                                 DAAVEPELYHWGKVWLPN   \n",
       "1642                               CVDIGFSPTGKRPPFCPYPG   \n",
       "1643                                  VADKRPYILREKKSIPY   \n",
       "1644  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                             Properties  \\\n",
       "0     {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "1     {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "2     {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "3     {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "4     {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                 ...   \n",
       "1640  {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "1641  {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "1642  {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "1643  {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "1644  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "               TruncSequence             ModSequence  Value  \n",
       "0     GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "1     GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "2     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "3     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "4           ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                      ...                     ...    ...  \n",
       "1640           VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "1641      DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "1642    CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "1643       VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "1644  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioBERT model and tokenizer...\n",
      "Generating embeddings for sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding sequences:   0%|                                                            | 2/2566 [00:01<20:38,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV\n",
      "Embedding shape: (1, 768)\n",
      "Embedding: [[-1.64003409e-02 -4.82710510e-01  2.49254555e-01 -1.09041380e-02\n",
      "   7.28634894e-01 -6.47612751e-01  5.33651933e-02  6.01541884e-02\n",
      "   1.70116290e-01  1.52809873e-01  6.76362962e-02 -3.91306877e-01\n",
      "   2.00200170e-01 -1.46960109e-01 -2.70447642e-01  1.64351575e-02\n",
      "  -3.26772660e-01 -5.08496761e-01 -6.19464889e-02 -2.41161324e-02\n",
      "  -2.76165307e-01 -1.97354943e-01 -1.28901646e-01  6.33836761e-02\n",
      "  -7.28787035e-02  9.17148683e-03 -1.29213050e-01  2.32727125e-01\n",
      "  -3.41059491e-02  5.04042029e-01 -1.14261426e-01 -3.74896735e-01\n",
      "  -3.62805247e-01 -8.44919980e-01  1.53892905e-01  1.49310201e-01\n",
      "  -3.13910916e-02 -1.67167813e-01  2.06563830e-01 -7.79478401e-02\n",
      "   2.59182304e-01  2.83368021e-01  9.26770031e-01  9.09246728e-02\n",
      "   2.39800140e-01 -2.97584422e-02 -3.81289199e-02 -4.01842356e-01\n",
      "   6.95645362e-02  4.24234152e-01 -2.79195487e-01  3.44893396e-01\n",
      "   4.80712652e-02  2.03084499e-01  3.26647237e-02 -7.62905851e-02\n",
      "  -4.92505938e-01 -6.87148347e-02  9.33533758e-02  2.80629188e-01\n",
      "   4.63714421e-01  8.73696685e-01  1.02981257e+00 -5.69522381e-01\n",
      "  -3.87014151e-01  4.33567584e-01  2.59703398e-02  3.32449228e-01\n",
      "  -2.81213373e-02 -5.18061034e-02 -5.19406438e-01 -3.57699215e-01\n",
      "  -2.03453451e-01 -2.34661371e-01 -3.22974026e-01  4.39279646e-01\n",
      "   8.12059045e-01 -2.29620606e-01 -3.03594284e-02 -4.21073973e-01\n",
      "   2.30230376e-01 -6.78885952e-02  5.24203300e-01  1.45139962e-01\n",
      "  -2.00255699e-02  8.59785229e-02  3.48286748e-01  2.23969296e-01\n",
      "  -1.21705174e-01  1.53178915e-01  2.31319994e-01  1.67289883e-01\n",
      "  -4.23715591e-01  2.60757711e-02  2.26204153e-02  2.78430760e-01\n",
      "  -2.30804868e-02 -2.27607433e-02  4.78556544e-01  1.02654457e-01\n",
      "   2.71262228e-01  3.51161718e-01  1.34500906e-01  4.18255419e-01\n",
      "   1.30037695e-01 -1.21921621e-01  3.21175396e-01 -2.23258898e-01\n",
      "  -9.98982191e-02  5.41192889e-02  4.95859802e-01 -1.45267636e-01\n",
      "   1.66579157e-01 -2.85401195e-01 -1.72845721e-01  2.15685457e-01\n",
      "  -2.97062516e-01 -5.28271571e-02  2.73609549e-01 -1.79285914e-01\n",
      "   4.15746272e-01 -5.27933165e-02  9.44848824e-03  2.77196169e-02\n",
      "   2.17274427e-01 -2.05512762e-01  1.36599347e-01  5.96080534e-02\n",
      "  -1.31294653e-01  1.51329175e-01 -1.59273148e-01  1.17665902e-01\n",
      "  -1.81695983e-01 -4.99983728e-01  3.25479925e-01 -1.90261342e-02\n",
      "  -7.54253745e-01  2.19560891e-01 -5.06306469e-01 -2.25222200e-01\n",
      "   2.23297626e-01 -5.72084114e-02  1.67503908e-01 -2.67478019e-01\n",
      "  -2.87734836e-01 -3.43581259e-01 -2.39790767e-01 -3.91018242e-01\n",
      "   6.54581726e-01 -4.03676510e-01 -3.89921010e-01 -8.68996605e-02\n",
      "  -2.36088231e-01 -2.46641323e-01  4.60999086e-02 -2.06793144e-01\n",
      "   2.50632823e-01  3.30883414e-01  2.59830058e-03  6.14218652e-01\n",
      "   3.36680114e-01  1.98538378e-01 -4.50240284e-01  3.77394587e-01\n",
      "  -1.09299123e-01  3.91387343e-01  2.83318430e-01  1.65441051e-01\n",
      "  -3.18492383e-01  3.36919963e-01  2.29316518e-01 -3.58706623e-01\n",
      "   1.10764980e-01  8.70121002e-01  5.66058457e-01  2.63886154e-01\n",
      "  -5.37948590e-03 -4.72802073e-01 -4.48753178e-01  1.19649008e-01\n",
      "  -1.17332954e-02 -1.74811766e-01 -1.70688957e-01 -1.02806211e-01\n",
      "   1.02928616e-01  8.03418905e-02  2.93123186e-01 -4.35834736e-01\n",
      "   8.07677135e-02  4.81469631e-01 -6.70536980e-02 -6.44497499e-02\n",
      "   7.04802126e-02 -2.27294832e-01 -1.93789646e-01 -3.53011429e-01\n",
      "  -5.27552009e-01  2.30557829e-01  6.43702149e-02  1.18900038e-01\n",
      "  -4.81676459e-02  2.72338092e-01 -4.48655307e-01  3.53000909e-01\n",
      "   5.64118505e-01  1.41916290e-01  2.51963675e-01  6.94609061e-02\n",
      "  -8.71846825e-02  3.24353069e-01 -1.00836203e-01  2.72442430e-01\n",
      "   3.56450737e-01  2.03076005e-01 -4.95974630e-01 -2.78541923e-01\n",
      "  -5.33652306e-02 -2.65108168e-01 -1.89601809e-01 -8.94655734e-02\n",
      "  -3.80511433e-01  1.21126339e-01 -9.39186439e-02  1.47225887e-01\n",
      "  -1.88323408e-01  2.40825325e-01  1.68419331e-01 -1.12176225e-01\n",
      "   7.82238007e-01 -1.05786487e-01  2.13067025e-01 -9.49328095e-02\n",
      "  -4.78597075e-01 -2.07954019e-01 -2.36499190e-01 -6.08335733e-01\n",
      "   2.03321770e-01 -3.26503694e-01 -3.38162512e-01 -1.43479168e-01\n",
      "   1.57580391e-01 -1.93155721e-01 -6.36705756e-02  1.44481589e-03\n",
      "  -1.33347228e-01  2.68800944e-01 -3.17861885e-01  3.18718910e-01\n",
      "   1.25608802e-01  1.93304233e-02  7.65366912e-01 -1.46732047e-01\n",
      "   2.15564460e-01  3.79094362e-01 -5.25868610e-02  2.29893595e-01\n",
      "  -7.92285427e-02  1.77079886e-01 -1.83717519e-01 -2.05646425e-01\n",
      "  -2.22441241e-01 -3.66842449e-01  3.69438469e-01  1.00258484e-01\n",
      "   3.05972070e-01  3.45058858e-01  1.43970773e-01 -4.20839004e-02\n",
      "  -4.73827660e-01 -1.16497472e-01 -3.23148727e-01  1.72412157e-01\n",
      "  -2.08704859e-01  2.38257796e-02  5.60933113e-01 -3.49931121e-01\n",
      "  -4.34415251e-01  1.10159613e-01  5.57218730e-01 -3.79327625e-01\n",
      "  -1.45372134e-02 -4.20112044e-01  9.88156721e-02  2.63102353e-01\n",
      "  -3.41233343e-01 -1.72946781e-01  2.17428118e-01  4.85941231e-01\n",
      "   1.17742859e-01 -1.63134485e-01  1.58303604e-01  1.63433939e-01\n",
      "  -9.93495621e-03  1.32355422e-01  2.33467907e-01  1.90930679e-01\n",
      "  -3.03264558e-02  3.41879278e-01  2.26501554e-01 -2.85040915e-01\n",
      "  -9.46656838e-02 -1.87640071e-01  1.20637253e-01  1.64712518e-01\n",
      "  -1.31472200e-01  2.14573339e-01 -6.09389059e-02 -1.01145655e-02\n",
      "   3.91925871e-01  2.98554957e-01 -1.11580804e-01  3.31197828e-01\n",
      "   1.76808432e-01 -2.55336463e-01  1.09789208e-01  6.78438306e-01\n",
      "   6.27605319e-02 -6.16012104e-02 -1.91079095e-01 -1.86340794e-01\n",
      "   5.02566211e-02  5.73267460e-01 -3.93331021e-01 -1.36839464e-01\n",
      "  -3.34180593e-01 -6.91046491e-02  1.66049451e-01 -3.67128760e-01\n",
      "  -5.45554638e-01 -2.83265412e-01 -8.87355059e-02 -1.53464288e-01\n",
      "   3.21482494e-02 -4.49454606e-01  3.23594928e-01 -3.04524422e-01\n",
      "  -6.24529235e-02 -1.48338705e-01 -2.94645965e-01  1.02758862e-01\n",
      "  -2.77663972e-02 -1.59768417e-01 -1.91362917e-01  2.62224734e-01\n",
      "  -2.55818516e-01 -3.11551511e-01 -1.86605453e-01  7.09620565e-02\n",
      "  -1.65994659e-01  2.49349266e-01 -1.31043300e-01  4.86724749e-02\n",
      "  -2.84670353e-01  4.55094039e-01  7.26821542e-01 -2.68644184e-01\n",
      "  -3.54816735e-01  4.85965669e-01  2.42982835e-01 -1.51281923e-01\n",
      "  -1.33383095e-01  1.81295767e-01  3.15533370e-01  3.76008153e-02\n",
      "   1.25981405e-01 -4.69311059e-01  3.29818636e-01  1.54262081e-01\n",
      "  -4.42143381e-01  1.85550258e-01  4.60503668e-01 -2.39960119e-01\n",
      "   2.40492553e-01  2.10199147e-01 -2.83194005e-01  6.61097467e-04\n",
      "   1.23950340e-01 -4.16454613e-01 -1.30853221e-01 -6.49896979e-01\n",
      "  -5.64791918e-01  3.32899868e-01  2.03123361e-01 -3.88354063e-04\n",
      "  -1.38105586e-01 -2.05863267e-03  2.34906763e-01  5.01994729e-01\n",
      "   3.64093393e-01  1.69165224e-01 -5.93554862e-02 -1.09114148e-01\n",
      "   3.82529534e-02 -8.29861462e-01  2.72085845e-01 -2.10565567e-01\n",
      "  -1.02890909e-01  1.43581361e-01  2.79628336e-01 -3.24829400e-01\n",
      "  -7.65102580e-02  5.35817862e-01 -1.62376195e-01 -3.88465934e-02\n",
      "  -1.59207910e-01 -1.46823050e-02  8.51192102e-02 -3.60138893e-01\n",
      "  -4.04286161e-02 -1.26875013e-01 -3.71843159e-01 -6.92612529e-01\n",
      "   3.77769917e-01  3.68672580e-01  5.66107988e-01 -3.40746492e-02\n",
      "   1.92986771e-01  5.99353790e-01 -5.11055887e-01  1.07030906e-01\n",
      "   2.74289846e-01  8.39815140e-02 -2.12958410e-01  4.22647968e-02\n",
      "   3.30490798e-01 -1.45351261e-01 -5.54383755e-01  7.27400035e-02\n",
      "  -3.00445825e-01  3.77484232e-01 -2.58035958e-02  1.96821094e-01\n",
      "   5.52675016e-02 -1.25040859e-01  5.05269878e-02 -8.35235417e-02\n",
      "   3.39868724e-01 -4.17065382e-01  4.03910279e-02 -2.44317561e-01\n",
      "   1.07356742e-01 -3.95538092e-01  3.16612661e-01 -4.80681658e-01\n",
      "   1.37501925e-01 -2.95745552e-01 -1.32596761e-01 -3.27217567e-04\n",
      "   3.14221442e-01  3.17523450e-01  3.87254238e-01  5.57496369e-01\n",
      "  -2.33228710e-02 -2.89946850e-02 -1.38014611e-02 -6.53989494e-01\n",
      "   9.42042023e-02 -3.78498197e-01 -3.52117389e-01 -3.10935795e-01\n",
      "  -1.22845054e-01 -5.16671062e-01 -2.00064704e-01  2.60457456e-01\n",
      "   4.41903114e-01  8.54933858e-02 -9.03007388e-02  5.02986670e-01\n",
      "   2.11076136e-03 -5.10823019e-02  3.10165614e-01  5.82528234e-01\n",
      "   1.86313912e-01  6.59858704e-01 -2.80341387e-01 -6.99331751e-03\n",
      "  -1.92157343e-01  3.61472890e-02 -1.76903576e-01  6.95914850e-02\n",
      "   1.40665531e-01  3.60881180e-01 -1.66920736e-01  3.55992854e-01\n",
      "  -4.19237375e-01  4.75794792e-01  4.69290130e-02 -1.37300596e-01\n",
      "   1.11848488e-01 -2.51817200e-02 -1.40645459e-01 -1.47531480e-02\n",
      "  -9.48466659e-02  2.73072720e-01  2.44653560e-02  7.33996916e-04\n",
      "  -2.78816130e-02 -2.12786153e-01 -2.35722214e-01 -1.14705071e-01\n",
      "  -2.47785956e-01  5.51830838e-03 -3.92451793e-01  4.14590351e-02\n",
      "  -2.52511084e-01  1.61845163e-01  2.88358152e-01 -3.10438816e-02\n",
      "   4.64073479e-01 -3.63943189e-01 -1.85232505e-01  3.37846220e-01\n",
      "   3.21000516e-01 -7.20554614e-04 -2.81546405e-03 -3.88722956e-01\n",
      "  -3.62940133e-01  9.64944996e-03  4.13027942e-01 -2.42494941e-01\n",
      "  -2.76208788e-01 -4.04574573e-01 -4.28979009e-01 -5.91866910e-01\n",
      "  -3.76135290e-01 -2.47574657e-01 -2.47372359e-01 -5.82553744e-01\n",
      "   1.54925570e-01 -5.12567401e-01 -4.27423239e-01  2.96844065e-01\n",
      "  -5.12302756e-01 -1.80125445e-01 -4.23329473e-02  7.96196833e-02\n",
      "  -2.76697010e-01 -1.37815803e-01 -3.40058953e-01  8.06401849e-01\n",
      "  -1.25470668e-01  3.59844238e-01 -7.24588484e-02  1.02231815e-01\n",
      "   2.14679435e-01  5.54944992e-01 -3.83526415e-01  2.40727946e-01\n",
      "   6.20963797e-02  2.89308876e-01  7.52661675e-02  2.16998070e-01\n",
      "   1.13193527e-01  2.03761190e-01  1.96365029e-01 -4.42474991e-01\n",
      "  -5.33894658e-01 -8.58926201e+00 -2.92451885e-02 -1.58291817e-01\n",
      "  -3.48334640e-01  6.66174144e-02 -2.74790943e-01  2.12331057e-01\n",
      "  -3.25475663e-01 -2.10324556e-01  7.96175078e-02 -6.68469012e-01\n",
      "   1.12953521e-01  2.46119112e-01 -4.08804357e-01  2.02097774e-01\n",
      "  -1.51490318e-02  3.20530027e-01 -2.04893157e-01  4.80153143e-01\n",
      "  -3.22802126e-01 -2.24535272e-01  3.27471010e-02  2.63910115e-01\n",
      "  -3.32639702e-02  4.02758747e-01  2.92285860e-01 -8.05698782e-02\n",
      "   7.57329047e-01  1.21529952e-01 -2.73500048e-02 -7.79089332e-01\n",
      "  -1.62480310e-01  3.10304135e-01  2.00903058e-01 -3.71160328e-01\n",
      "   2.81329937e-02  3.72705609e-01 -3.64656955e-01  1.09729931e-01\n",
      "  -2.03583866e-01  1.58804864e-01 -2.97934711e-01  6.71295077e-02\n",
      "   1.70671493e-01  2.64702559e-01  2.11844206e-01  2.25563675e-01\n",
      "   3.86612684e-01 -2.69972682e-01  2.83501416e-01  2.62707710e-01\n",
      "   4.97025073e-01 -1.02322683e-01 -5.32655954e-01 -1.62199393e-01\n",
      "   2.47249693e-01 -5.26982665e-01  5.24339378e-01 -9.16092023e-02\n",
      "  -3.24983299e-01 -1.81029290e-02  1.66945383e-01 -1.68297574e-01\n",
      "  -1.58478349e-01 -1.13397874e-01  3.15888613e-01 -3.95438612e-01\n",
      "  -3.03851157e-01 -3.77259031e-02  9.96890664e-02 -4.03561652e-01\n",
      "   2.35031128e-01  3.63694757e-01  4.88871574e-01  1.42041013e-01\n",
      "  -4.67888355e-01 -3.25319350e-01  2.36944959e-01  8.86314213e-01\n",
      "   5.04088163e-01 -7.89883792e-01 -1.72655016e-01 -2.54145209e-02\n",
      "  -3.25324237e-01 -9.64408088e-03 -2.55575001e-01 -1.83248371e-01\n",
      "   3.51095587e-01  2.31729537e-01 -1.20018221e-01  2.18523428e-01\n",
      "  -3.36666912e-01 -1.70587786e-02  2.39727825e-01 -5.07556140e-01\n",
      "  -1.13659620e-01 -2.93289572e-01 -5.70242517e-02 -3.97179902e-01\n",
      "  -2.49347299e-01 -9.01850462e-02  5.02324164e-01  1.40607372e-01\n",
      "  -2.36753225e-02  1.17642954e-02 -2.54781187e-01 -7.96463192e-02\n",
      "  -2.42033213e-01  5.78561723e-02  8.64051431e-02  1.83538586e-01\n",
      "  -5.34611456e-02  1.32681783e-02 -1.74065217e-01  2.94749588e-01\n",
      "   3.33616197e-01  4.05905634e-01  1.93764135e-01  3.22997957e-01\n",
      "  -3.73488967e-03 -1.17702581e-01 -3.95532325e-02 -1.22108258e-01\n",
      "  -6.23045921e-01  1.46333143e-01  1.16850272e-01 -1.34916618e-01\n",
      "  -5.80624938e-02  1.37720361e-01 -2.76611805e-01  4.56170559e-01\n",
      "  -1.68050498e-01 -3.45749050e-01 -4.51550782e-02 -1.52641416e-01\n",
      "   5.17635755e-02  1.62691310e-01  3.19137096e-01 -4.13682640e-01\n",
      "  -3.62632096e-01 -1.27217740e-01  1.70231387e-02  3.86574179e-01\n",
      "   3.09289783e-01  1.02399215e-01  4.32819128e-03 -2.46804982e-01\n",
      "   1.21136591e-01 -7.12259188e-02  2.82539129e-01 -2.01454446e-01\n",
      "   6.93570793e-01  1.80779025e-01 -2.27622673e-01  5.48453815e-02\n",
      "   5.70640445e-01  2.03180939e-01 -2.10896447e-01 -1.70503914e-01\n",
      "  -3.22827691e-04  4.98328537e-01  6.67827189e-01  8.46041813e-02\n",
      "   5.31485260e-01  5.02188563e-01 -3.26934755e-01  1.61333472e-01\n",
      "   5.01764119e-01 -2.35691950e-01 -3.07621688e-01 -3.19027573e-01\n",
      "   1.68065995e-01  1.98655620e-01 -7.21843094e-02  2.99447328e-01\n",
      "  -8.50660168e-03 -1.77459195e-01 -6.97741956e-02 -6.71077380e-03\n",
      "   4.05107811e-02  1.17405519e-01 -3.39172259e-02 -6.57981187e-02\n",
      "  -3.62121135e-01  4.44906890e-01  5.23720622e-01 -1.05864853e-02\n",
      "   1.48474630e-02  9.57417190e-02 -1.07464336e-01 -1.46982104e-01\n",
      "  -8.81997943e-02 -2.85884321e-01 -1.53922975e-01  3.30416024e-01\n",
      "   3.81363034e-02 -2.72856653e-01  2.69623250e-01 -2.28136301e-01\n",
      "   4.03578043e-01 -1.03221692e-01 -8.57222453e-02  9.04977918e-02\n",
      "  -4.45217609e-01 -6.99968994e-01 -2.87689686e-01  4.83651340e-01\n",
      "  -1.54991731e-01  1.25976503e-01 -3.59323084e-01 -5.95465064e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding sequences: 100%|█████████████████████████████████████████████████████████| 2566/2566 [05:25<00:00,  7.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DADP ID     Entry Name Uniprot code                Species  \\\n",
       "0     SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "1       SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "2       SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "3       SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "4       SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...         ...            ...          ...                    ...   \n",
       "1640    SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "1641    SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "1642    SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "1643    SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "1644  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "      Sequence length         signal sequence  \\\n",
       "0                  81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "1                  23                       /   \n",
       "2                  23                       /   \n",
       "3                  23                       /   \n",
       "4                  16                       /   \n",
       "...               ...                     ...   \n",
       "1640               59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "1641               55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "1642               67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "1643               64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "1644               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                     Bioactive sequence  \\\n",
       "0                     GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "1                               GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "2                               GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "3                               GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "4                                      ILGKLLSTAAGLLSNL   \n",
       "...                                                 ...   \n",
       "1640                                      VFLGNIVSMGKKI   \n",
       "1641                                 DAAVEPELYHWGKVWLPN   \n",
       "1642                               CVDIGFSPTGKRPPFCPYPG   \n",
       "1643                                  VADKRPYILREKKSIPY   \n",
       "1644  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                             Properties  \\\n",
       "0     {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "1     {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "2     {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "3     {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "4     {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                 ...   \n",
       "1640  {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "1641  {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "1642  {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "1643  {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "1644  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "               TruncSequence             ModSequence  Value  \n",
       "0     GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "1     GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "2     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "3     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "4           ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                      ...                     ...    ...  \n",
       "1640           VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "1641      DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "1642    CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "1643       VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "1644  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n",
      "Training the neural network...\n",
      "Epoch 1/500, Loss: 0.6865757703781128\n",
      "Epoch 2/500, Loss: 0.6605392694473267\n",
      "Epoch 3/500, Loss: 0.6530036330223083\n",
      "Epoch 4/500, Loss: 0.6509491205215454\n",
      "Epoch 5/500, Loss: 0.648556649684906\n",
      "Epoch 6/500, Loss: 0.6447200775146484\n",
      "Epoch 7/500, Loss: 0.6398990154266357\n",
      "Epoch 8/500, Loss: 0.6351245641708374\n",
      "Epoch 9/500, Loss: 0.6317211389541626\n",
      "Epoch 10/500, Loss: 0.6299514770507812\n",
      "Epoch 11/500, Loss: 0.6290957927703857\n",
      "Epoch 12/500, Loss: 0.62818443775177\n",
      "Epoch 13/500, Loss: 0.6267038583755493\n",
      "Epoch 14/500, Loss: 0.624924898147583\n",
      "Epoch 15/500, Loss: 0.6233696937561035\n",
      "Epoch 16/500, Loss: 0.6224385499954224\n",
      "Epoch 17/500, Loss: 0.6220150589942932\n",
      "Epoch 18/500, Loss: 0.6216003894805908\n",
      "Epoch 19/500, Loss: 0.6208159327507019\n",
      "Epoch 20/500, Loss: 0.6197201013565063\n",
      "Epoch 21/500, Loss: 0.6186395883560181\n",
      "Epoch 22/500, Loss: 0.6178091168403625\n",
      "Epoch 23/500, Loss: 0.6172037720680237\n",
      "Epoch 24/500, Loss: 0.616568386554718\n",
      "Epoch 25/500, Loss: 0.6156913638114929\n",
      "Epoch 26/500, Loss: 0.6145875453948975\n",
      "Epoch 27/500, Loss: 0.6134687662124634\n",
      "Epoch 28/500, Loss: 0.6125226020812988\n",
      "Epoch 29/500, Loss: 0.6116997003555298\n",
      "Epoch 30/500, Loss: 0.610809862613678\n",
      "Epoch 31/500, Loss: 0.6097714304924011\n",
      "Epoch 32/500, Loss: 0.6086669564247131\n",
      "Epoch 33/500, Loss: 0.6076578497886658\n",
      "Epoch 34/500, Loss: 0.6067896485328674\n",
      "Epoch 35/500, Loss: 0.6059290170669556\n",
      "Epoch 36/500, Loss: 0.6049290895462036\n",
      "Epoch 37/500, Loss: 0.6039242744445801\n",
      "Epoch 38/500, Loss: 0.6030504107475281\n",
      "Epoch 39/500, Loss: 0.6022434830665588\n",
      "Epoch 40/500, Loss: 0.6013628244400024\n",
      "Epoch 41/500, Loss: 0.6003851890563965\n",
      "Epoch 42/500, Loss: 0.5994414687156677\n",
      "Epoch 43/500, Loss: 0.5985317826271057\n",
      "Epoch 44/500, Loss: 0.5975731611251831\n",
      "Epoch 45/500, Loss: 0.5965269207954407\n",
      "Epoch 46/500, Loss: 0.5954941511154175\n",
      "Epoch 47/500, Loss: 0.5944978594779968\n",
      "Epoch 48/500, Loss: 0.5934503078460693\n",
      "Epoch 49/500, Loss: 0.5923763513565063\n",
      "Epoch 50/500, Loss: 0.5913364887237549\n",
      "Epoch 51/500, Loss: 0.5902963280677795\n",
      "Epoch 52/500, Loss: 0.5892143845558167\n",
      "Epoch 53/500, Loss: 0.5881437063217163\n",
      "Epoch 54/500, Loss: 0.5870794057846069\n",
      "Epoch 55/500, Loss: 0.5859771370887756\n",
      "Epoch 56/500, Loss: 0.5848788619041443\n",
      "Epoch 57/500, Loss: 0.5838051438331604\n",
      "Epoch 58/500, Loss: 0.5827259421348572\n",
      "Epoch 59/500, Loss: 0.5816566944122314\n",
      "Epoch 60/500, Loss: 0.5805918574333191\n",
      "Epoch 61/500, Loss: 0.579524040222168\n",
      "Epoch 62/500, Loss: 0.5784599781036377\n",
      "Epoch 63/500, Loss: 0.577402651309967\n",
      "Epoch 64/500, Loss: 0.5763437747955322\n",
      "Epoch 65/500, Loss: 0.5752764940261841\n",
      "Epoch 66/500, Loss: 0.5742194056510925\n",
      "Epoch 67/500, Loss: 0.5731762051582336\n",
      "Epoch 68/500, Loss: 0.5721400380134583\n",
      "Epoch 69/500, Loss: 0.5710896849632263\n",
      "Epoch 70/500, Loss: 0.5700438618659973\n",
      "Epoch 71/500, Loss: 0.569014847278595\n",
      "Epoch 72/500, Loss: 0.5680005550384521\n",
      "Epoch 73/500, Loss: 0.5669960379600525\n",
      "Epoch 74/500, Loss: 0.5659874081611633\n",
      "Epoch 75/500, Loss: 0.5649915933609009\n",
      "Epoch 76/500, Loss: 0.5639943480491638\n",
      "Epoch 77/500, Loss: 0.5630105137825012\n",
      "Epoch 78/500, Loss: 0.5620436668395996\n",
      "Epoch 79/500, Loss: 0.5610629916191101\n",
      "Epoch 80/500, Loss: 0.5600907802581787\n",
      "Epoch 81/500, Loss: 0.5591393113136292\n",
      "Epoch 82/500, Loss: 0.5581722259521484\n",
      "Epoch 83/500, Loss: 0.5571922063827515\n",
      "Epoch 84/500, Loss: 0.5562252402305603\n",
      "Epoch 85/500, Loss: 0.5552640557289124\n",
      "Epoch 86/500, Loss: 0.5543004870414734\n",
      "Epoch 87/500, Loss: 0.5533397197723389\n",
      "Epoch 88/500, Loss: 0.5523744821548462\n",
      "Epoch 89/500, Loss: 0.5514152646064758\n",
      "Epoch 90/500, Loss: 0.5504569411277771\n",
      "Epoch 91/500, Loss: 0.549497127532959\n",
      "Epoch 92/500, Loss: 0.5485345125198364\n",
      "Epoch 93/500, Loss: 0.5475808382034302\n",
      "Epoch 94/500, Loss: 0.5466295480728149\n",
      "Epoch 95/500, Loss: 0.5456695556640625\n",
      "Epoch 96/500, Loss: 0.5447357296943665\n",
      "Epoch 97/500, Loss: 0.5437899231910706\n",
      "Epoch 98/500, Loss: 0.5428406596183777\n",
      "Epoch 99/500, Loss: 0.5419066548347473\n",
      "Epoch 100/500, Loss: 0.5409919619560242\n",
      "Epoch 101/500, Loss: 0.5400552153587341\n",
      "Epoch 102/500, Loss: 0.5391484498977661\n",
      "Epoch 103/500, Loss: 0.5382400155067444\n",
      "Epoch 104/500, Loss: 0.5373308658599854\n",
      "Epoch 105/500, Loss: 0.5364528298377991\n",
      "Epoch 106/500, Loss: 0.5355747938156128\n",
      "Epoch 107/500, Loss: 0.5346899032592773\n",
      "Epoch 108/500, Loss: 0.5338142514228821\n",
      "Epoch 109/500, Loss: 0.532959520816803\n",
      "Epoch 110/500, Loss: 0.5321040749549866\n",
      "Epoch 111/500, Loss: 0.5312401056289673\n",
      "Epoch 112/500, Loss: 0.5303654074668884\n",
      "Epoch 113/500, Loss: 0.5295416712760925\n",
      "Epoch 114/500, Loss: 0.5287230610847473\n",
      "Epoch 115/500, Loss: 0.5278613567352295\n",
      "Epoch 116/500, Loss: 0.5270359516143799\n",
      "Epoch 117/500, Loss: 0.5262773633003235\n",
      "Epoch 118/500, Loss: 0.5255985260009766\n",
      "Epoch 119/500, Loss: 0.5250298976898193\n",
      "Epoch 120/500, Loss: 0.5244097113609314\n",
      "Epoch 121/500, Loss: 0.5234457850456238\n",
      "Epoch 122/500, Loss: 0.5223418474197388\n",
      "Epoch 123/500, Loss: 0.52167809009552\n",
      "Epoch 124/500, Loss: 0.5211782455444336\n",
      "Epoch 125/500, Loss: 0.5204117894172668\n",
      "Epoch 126/500, Loss: 0.5194648504257202\n",
      "Epoch 127/500, Loss: 0.5187123417854309\n",
      "Epoch 128/500, Loss: 0.5181617140769958\n",
      "Epoch 129/500, Loss: 0.5175876617431641\n",
      "Epoch 130/500, Loss: 0.5167468786239624\n",
      "Epoch 131/500, Loss: 0.5158936977386475\n",
      "Epoch 132/500, Loss: 0.515303373336792\n",
      "Epoch 133/500, Loss: 0.5147708058357239\n",
      "Epoch 134/500, Loss: 0.5140348672866821\n",
      "Epoch 135/500, Loss: 0.5132228136062622\n",
      "Epoch 136/500, Loss: 0.5125610828399658\n",
      "Epoch 137/500, Loss: 0.5119971036911011\n",
      "Epoch 138/500, Loss: 0.5113852620124817\n",
      "Epoch 139/500, Loss: 0.5106721520423889\n",
      "Epoch 140/500, Loss: 0.5099361538887024\n",
      "Epoch 141/500, Loss: 0.5093256831169128\n",
      "Epoch 142/500, Loss: 0.508767306804657\n",
      "Epoch 143/500, Loss: 0.5081851482391357\n",
      "Epoch 144/500, Loss: 0.5076073408126831\n",
      "Epoch 145/500, Loss: 0.506934404373169\n",
      "Epoch 146/500, Loss: 0.5062525868415833\n",
      "Epoch 147/500, Loss: 0.5055943131446838\n",
      "Epoch 148/500, Loss: 0.5049570798873901\n",
      "Epoch 149/500, Loss: 0.5043333768844604\n",
      "Epoch 150/500, Loss: 0.5037543773651123\n",
      "Epoch 151/500, Loss: 0.5031648874282837\n",
      "Epoch 152/500, Loss: 0.5026019215583801\n",
      "Epoch 153/500, Loss: 0.5020703077316284\n",
      "Epoch 154/500, Loss: 0.5016130805015564\n",
      "Epoch 155/500, Loss: 0.501228392124176\n",
      "Epoch 156/500, Loss: 0.5008909702301025\n",
      "Epoch 157/500, Loss: 0.5003580451011658\n",
      "Epoch 158/500, Loss: 0.49952033162117004\n",
      "Epoch 159/500, Loss: 0.49865564703941345\n",
      "Epoch 160/500, Loss: 0.49800458550453186\n",
      "Epoch 161/500, Loss: 0.4975873529911041\n",
      "Epoch 162/500, Loss: 0.4972684979438782\n",
      "Epoch 163/500, Loss: 0.49678120017051697\n",
      "Epoch 164/500, Loss: 0.4961310625076294\n",
      "Epoch 165/500, Loss: 0.4953969120979309\n",
      "Epoch 166/500, Loss: 0.4947968125343323\n",
      "Epoch 167/500, Loss: 0.4943227767944336\n",
      "Epoch 168/500, Loss: 0.49391791224479675\n",
      "Epoch 169/500, Loss: 0.49351274967193604\n",
      "Epoch 170/500, Loss: 0.4930412173271179\n",
      "Epoch 171/500, Loss: 0.4924819767475128\n",
      "Epoch 172/500, Loss: 0.49183785915374756\n",
      "Epoch 173/500, Loss: 0.49124693870544434\n",
      "Epoch 174/500, Loss: 0.4906735420227051\n",
      "Epoch 175/500, Loss: 0.49015897512435913\n",
      "Epoch 176/500, Loss: 0.4897248446941376\n",
      "Epoch 177/500, Loss: 0.48931777477264404\n",
      "Epoch 178/500, Loss: 0.48898953199386597\n",
      "Epoch 179/500, Loss: 0.48865339159965515\n",
      "Epoch 180/500, Loss: 0.488299697637558\n",
      "Epoch 181/500, Loss: 0.4877901077270508\n",
      "Epoch 182/500, Loss: 0.4871131181716919\n",
      "Epoch 183/500, Loss: 0.48638707399368286\n",
      "Epoch 184/500, Loss: 0.48578929901123047\n",
      "Epoch 185/500, Loss: 0.48539915680885315\n",
      "Epoch 186/500, Loss: 0.4852205812931061\n",
      "Epoch 187/500, Loss: 0.4850214719772339\n",
      "Epoch 188/500, Loss: 0.48457369208335876\n",
      "Epoch 189/500, Loss: 0.48388928174972534\n",
      "Epoch 190/500, Loss: 0.4831618368625641\n",
      "Epoch 191/500, Loss: 0.48260703682899475\n",
      "Epoch 192/500, Loss: 0.4822138547897339\n",
      "Epoch 193/500, Loss: 0.48187536001205444\n",
      "Epoch 194/500, Loss: 0.4815288484096527\n",
      "Epoch 195/500, Loss: 0.481116384267807\n",
      "Epoch 196/500, Loss: 0.4806655943393707\n",
      "Epoch 197/500, Loss: 0.4801226258277893\n",
      "Epoch 198/500, Loss: 0.47956088185310364\n",
      "Epoch 199/500, Loss: 0.479072242975235\n",
      "Epoch 200/500, Loss: 0.4786427617073059\n",
      "Epoch 201/500, Loss: 0.4782290458679199\n",
      "Epoch 202/500, Loss: 0.4778469204902649\n",
      "Epoch 203/500, Loss: 0.47751912474632263\n",
      "Epoch 204/500, Loss: 0.47725924849510193\n",
      "Epoch 205/500, Loss: 0.47698161005973816\n",
      "Epoch 206/500, Loss: 0.47666046023368835\n",
      "Epoch 207/500, Loss: 0.47628968954086304\n",
      "Epoch 208/500, Loss: 0.47576382756233215\n",
      "Epoch 209/500, Loss: 0.47513964772224426\n",
      "Epoch 210/500, Loss: 0.4745652973651886\n",
      "Epoch 211/500, Loss: 0.4740820527076721\n",
      "Epoch 212/500, Loss: 0.4736537039279938\n",
      "Epoch 213/500, Loss: 0.47329258918762207\n",
      "Epoch 214/500, Loss: 0.47294318675994873\n",
      "Epoch 215/500, Loss: 0.47259601950645447\n",
      "Epoch 216/500, Loss: 0.47230151295661926\n",
      "Epoch 217/500, Loss: 0.47207751870155334\n",
      "Epoch 218/500, Loss: 0.47184666991233826\n",
      "Epoch 219/500, Loss: 0.4715888202190399\n",
      "Epoch 220/500, Loss: 0.4712524116039276\n",
      "Epoch 221/500, Loss: 0.47072815895080566\n",
      "Epoch 222/500, Loss: 0.47007134556770325\n",
      "Epoch 223/500, Loss: 0.46949297189712524\n",
      "Epoch 224/500, Loss: 0.46901842951774597\n",
      "Epoch 225/500, Loss: 0.4687063992023468\n",
      "Epoch 226/500, Loss: 0.4684785008430481\n",
      "Epoch 227/500, Loss: 0.46826913952827454\n",
      "Epoch 228/500, Loss: 0.4680795669555664\n",
      "Epoch 229/500, Loss: 0.46774983406066895\n",
      "Epoch 230/500, Loss: 0.4673275649547577\n",
      "Epoch 231/500, Loss: 0.46681517362594604\n",
      "Epoch 232/500, Loss: 0.46624812483787537\n",
      "Epoch 233/500, Loss: 0.46580594778060913\n",
      "Epoch 234/500, Loss: 0.46544086933135986\n",
      "Epoch 235/500, Loss: 0.4651460647583008\n",
      "Epoch 236/500, Loss: 0.4648931622505188\n",
      "Epoch 237/500, Loss: 0.46467944979667664\n",
      "Epoch 238/500, Loss: 0.4644830524921417\n",
      "Epoch 239/500, Loss: 0.46419841051101685\n",
      "Epoch 240/500, Loss: 0.4638527035713196\n",
      "Epoch 241/500, Loss: 0.4634196162223816\n",
      "Epoch 242/500, Loss: 0.46292823553085327\n",
      "Epoch 243/500, Loss: 0.46245741844177246\n",
      "Epoch 244/500, Loss: 0.4620567262172699\n",
      "Epoch 245/500, Loss: 0.4617059528827667\n",
      "Epoch 246/500, Loss: 0.4614214599132538\n",
      "Epoch 247/500, Loss: 0.4611867666244507\n",
      "Epoch 248/500, Loss: 0.46096548438072205\n",
      "Epoch 249/500, Loss: 0.46079352498054504\n",
      "Epoch 250/500, Loss: 0.46068960428237915\n",
      "Epoch 251/500, Loss: 0.4604664742946625\n",
      "Epoch 252/500, Loss: 0.4601014256477356\n",
      "Epoch 253/500, Loss: 0.4596460163593292\n",
      "Epoch 254/500, Loss: 0.4590507745742798\n",
      "Epoch 255/500, Loss: 0.45853787660598755\n",
      "Epoch 256/500, Loss: 0.4581778347492218\n",
      "Epoch 257/500, Loss: 0.4579029977321625\n",
      "Epoch 258/500, Loss: 0.45770183205604553\n",
      "Epoch 259/500, Loss: 0.45759642124176025\n",
      "Epoch 260/500, Loss: 0.4574596881866455\n",
      "Epoch 261/500, Loss: 0.4572911858558655\n",
      "Epoch 262/500, Loss: 0.45697060227394104\n",
      "Epoch 263/500, Loss: 0.45654377341270447\n",
      "Epoch 264/500, Loss: 0.4560427963733673\n",
      "Epoch 265/500, Loss: 0.4555513560771942\n",
      "Epoch 266/500, Loss: 0.45515280961990356\n",
      "Epoch 267/500, Loss: 0.4548531770706177\n",
      "Epoch 268/500, Loss: 0.4546200931072235\n",
      "Epoch 269/500, Loss: 0.4544205367565155\n",
      "Epoch 270/500, Loss: 0.4542395770549774\n",
      "Epoch 271/500, Loss: 0.4540351629257202\n",
      "Epoch 272/500, Loss: 0.45379894971847534\n",
      "Epoch 273/500, Loss: 0.45355314016342163\n",
      "Epoch 274/500, Loss: 0.45323851704597473\n",
      "Epoch 275/500, Loss: 0.45286279916763306\n",
      "Epoch 276/500, Loss: 0.45249372720718384\n",
      "Epoch 277/500, Loss: 0.4521052837371826\n",
      "Epoch 278/500, Loss: 0.4517488479614258\n",
      "Epoch 279/500, Loss: 0.45147934556007385\n",
      "Epoch 280/500, Loss: 0.4511871039867401\n",
      "Epoch 281/500, Loss: 0.4508744180202484\n",
      "Epoch 282/500, Loss: 0.45063528418540955\n",
      "Epoch 283/500, Loss: 0.45038220286369324\n",
      "Epoch 284/500, Loss: 0.4501168429851532\n",
      "Epoch 285/500, Loss: 0.4498726427555084\n",
      "Epoch 286/500, Loss: 0.44967302680015564\n",
      "Epoch 287/500, Loss: 0.4495377838611603\n",
      "Epoch 288/500, Loss: 0.4494970440864563\n",
      "Epoch 289/500, Loss: 0.4496300518512726\n",
      "Epoch 290/500, Loss: 0.4498908519744873\n",
      "Epoch 291/500, Loss: 0.4500432312488556\n",
      "Epoch 292/500, Loss: 0.4496396780014038\n",
      "Epoch 293/500, Loss: 0.4486418664455414\n",
      "Epoch 294/500, Loss: 0.44767266511917114\n",
      "Epoch 295/500, Loss: 0.44735682010650635\n",
      "Epoch 296/500, Loss: 0.44766443967819214\n",
      "Epoch 297/500, Loss: 0.44796478748321533\n",
      "Epoch 298/500, Loss: 0.4476505219936371\n",
      "Epoch 299/500, Loss: 0.4468156397342682\n",
      "Epoch 300/500, Loss: 0.4461880028247833\n",
      "Epoch 301/500, Loss: 0.4460943639278412\n",
      "Epoch 302/500, Loss: 0.44624486565589905\n",
      "Epoch 303/500, Loss: 0.4461362659931183\n",
      "Epoch 304/500, Loss: 0.4456635117530823\n",
      "Epoch 305/500, Loss: 0.4451003670692444\n",
      "Epoch 306/500, Loss: 0.44482356309890747\n",
      "Epoch 307/500, Loss: 0.4448672831058502\n",
      "Epoch 308/500, Loss: 0.44484198093414307\n",
      "Epoch 309/500, Loss: 0.44454270601272583\n",
      "Epoch 310/500, Loss: 0.4440667927265167\n",
      "Epoch 311/500, Loss: 0.44372910261154175\n",
      "Epoch 312/500, Loss: 0.44361844658851624\n",
      "Epoch 313/500, Loss: 0.4435870051383972\n",
      "Epoch 314/500, Loss: 0.44345542788505554\n",
      "Epoch 315/500, Loss: 0.4431142508983612\n",
      "Epoch 316/500, Loss: 0.4427543878555298\n",
      "Epoch 317/500, Loss: 0.44245532155036926\n",
      "Epoch 318/500, Loss: 0.44229283928871155\n",
      "Epoch 319/500, Loss: 0.4422154128551483\n",
      "Epoch 320/500, Loss: 0.4420613646507263\n",
      "Epoch 321/500, Loss: 0.44186848402023315\n",
      "Epoch 322/500, Loss: 0.4415999948978424\n",
      "Epoch 323/500, Loss: 0.4412723481655121\n",
      "Epoch 324/500, Loss: 0.4410281777381897\n",
      "Epoch 325/500, Loss: 0.44086048007011414\n",
      "Epoch 326/500, Loss: 0.4407309889793396\n",
      "Epoch 327/500, Loss: 0.44065147638320923\n",
      "Epoch 328/500, Loss: 0.44050663709640503\n",
      "Epoch 329/500, Loss: 0.44028252363204956\n",
      "Epoch 330/500, Loss: 0.4400203227996826\n",
      "Epoch 331/500, Loss: 0.43973633646965027\n",
      "Epoch 332/500, Loss: 0.43948253989219666\n",
      "Epoch 333/500, Loss: 0.4392824172973633\n",
      "Epoch 334/500, Loss: 0.43909886479377747\n",
      "Epoch 335/500, Loss: 0.43898048996925354\n",
      "Epoch 336/500, Loss: 0.43885931372642517\n",
      "Epoch 337/500, Loss: 0.43869638442993164\n",
      "Epoch 338/500, Loss: 0.4385449290275574\n",
      "Epoch 339/500, Loss: 0.43833163380622864\n",
      "Epoch 340/500, Loss: 0.43808114528656006\n",
      "Epoch 341/500, Loss: 0.43785107135772705\n",
      "Epoch 342/500, Loss: 0.43760061264038086\n",
      "Epoch 343/500, Loss: 0.43739059567451477\n",
      "Epoch 344/500, Loss: 0.4372025430202484\n",
      "Epoch 345/500, Loss: 0.4370080232620239\n",
      "Epoch 346/500, Loss: 0.43682780861854553\n",
      "Epoch 347/500, Loss: 0.43668001890182495\n",
      "Epoch 348/500, Loss: 0.43654176592826843\n",
      "Epoch 349/500, Loss: 0.43642866611480713\n",
      "Epoch 350/500, Loss: 0.4363565146923065\n",
      "Epoch 351/500, Loss: 0.43629828095436096\n",
      "Epoch 352/500, Loss: 0.4362157881259918\n",
      "Epoch 353/500, Loss: 0.4360654056072235\n",
      "Epoch 354/500, Loss: 0.4358830451965332\n",
      "Epoch 355/500, Loss: 0.43553924560546875\n",
      "Epoch 356/500, Loss: 0.4351634681224823\n",
      "Epoch 357/500, Loss: 0.4349052608013153\n",
      "Epoch 358/500, Loss: 0.4346665143966675\n",
      "Epoch 359/500, Loss: 0.43448522686958313\n",
      "Epoch 360/500, Loss: 0.4343414902687073\n",
      "Epoch 361/500, Loss: 0.4342239201068878\n",
      "Epoch 362/500, Loss: 0.43407729268074036\n",
      "Epoch 363/500, Loss: 0.43397486209869385\n",
      "Epoch 364/500, Loss: 0.43394768238067627\n",
      "Epoch 365/500, Loss: 0.4338889718055725\n",
      "Epoch 366/500, Loss: 0.4337855279445648\n",
      "Epoch 367/500, Loss: 0.433639258146286\n",
      "Epoch 368/500, Loss: 0.4333883225917816\n",
      "Epoch 369/500, Loss: 0.4330359101295471\n",
      "Epoch 370/500, Loss: 0.43268004059791565\n",
      "Epoch 371/500, Loss: 0.4323962926864624\n",
      "Epoch 372/500, Loss: 0.43222522735595703\n",
      "Epoch 373/500, Loss: 0.43212172389030457\n",
      "Epoch 374/500, Loss: 0.4320557415485382\n",
      "Epoch 375/500, Loss: 0.4320714771747589\n",
      "Epoch 376/500, Loss: 0.4320821166038513\n",
      "Epoch 377/500, Loss: 0.43194580078125\n",
      "Epoch 378/500, Loss: 0.43173691630363464\n",
      "Epoch 379/500, Loss: 0.4314257800579071\n",
      "Epoch 380/500, Loss: 0.4310499429702759\n",
      "Epoch 381/500, Loss: 0.43075716495513916\n",
      "Epoch 382/500, Loss: 0.4305790364742279\n",
      "Epoch 383/500, Loss: 0.43044400215148926\n",
      "Epoch 384/500, Loss: 0.43038102984428406\n",
      "Epoch 385/500, Loss: 0.4303602874279022\n",
      "Epoch 386/500, Loss: 0.4303014874458313\n",
      "Epoch 387/500, Loss: 0.43017876148223877\n",
      "Epoch 388/500, Loss: 0.43000274896621704\n",
      "Epoch 389/500, Loss: 0.42979320883750916\n",
      "Epoch 390/500, Loss: 0.4295291006565094\n",
      "Epoch 391/500, Loss: 0.42926594614982605\n",
      "Epoch 392/500, Loss: 0.4290710687637329\n",
      "Epoch 393/500, Loss: 0.4288920760154724\n",
      "Epoch 394/500, Loss: 0.42870262265205383\n",
      "Epoch 395/500, Loss: 0.4285576641559601\n",
      "Epoch 396/500, Loss: 0.428435355424881\n",
      "Epoch 397/500, Loss: 0.4282975494861603\n",
      "Epoch 398/500, Loss: 0.42817461490631104\n",
      "Epoch 399/500, Loss: 0.4280693233013153\n",
      "Epoch 400/500, Loss: 0.4279966354370117\n",
      "Epoch 401/500, Loss: 0.42795586585998535\n",
      "Epoch 402/500, Loss: 0.42796576023101807\n",
      "Epoch 403/500, Loss: 0.4279785454273224\n",
      "Epoch 404/500, Loss: 0.42793574929237366\n",
      "Epoch 405/500, Loss: 0.42768701910972595\n",
      "Epoch 406/500, Loss: 0.42738330364227295\n",
      "Epoch 407/500, Loss: 0.4270244836807251\n",
      "Epoch 408/500, Loss: 0.4266960024833679\n",
      "Epoch 409/500, Loss: 0.4265718162059784\n",
      "Epoch 410/500, Loss: 0.42650511860847473\n",
      "Epoch 411/500, Loss: 0.4264586865901947\n",
      "Epoch 412/500, Loss: 0.4264638125896454\n",
      "Epoch 413/500, Loss: 0.4265029728412628\n",
      "Epoch 414/500, Loss: 0.42639923095703125\n",
      "Epoch 415/500, Loss: 0.42615070939064026\n",
      "Epoch 416/500, Loss: 0.4258299767971039\n",
      "Epoch 417/500, Loss: 0.4255419671535492\n",
      "Epoch 418/500, Loss: 0.425322026014328\n",
      "Epoch 419/500, Loss: 0.4251668155193329\n",
      "Epoch 420/500, Loss: 0.42509257793426514\n",
      "Epoch 421/500, Loss: 0.4250773787498474\n",
      "Epoch 422/500, Loss: 0.425063818693161\n",
      "Epoch 423/500, Loss: 0.4249449074268341\n",
      "Epoch 424/500, Loss: 0.42474398016929626\n",
      "Epoch 425/500, Loss: 0.4244881868362427\n",
      "Epoch 426/500, Loss: 0.4242362380027771\n",
      "Epoch 427/500, Loss: 0.42406636476516724\n",
      "Epoch 428/500, Loss: 0.42386141419410706\n",
      "Epoch 429/500, Loss: 0.42370176315307617\n",
      "Epoch 430/500, Loss: 0.42355889081954956\n",
      "Epoch 431/500, Loss: 0.423412948846817\n",
      "Epoch 432/500, Loss: 0.42330271005630493\n",
      "Epoch 433/500, Loss: 0.4232156276702881\n",
      "Epoch 434/500, Loss: 0.42317426204681396\n",
      "Epoch 435/500, Loss: 0.4231330454349518\n",
      "Epoch 436/500, Loss: 0.42314377427101135\n",
      "Epoch 437/500, Loss: 0.42317673563957214\n",
      "Epoch 438/500, Loss: 0.422976016998291\n",
      "Epoch 439/500, Loss: 0.4227023422718048\n",
      "Epoch 440/500, Loss: 0.4224035143852234\n",
      "Epoch 441/500, Loss: 0.42208877205848694\n",
      "Epoch 442/500, Loss: 0.42183810472488403\n",
      "Epoch 443/500, Loss: 0.42170241475105286\n",
      "Epoch 444/500, Loss: 0.4215855598449707\n",
      "Epoch 445/500, Loss: 0.4215441048145294\n",
      "Epoch 446/500, Loss: 0.42157071828842163\n",
      "Epoch 447/500, Loss: 0.42156046628952026\n",
      "Epoch 448/500, Loss: 0.42152225971221924\n",
      "Epoch 449/500, Loss: 0.4213123023509979\n",
      "Epoch 450/500, Loss: 0.4209989905357361\n",
      "Epoch 451/500, Loss: 0.42072775959968567\n",
      "Epoch 452/500, Loss: 0.42052099108695984\n",
      "Epoch 453/500, Loss: 0.42036768794059753\n",
      "Epoch 454/500, Loss: 0.42028287053108215\n",
      "Epoch 455/500, Loss: 0.4202174246311188\n",
      "Epoch 456/500, Loss: 0.4201640486717224\n",
      "Epoch 457/500, Loss: 0.4201313555240631\n",
      "Epoch 458/500, Loss: 0.42007195949554443\n",
      "Epoch 459/500, Loss: 0.41998350620269775\n",
      "Epoch 460/500, Loss: 0.4197777211666107\n",
      "Epoch 461/500, Loss: 0.41953596472740173\n",
      "Epoch 462/500, Loss: 0.4193072021007538\n",
      "Epoch 463/500, Loss: 0.419103741645813\n",
      "Epoch 464/500, Loss: 0.4189571142196655\n",
      "Epoch 465/500, Loss: 0.4188403785228729\n",
      "Epoch 466/500, Loss: 0.41875332593917847\n",
      "Epoch 467/500, Loss: 0.41868504881858826\n",
      "Epoch 468/500, Loss: 0.4186546802520752\n",
      "Epoch 469/500, Loss: 0.4186057150363922\n",
      "Epoch 470/500, Loss: 0.4185431897640228\n",
      "Epoch 471/500, Loss: 0.4184201657772064\n",
      "Epoch 472/500, Loss: 0.41819679737091064\n",
      "Epoch 473/500, Loss: 0.41796109080314636\n",
      "Epoch 474/500, Loss: 0.4177384078502655\n",
      "Epoch 475/500, Loss: 0.4175381660461426\n",
      "Epoch 476/500, Loss: 0.4174157381057739\n",
      "Epoch 477/500, Loss: 0.4173097610473633\n",
      "Epoch 478/500, Loss: 0.41720861196517944\n",
      "Epoch 479/500, Loss: 0.41713064908981323\n",
      "Epoch 480/500, Loss: 0.41703861951828003\n",
      "Epoch 481/500, Loss: 0.4169471263885498\n",
      "Epoch 482/500, Loss: 0.41685181856155396\n",
      "Epoch 483/500, Loss: 0.41675305366516113\n",
      "Epoch 484/500, Loss: 0.41663649678230286\n",
      "Epoch 485/500, Loss: 0.41649797558784485\n",
      "Epoch 486/500, Loss: 0.41633141040802\n",
      "Epoch 487/500, Loss: 0.41617047786712646\n",
      "Epoch 488/500, Loss: 0.41600361466407776\n",
      "Epoch 489/500, Loss: 0.4158153533935547\n",
      "Epoch 490/500, Loss: 0.4156529903411865\n",
      "Epoch 491/500, Loss: 0.4155195653438568\n",
      "Epoch 492/500, Loss: 0.41540148854255676\n",
      "Epoch 493/500, Loss: 0.4152795672416687\n",
      "Epoch 494/500, Loss: 0.41517239809036255\n",
      "Epoch 495/500, Loss: 0.41506195068359375\n",
      "Epoch 496/500, Loss: 0.4149389863014221\n",
      "Epoch 497/500, Loss: 0.4148535430431366\n",
      "Epoch 498/500, Loss: 0.41477471590042114\n",
      "Epoch 499/500, Loss: 0.41470417380332947\n",
      "Epoch 500/500, Loss: 0.41467320919036865\n",
      "Evaluating the model...\n",
      "Test Accuracy: 77.04%\n",
      "Precision: 0.67\n",
      "Recall: 0.64\n",
      "F1 Score: 0.65\n",
      "Confusion Matrix:\n",
      "[[285  55]\n",
      " [ 63 111]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       340\n",
      "           1       0.67      0.64      0.65       174\n",
      "\n",
      "    accuracy                           0.77       514\n",
      "   macro avg       0.74      0.74      0.74       514\n",
      "weighted avg       0.77      0.77      0.77       514\n",
      "\n",
      "Predicting for sequence: MFTLKKSMLLLFFLGTISLSLC\n",
      "Prediction: Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming posDF and negDF are already defined and concatenated into allDataDF\n",
    "allDataDF = pd.concat([posDF, negDF])\n",
    "display(allDataDF)\n",
    "\n",
    "# Define the neural network\n",
    "class BioBERTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BioBERTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "def get_bert_embeddings(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "sequences = allDataDF['Bioactive sequence'].tolist()\n",
    "labels = allDataDF['Value'].tolist()\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Get embeddings for each sequence\n",
    "print(\"Generating embeddings for sequences...\")\n",
    "embeddings = []\n",
    "for seq in tqdm(sequences, desc=\"Embedding sequences\"):\n",
    "    embedding = get_bert_embeddings(seq, tokenizer, bert_model)\n",
    "    embeddings.append(embedding)\n",
    "    # Print features for the first sequence\n",
    "    if len(embeddings) <= 1:\n",
    "        print(f\"Sequence: {seq}\")\n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"Embedding: {embedding}\")\n",
    "\n",
    "allDataDF2['Bert'] = embeddings\n",
    "display(allDataDF)\n",
    "\n",
    "X = np.array(embeddings).squeeze()\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Number of classes\n",
    "\n",
    "classifier = BioBERTClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "print(\"Training the neural network...\")\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "    precision = precision_score(y_test, predicted.numpy())\n",
    "    recall = recall_score(y_test, predicted.numpy())\n",
    "    f1 = f1_score(y_test, predicted.numpy())\n",
    "    roc_auc=roc_auc_score(y_test, predicted.numpy())\n",
    "    confusion_mat = confusion_matrix(y_test, predicted.numpy())\n",
    "    class_report = classification_report(y_test, predicted.numpy())\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method3 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "\n",
    "# Function to predict antimicrobial property using BioBERT embeddings\n",
    "def predict_method(sequence, classifier, tokenizer, bert_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    embedding = get_bert_embeddings(sequence, tokenizer, bert_model)\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier(embedding_tensor)\n",
    "        _, prediction = torch.max(output.data, 1)\n",
    "        result = 'Antimicrobial' if prediction.item() == 1 else 'Non-antimicrobial'\n",
    "    return result, embedding\n",
    "\n",
    "sequence = \"MFTLKKSMLLLFFLGTISLSLC\"\n",
    "prediction, embed = predict_method(sequence, classifier, tokenizer, bert_model)\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c4bde-833f-49f0-bf6c-da6dad7fb3fc",
   "metadata": {},
   "source": [
    "**Word2Vec Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec9addb-a389-4502-a34f-c16085bb31e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing data...\n",
      "Processing sequences for Word2Vec...\n",
      "Training Word2Vec model...\n",
      "Generating sequence vectors...\n",
      "Preparing data for classification...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>[-0.086681925, 0.0037621346, 0.15957397, 0.302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>[-0.097797126, 0.021844227, 0.12933321, 0.2740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>[-0.09720402, 0.021139316, 0.13208055, 0.27677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>[-0.0994291, 0.025758494, 0.12514107, 0.271076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>[-0.08457472, 0.029784856, 0.15422323, 0.29593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>[-0.11766319, 0.02835577, 0.09145752, 0.243053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>[-0.13313583, -0.02140558, 0.065607786, 0.2281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>[-0.18018624, -0.014238949, -0.049044594, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>[-0.15369546, -0.028014425, 0.01341528, 0.1866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>[-0.114356086, -0.0051107393, 0.112075634, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DADP ID     Entry Name Uniprot code                Species  \\\n",
       "DADP ID                                                                   \n",
       "SP_P31107  SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "SP_2643      SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "SP_2644      SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "SP_2645      SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "SP_2646      SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...              ...            ...          ...                    ...   \n",
       "SP_2852      SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "SP_2853      SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "SP_2854      SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "SP_2855      SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "SP_Q09022  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "           Sequence length         signal sequence  \\\n",
       "DADP ID                                              \n",
       "SP_P31107               81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "SP_2643                 23                       /   \n",
       "SP_2644                 23                       /   \n",
       "SP_2645                 23                       /   \n",
       "SP_2646                 16                       /   \n",
       "...                    ...                     ...   \n",
       "SP_2852                 59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "SP_2853                 55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "SP_2854                 67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "SP_2855                 64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "SP_Q09022               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                          Bioactive sequence  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107                  GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "SP_2643                              GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2644                              GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2645                              GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "SP_2646                                     ILGKLLSTAAGLLSNL   \n",
       "...                                                      ...   \n",
       "SP_2852                                        VFLGNIVSMGKKI   \n",
       "SP_2853                                   DAAVEPELYHWGKVWLPN   \n",
       "SP_2854                                 CVDIGFSPTGKRPPFCPYPG   \n",
       "SP_2855                                    VADKRPYILREKKSIPY   \n",
       "SP_Q09022  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.086681925, 0.0037621346, 0.15957397, 0.302...  \n",
       "SP_2643    [-0.097797126, 0.021844227, 0.12933321, 0.2740...  \n",
       "SP_2644    [-0.09720402, 0.021139316, 0.13208055, 0.27677...  \n",
       "SP_2645    [-0.0994291, 0.025758494, 0.12514107, 0.271076...  \n",
       "SP_2646    [-0.08457472, 0.029784856, 0.15422323, 0.29593...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11766319, 0.02835577, 0.09145752, 0.243053...  \n",
       "SP_2853    [-0.13313583, -0.02140558, 0.065607786, 0.2281...  \n",
       "SP_2854    [-0.18018624, -0.014238949, -0.049044594, 0.12...  \n",
       "SP_2855    [-0.15369546, -0.028014425, 0.01341528, 0.1866...  \n",
       "SP_Q09022  [-0.114356086, -0.0051107393, 0.112075634, 0.2...  \n",
       "\n",
       "[2566 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.086681925, 0.0037621346, 0.15957397, 0.302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.097797126, 0.021844227, 0.12933321, 0.2740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.09720402, 0.021139316, 0.13208055, 0.27677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.0994291, 0.025758494, 0.12514107, 0.271076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.08457472, 0.029784856, 0.15422323, 0.29593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11766319, 0.02835577, 0.09145752, 0.243053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.13313583, -0.02140558, 0.065607786, 0.2281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.18018624, -0.014238949, -0.049044594, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.15369546, -0.028014425, 0.01341528, 0.1866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.114356086, -0.0051107393, 0.112075634, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.086681925, 0.0037621346, 0.15957397, 0.302...  \n",
       "SP_2643    [-0.097797126, 0.021844227, 0.12933321, 0.2740...  \n",
       "SP_2644    [-0.09720402, 0.021139316, 0.13208055, 0.27677...  \n",
       "SP_2645    [-0.0994291, 0.025758494, 0.12514107, 0.271076...  \n",
       "SP_2646    [-0.08457472, 0.029784856, 0.15422323, 0.29593...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11766319, 0.02835577, 0.09145752, 0.243053...  \n",
       "SP_2853    [-0.13313583, -0.02140558, 0.065607786, 0.2281...  \n",
       "SP_2854    [-0.18018624, -0.014238949, -0.049044594, 0.12...  \n",
       "SP_2855    [-0.15369546, -0.028014425, 0.01341528, 0.1866...  \n",
       "SP_Q09022  [-0.114356086, -0.0051107393, 0.112075634, 0.2...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the neural network model...\n",
      "Training the model...\n",
      "Epoch 1/500\n",
      "2/2 [==============================] - 3s 412ms/step - loss: 1.0211 - accuracy: 0.4937 - val_loss: 0.6810 - val_accuracy: 0.6615\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.9031 - accuracy: 0.5429 - val_loss: 0.6809 - val_accuracy: 0.6712\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.8459 - accuracy: 0.5541 - val_loss: 0.6798 - val_accuracy: 0.6537\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.9260 - accuracy: 0.5419 - val_loss: 0.6782 - val_accuracy: 0.6615\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.9011 - accuracy: 0.5424 - val_loss: 0.6763 - val_accuracy: 0.6615\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8565 - accuracy: 0.5541 - val_loss: 0.6737 - val_accuracy: 0.6615\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.8437 - accuracy: 0.5404 - val_loss: 0.6706 - val_accuracy: 0.6615\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.8242 - accuracy: 0.5482 - val_loss: 0.6679 - val_accuracy: 0.6615\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.7950 - accuracy: 0.5536 - val_loss: 0.6656 - val_accuracy: 0.6615\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.8293 - accuracy: 0.5551 - val_loss: 0.6642 - val_accuracy: 0.6615\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7915 - accuracy: 0.5507 - val_loss: 0.6630 - val_accuracy: 0.6615\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.8247 - accuracy: 0.5317 - val_loss: 0.6616 - val_accuracy: 0.6615\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.8041 - accuracy: 0.5419 - val_loss: 0.6605 - val_accuracy: 0.6615\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.7801 - accuracy: 0.5551 - val_loss: 0.6596 - val_accuracy: 0.6615\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.7857 - accuracy: 0.5585 - val_loss: 0.6588 - val_accuracy: 0.6615\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.7748 - accuracy: 0.5526 - val_loss: 0.6578 - val_accuracy: 0.6615\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7725 - accuracy: 0.5497 - val_loss: 0.6566 - val_accuracy: 0.6615\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.7825 - accuracy: 0.5463 - val_loss: 0.6553 - val_accuracy: 0.6615\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 427ms/step - loss: 0.7807 - accuracy: 0.5750 - val_loss: 0.6544 - val_accuracy: 0.6615\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.7667 - accuracy: 0.5570 - val_loss: 0.6533 - val_accuracy: 0.6615\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.7690 - accuracy: 0.5453 - val_loss: 0.6524 - val_accuracy: 0.6615\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.7484 - accuracy: 0.5663 - val_loss: 0.6514 - val_accuracy: 0.6615\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.7552 - accuracy: 0.5702 - val_loss: 0.6500 - val_accuracy: 0.6615\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.7824 - accuracy: 0.5346 - val_loss: 0.6487 - val_accuracy: 0.6615\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.7735 - accuracy: 0.5434 - val_loss: 0.6472 - val_accuracy: 0.6615\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.7254 - accuracy: 0.5858 - val_loss: 0.6461 - val_accuracy: 0.6615\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.7519 - accuracy: 0.5682 - val_loss: 0.6450 - val_accuracy: 0.6615\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.7218 - accuracy: 0.5872 - val_loss: 0.6442 - val_accuracy: 0.6615\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.7372 - accuracy: 0.5629 - val_loss: 0.6437 - val_accuracy: 0.6615\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.7354 - accuracy: 0.5692 - val_loss: 0.6434 - val_accuracy: 0.6615\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.7256 - accuracy: 0.5638 - val_loss: 0.6429 - val_accuracy: 0.6615\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.7082 - accuracy: 0.5785 - val_loss: 0.6424 - val_accuracy: 0.6615\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.7068 - accuracy: 0.5804 - val_loss: 0.6420 - val_accuracy: 0.6615\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.7273 - accuracy: 0.5668 - val_loss: 0.6416 - val_accuracy: 0.6615\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.7255 - accuracy: 0.5604 - val_loss: 0.6412 - val_accuracy: 0.6615\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.7221 - accuracy: 0.5770 - val_loss: 0.6408 - val_accuracy: 0.6615\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.7251 - accuracy: 0.5765 - val_loss: 0.6404 - val_accuracy: 0.6615\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.7172 - accuracy: 0.5697 - val_loss: 0.6402 - val_accuracy: 0.6615\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.7099 - accuracy: 0.5877 - val_loss: 0.6402 - val_accuracy: 0.6615\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7096 - accuracy: 0.5794 - val_loss: 0.6402 - val_accuracy: 0.6615\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.7181 - accuracy: 0.5950 - val_loss: 0.6401 - val_accuracy: 0.6615\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.7042 - accuracy: 0.5799 - val_loss: 0.6401 - val_accuracy: 0.6615\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.6927 - accuracy: 0.5858 - val_loss: 0.6401 - val_accuracy: 0.6615\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6905 - accuracy: 0.5984 - val_loss: 0.6400 - val_accuracy: 0.6615\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.7111 - accuracy: 0.5634 - val_loss: 0.6399 - val_accuracy: 0.6615\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.7055 - accuracy: 0.5794 - val_loss: 0.6397 - val_accuracy: 0.6615\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6999 - accuracy: 0.5853 - val_loss: 0.6397 - val_accuracy: 0.6615\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6877 - accuracy: 0.5853 - val_loss: 0.6398 - val_accuracy: 0.6615\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6886 - accuracy: 0.5819 - val_loss: 0.6400 - val_accuracy: 0.6615\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6870 - accuracy: 0.5902 - val_loss: 0.6402 - val_accuracy: 0.6615\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6783 - accuracy: 0.5960 - val_loss: 0.6403 - val_accuracy: 0.6615\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6935 - accuracy: 0.5882 - val_loss: 0.6404 - val_accuracy: 0.6615\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6878 - accuracy: 0.5863 - val_loss: 0.6407 - val_accuracy: 0.6615\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6955 - accuracy: 0.5941 - val_loss: 0.6411 - val_accuracy: 0.6615\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6725 - accuracy: 0.5916 - val_loss: 0.6417 - val_accuracy: 0.6615\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6851 - accuracy: 0.5975 - val_loss: 0.6421 - val_accuracy: 0.6615\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6818 - accuracy: 0.5892 - val_loss: 0.6424 - val_accuracy: 0.6615\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6885 - accuracy: 0.5863 - val_loss: 0.6426 - val_accuracy: 0.6615\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6692 - accuracy: 0.5897 - val_loss: 0.6427 - val_accuracy: 0.6615\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6747 - accuracy: 0.5970 - val_loss: 0.6429 - val_accuracy: 0.6615\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.6683 - accuracy: 0.6028 - val_loss: 0.6430 - val_accuracy: 0.6615\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.6786 - accuracy: 0.5828 - val_loss: 0.6430 - val_accuracy: 0.6615\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6717 - accuracy: 0.6062 - val_loss: 0.6430 - val_accuracy: 0.6615\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6594 - accuracy: 0.6077 - val_loss: 0.6428 - val_accuracy: 0.6615\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6783 - accuracy: 0.5984 - val_loss: 0.6427 - val_accuracy: 0.6615\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6616 - accuracy: 0.6048 - val_loss: 0.6427 - val_accuracy: 0.6615\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.6786 - accuracy: 0.5887 - val_loss: 0.6428 - val_accuracy: 0.6615\n",
      "Evaluating the model...\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 4 (Word2Vec):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.661479\n",
      "1  Precision  0.000000\n",
      "2     Recall  0.000000\n",
      "3   F1 Score  0.000000\n",
      "4    ROC AUC  0.651766\n",
      "\n",
      "Confusion Matrix for Method 4 (Word2Vec):\n",
      " [[340   0]\n",
      " [174   0]]\n",
      "Generating classification report...\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.80       340\n",
      "           1       0.00      0.00      0.00       174\n",
      "\n",
      "    accuracy                           0.66       514\n",
      "   macro avg       0.33      0.50      0.40       514\n",
      "weighted avg       0.44      0.66      0.53       514\n",
      "\n",
      "[[340   0]\n",
      " [174   0]]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Prediction for sequence 'GLWSKIKEVGKEAAKAAAKAAG': Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "posDF = pd.read_csv(r\"C:\\Users\\advik\\Downloads\\bio project\\PositiveSequences.csv\")\n",
    "allDataDF = pd.read_csv(r'C:\\Users\\advik\\Downloads\\bio project\\nlp_data.csv')\n",
    "dadp = allDataDF['DADP ID']\n",
    "\n",
    "# Process data\n",
    "print(\"Processing data...\")\n",
    "allDataDF = allDataDF.set_index(allDataDF['DADP ID'])\n",
    "posDF = posDF.set_index(posDF['DADP ID'])\n",
    "posID_List = posDF[\"DADP ID\"].tolist()\n",
    "negDF = allDataDF.drop(posID_List)\n",
    "\n",
    "invalid_chars = \"/\"\n",
    "negDF = negDF[~negDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "posDF = posDF[~posDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "\n",
    "# Process sequences for Word2Vec\n",
    "print(\"Processing sequences for Word2Vec...\")\n",
    "pos_sequences = posDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "neg_sequences = negDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(sentences=all_sequences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate sequence vectors\n",
    "print(\"Generating sequence vectors...\")\n",
    "\n",
    "def sequence_vector(seq, model):\n",
    "    return np.mean([model.wv[char] for char in seq if char in model.wv], axis=0)\n",
    "\n",
    "posDF['vector'] = posDF['Bioactive sequence'].str.upper().apply(lambda x: sequence_vector(list(x), w2v_model))\n",
    "negDF['vector'] = negDF['Bioactive sequence'].str.upper().apply(lambda x: sequence_vector(list(x), w2v_model))\n",
    "\n",
    "# Prepare data for classification\n",
    "print(\"Preparing data for classification...\")\n",
    "allDataDF3 = pd.concat([posDF, negDF])\n",
    "display(allDataDF3)\n",
    "allDataDF2['vector']=allDataDF3['vector']\n",
    "display(allDataDF2)\n",
    "X = np.stack(allDataDF3['vector'].values)\n",
    "y = np.array([1]*len(posDF) + [0]*len(negDF))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Build the neural network model\n",
    "print(\"Building the neural network model...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=2000, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method4 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 4 (Word2Vec):\\n\", performance_metrics_method4)\n",
    "print(\"\\nConfusion Matrix for Method 4 (Word2Vec):\\n\", conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Generating classification report...\")\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Function to predict antimicrobial property using the neural network model\n",
    "def predict_nn(sequence, model, w2v_model):\n",
    "    sequence = sequence.upper()\n",
    "    vector = sequence_vector(list(sequence), w2v_model)\n",
    "    prediction = model.predict(np.array([vector]))[0][0]\n",
    "    return 'Antimicrobial' if prediction > 0.5 else 'Non-antimicrobial'\n",
    "\n",
    "# Predictions using the neural network model\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "prediction_nn = predict_nn(sequence, model, w2v_model)\n",
    "print(f\"Prediction for sequence '{sequence}': {prediction_nn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee92c4-fc82-4231-86ad-43731a026b08",
   "metadata": {},
   "source": [
    "**Combined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3893737f-0127-47bf-811c-a68f511b23e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.086681925, 0.0037621346, 0.15957397, 0.302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.097797126, 0.021844227, 0.12933321, 0.2740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.09720402, 0.021139316, 0.13208055, 0.27677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.0994291, 0.025758494, 0.12514107, 0.271076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.08457472, 0.029784856, 0.15422323, 0.29593...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11766319, 0.02835577, 0.09145752, 0.243053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.13313583, -0.02140558, 0.065607786, 0.2281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.18018624, -0.014238949, -0.049044594, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.15369546, -0.028014425, 0.01341528, 0.1866...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.114356086, -0.0051107393, 0.112075634, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.086681925, 0.0037621346, 0.15957397, 0.302...  \n",
       "SP_2643    [-0.097797126, 0.021844227, 0.12933321, 0.2740...  \n",
       "SP_2644    [-0.09720402, 0.021139316, 0.13208055, 0.27677...  \n",
       "SP_2645    [-0.0994291, 0.025758494, 0.12514107, 0.271076...  \n",
       "SP_2646    [-0.08457472, 0.029784856, 0.15422323, 0.29593...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11766319, 0.02835577, 0.09145752, 0.243053...  \n",
       "SP_2853    [-0.13313583, -0.02140558, 0.065607786, 0.2281...  \n",
       "SP_2854    [-0.18018624, -0.014238949, -0.049044594, 0.12...  \n",
       "SP_2855    [-0.15369546, -0.028014425, 0.01341528, 0.1866...  \n",
       "SP_Q09022  [-0.114356086, -0.0051107393, 0.112075634, 0.2...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \n",
       "DADP ID                                                                   \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319  \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875  \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542  \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154  \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037  \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541  \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362  \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276  \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679  \n",
       "\n",
       "[2566 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1333</th>\n",
       "      <th>1334</th>\n",
       "      <th>1335</th>\n",
       "      <th>1336</th>\n",
       "      <th>1337</th>\n",
       "      <th>1338</th>\n",
       "      <th>1339</th>\n",
       "      <th>1340</th>\n",
       "      <th>1341</th>\n",
       "      <th>1342</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050115</td>\n",
       "      <td>0.062575</td>\n",
       "      <td>-0.067069</td>\n",
       "      <td>-0.168609</td>\n",
       "      <td>0.197972</td>\n",
       "      <td>0.235845</td>\n",
       "      <td>-0.044370</td>\n",
       "      <td>-0.099881</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.173850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>0.062634</td>\n",
       "      <td>-0.066433</td>\n",
       "      <td>-0.149390</td>\n",
       "      <td>0.194088</td>\n",
       "      <td>0.234853</td>\n",
       "      <td>-0.050436</td>\n",
       "      <td>-0.100322</td>\n",
       "      <td>0.027011</td>\n",
       "      <td>0.164492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034706</td>\n",
       "      <td>0.063310</td>\n",
       "      <td>-0.065910</td>\n",
       "      <td>-0.148695</td>\n",
       "      <td>0.192290</td>\n",
       "      <td>0.232375</td>\n",
       "      <td>-0.050339</td>\n",
       "      <td>-0.099044</td>\n",
       "      <td>0.028536</td>\n",
       "      <td>0.165087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030858</td>\n",
       "      <td>0.063495</td>\n",
       "      <td>-0.063635</td>\n",
       "      <td>-0.145981</td>\n",
       "      <td>0.192047</td>\n",
       "      <td>0.232704</td>\n",
       "      <td>-0.050766</td>\n",
       "      <td>-0.098171</td>\n",
       "      <td>0.028166</td>\n",
       "      <td>0.165582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.079276</td>\n",
       "      <td>-0.052543</td>\n",
       "      <td>-0.144831</td>\n",
       "      <td>0.149639</td>\n",
       "      <td>0.225253</td>\n",
       "      <td>-0.069497</td>\n",
       "      <td>-0.105067</td>\n",
       "      <td>0.024888</td>\n",
       "      <td>0.174143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>-0.090677</td>\n",
       "      <td>-0.139053</td>\n",
       "      <td>0.234250</td>\n",
       "      <td>0.228453</td>\n",
       "      <td>-0.029094</td>\n",
       "      <td>-0.085891</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.148333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031051</td>\n",
       "      <td>0.011664</td>\n",
       "      <td>-0.175446</td>\n",
       "      <td>-0.165371</td>\n",
       "      <td>0.346447</td>\n",
       "      <td>0.203199</td>\n",
       "      <td>0.018820</td>\n",
       "      <td>-0.015358</td>\n",
       "      <td>0.085541</td>\n",
       "      <td>0.132522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>-0.034970</td>\n",
       "      <td>-0.232600</td>\n",
       "      <td>-0.141878</td>\n",
       "      <td>0.431297</td>\n",
       "      <td>0.183979</td>\n",
       "      <td>0.068578</td>\n",
       "      <td>0.024213</td>\n",
       "      <td>0.089616</td>\n",
       "      <td>0.101117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>-0.017234</td>\n",
       "      <td>-0.223877</td>\n",
       "      <td>-0.164408</td>\n",
       "      <td>0.398953</td>\n",
       "      <td>0.201343</td>\n",
       "      <td>0.057204</td>\n",
       "      <td>0.006612</td>\n",
       "      <td>0.093976</td>\n",
       "      <td>0.123447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>0.025363</td>\n",
       "      <td>-0.137481</td>\n",
       "      <td>-0.162828</td>\n",
       "      <td>0.278318</td>\n",
       "      <td>0.210236</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>-0.057061</td>\n",
       "      <td>0.057098</td>\n",
       "      <td>0.142793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 1343 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2          3         4         5         6     \\\n",
       "0     3181.6835   9.701025  0.030303   7.272727  0.196970  2.762539  3181.736   \n",
       "1     2279.6794   9.703153  0.043478 -12.986957  0.630435  1.845400  2279.719   \n",
       "2     2293.7060   9.703153  0.043478  -4.613043  0.630435  1.847089  2293.746   \n",
       "3     2266.6807   9.703153  0.043478  -4.613043  0.747826  1.847089  2266.720   \n",
       "4     1583.9103   8.750052  0.000000  10.800000  1.275000  0.759103  1583.937   \n",
       "...         ...        ...       ...        ...       ...       ...       ...   \n",
       "2561  1405.7470  10.002350  0.076923  16.907692  1.000000  1.731990  1405.774   \n",
       "2562  2124.3526   4.651158  0.166667  37.416667 -0.455556 -2.147981  2124.388   \n",
       "2563  2138.4670   8.045284  0.150000  72.685000 -0.230000  0.739412  2138.506   \n",
       "2564  2076.4408   9.820679  0.117647  50.894118 -0.911765  2.732905  2076.477   \n",
       "2565  7235.6096   8.884082  0.015152  44.725758 -0.174242  4.685244  7235.743   \n",
       "\n",
       "          7        8      9     ...      1333      1334      1335      1336  \\\n",
       "0    -13.16030  1318.79   46.0  ...  0.050115  0.062575 -0.067069 -0.168609   \n",
       "1     -7.85050   910.88   31.0  ...  0.034184  0.062634 -0.066433 -0.149390   \n",
       "2     -7.46040   910.88   31.0  ...  0.034706  0.063310 -0.065910 -0.148695   \n",
       "3     -7.34350   888.02   31.0  ...  0.030858  0.063495 -0.063635 -0.145981   \n",
       "4     -5.33460   629.62   22.0  ...  0.026885  0.079276 -0.052543 -0.144831   \n",
       "...        ...      ...    ...  ...       ...       ...       ...       ...   \n",
       "2561  -2.94950   527.88   18.0  ...  0.024793  0.050498 -0.090677 -0.139053   \n",
       "2562  -2.70150   801.94   26.0  ...  0.031051  0.011664 -0.175446 -0.165371   \n",
       "2563  -6.72983   758.18   26.0  ...  0.011956 -0.034970 -0.232600 -0.141878   \n",
       "2564  -4.87636   848.49   30.0  ...  0.024290 -0.017234 -0.223877 -0.164408   \n",
       "2565 -36.35396  2988.65  111.0  ...  0.043245  0.025363 -0.137481 -0.162828   \n",
       "\n",
       "          1337      1338      1339      1340      1341      1342  \n",
       "0     0.197972  0.235845 -0.044370 -0.099881  0.032516  0.173850  \n",
       "1     0.194088  0.234853 -0.050436 -0.100322  0.027011  0.164492  \n",
       "2     0.192290  0.232375 -0.050339 -0.099044  0.028536  0.165087  \n",
       "3     0.192047  0.232704 -0.050766 -0.098171  0.028166  0.165582  \n",
       "4     0.149639  0.225253 -0.069497 -0.105067  0.024888  0.174143  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2561  0.234250  0.228453 -0.029094 -0.085891  0.036528  0.148333  \n",
       "2562  0.346447  0.203199  0.018820 -0.015358  0.085541  0.132522  \n",
       "2563  0.431297  0.183979  0.068578  0.024213  0.089616  0.101117  \n",
       "2564  0.398953  0.201343  0.057204  0.006612  0.093976  0.123447  \n",
       "2565  0.278318  0.210236 -0.000323 -0.057061  0.057098  0.142793  \n",
       "\n",
       "[2566 rows x 1343 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2566, 19) (2566, 506) (2566, 50) (2566, 768) (2566, 1343)\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3071 - accuracy: 0.3876 - val_loss: 0.7654 - val_accuracy: 0.4599\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1.0379 - accuracy: 0.4388 - val_loss: 0.6558 - val_accuracy: 0.5693\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.8706 - accuracy: 0.4997 - val_loss: 0.5985 - val_accuracy: 0.6448\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.7751 - accuracy: 0.5430 - val_loss: 0.5713 - val_accuracy: 0.6934\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.7136 - accuracy: 0.5771 - val_loss: 0.5637 - val_accuracy: 0.7178\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6835 - accuracy: 0.6069 - val_loss: 0.5681 - val_accuracy: 0.7007\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.6416 - accuracy: 0.6399 - val_loss: 0.5760 - val_accuracy: 0.7032\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.6550 - accuracy: 0.6350 - val_loss: 0.5835 - val_accuracy: 0.7080\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6638 - accuracy: 0.6447 - val_loss: 0.5868 - val_accuracy: 0.7007\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6432 - accuracy: 0.6618 - val_loss: 0.5841 - val_accuracy: 0.7153\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.6118 - accuracy: 0.6819 - val_loss: 0.5793 - val_accuracy: 0.7153\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.6188 - accuracy: 0.6831 - val_loss: 0.5727 - val_accuracy: 0.7178\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6054 - accuracy: 0.6679 - val_loss: 0.5659 - val_accuracy: 0.7202\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5837 - accuracy: 0.6868 - val_loss: 0.5591 - val_accuracy: 0.7275\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.5940 - accuracy: 0.6849 - val_loss: 0.5537 - val_accuracy: 0.7275\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5568 - accuracy: 0.6947 - val_loss: 0.5500 - val_accuracy: 0.7324\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5709 - accuracy: 0.6764 - val_loss: 0.5469 - val_accuracy: 0.7275\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5343 - accuracy: 0.7069 - val_loss: 0.5452 - val_accuracy: 0.7372\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5333 - accuracy: 0.7130 - val_loss: 0.5445 - val_accuracy: 0.7275\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.5507 - accuracy: 0.6880 - val_loss: 0.5441 - val_accuracy: 0.7397\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5314 - accuracy: 0.7258 - val_loss: 0.5438 - val_accuracy: 0.7397\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5237 - accuracy: 0.7166 - val_loss: 0.5435 - val_accuracy: 0.7397\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.5175 - accuracy: 0.7300 - val_loss: 0.5434 - val_accuracy: 0.7397\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5103 - accuracy: 0.7300 - val_loss: 0.5429 - val_accuracy: 0.7445\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4990 - accuracy: 0.7447 - val_loss: 0.5421 - val_accuracy: 0.7494\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4978 - accuracy: 0.7453 - val_loss: 0.5418 - val_accuracy: 0.7397\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4922 - accuracy: 0.7441 - val_loss: 0.5420 - val_accuracy: 0.7421\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4854 - accuracy: 0.7544 - val_loss: 0.5430 - val_accuracy: 0.7397\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.4953 - accuracy: 0.7489 - val_loss: 0.5437 - val_accuracy: 0.7372\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.4787 - accuracy: 0.7556 - val_loss: 0.5441 - val_accuracy: 0.7372\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.4813 - accuracy: 0.7502 - val_loss: 0.5449 - val_accuracy: 0.7445\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4541 - accuracy: 0.7690 - val_loss: 0.5460 - val_accuracy: 0.7445\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4609 - accuracy: 0.7605 - val_loss: 0.5472 - val_accuracy: 0.7421\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4726 - accuracy: 0.7550 - val_loss: 0.5486 - val_accuracy: 0.7445\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4664 - accuracy: 0.7642 - val_loss: 0.5501 - val_accuracy: 0.7470\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4442 - accuracy: 0.7867 - val_loss: 0.5523 - val_accuracy: 0.7494\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4374 - accuracy: 0.7800 - val_loss: 0.5539 - val_accuracy: 0.7494\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.4415 - accuracy: 0.7703 - val_loss: 0.5551 - val_accuracy: 0.7421\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4321 - accuracy: 0.7904 - val_loss: 0.5564 - val_accuracy: 0.7470\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4178 - accuracy: 0.7965 - val_loss: 0.5579 - val_accuracy: 0.7470\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.4387 - accuracy: 0.7818 - val_loss: 0.5588 - val_accuracy: 0.7494\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.4206 - accuracy: 0.8013 - val_loss: 0.5598 - val_accuracy: 0.7470\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.4189 - accuracy: 0.7879 - val_loss: 0.5607 - val_accuracy: 0.7494\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4078 - accuracy: 0.8013 - val_loss: 0.5619 - val_accuracy: 0.7445\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3954 - accuracy: 0.8178 - val_loss: 0.5634 - val_accuracy: 0.7421\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3920 - accuracy: 0.8202 - val_loss: 0.5652 - val_accuracy: 0.7421\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3928 - accuracy: 0.8044 - val_loss: 0.5672 - val_accuracy: 0.7445\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.3866 - accuracy: 0.8166 - val_loss: 0.5695 - val_accuracy: 0.7421\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3884 - accuracy: 0.8026 - val_loss: 0.5717 - val_accuracy: 0.7397\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3899 - accuracy: 0.8172 - val_loss: 0.5744 - val_accuracy: 0.7445\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3827 - accuracy: 0.8184 - val_loss: 0.5774 - val_accuracy: 0.7445\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3766 - accuracy: 0.8269 - val_loss: 0.5804 - val_accuracy: 0.7470\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3610 - accuracy: 0.8312 - val_loss: 0.5840 - val_accuracy: 0.7470\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3670 - accuracy: 0.8263 - val_loss: 0.5881 - val_accuracy: 0.7470\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.3535 - accuracy: 0.8342 - val_loss: 0.5919 - val_accuracy: 0.7470\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3531 - accuracy: 0.8379 - val_loss: 0.5958 - val_accuracy: 0.7518\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3429 - accuracy: 0.8501 - val_loss: 0.5992 - val_accuracy: 0.7470\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3470 - accuracy: 0.8385 - val_loss: 0.6025 - val_accuracy: 0.7445\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.3382 - accuracy: 0.8598 - val_loss: 0.6062 - val_accuracy: 0.7421\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3493 - accuracy: 0.8391 - val_loss: 0.6099 - val_accuracy: 0.7494\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3370 - accuracy: 0.8458 - val_loss: 0.6132 - val_accuracy: 0.7470\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3323 - accuracy: 0.8477 - val_loss: 0.6169 - val_accuracy: 0.7518\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3192 - accuracy: 0.8580 - val_loss: 0.6210 - val_accuracy: 0.7543\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.3211 - accuracy: 0.8592 - val_loss: 0.6256 - val_accuracy: 0.7518\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.3076 - accuracy: 0.8751 - val_loss: 0.6295 - val_accuracy: 0.7518\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3133 - accuracy: 0.8678 - val_loss: 0.6336 - val_accuracy: 0.7518\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2925 - accuracy: 0.8684 - val_loss: 0.6382 - val_accuracy: 0.7494\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2911 - accuracy: 0.8702 - val_loss: 0.6427 - val_accuracy: 0.7494\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3117 - accuracy: 0.8672 - val_loss: 0.6461 - val_accuracy: 0.7445\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.3002 - accuracy: 0.8623 - val_loss: 0.6504 - val_accuracy: 0.7470\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.2883 - accuracy: 0.8726 - val_loss: 0.6567 - val_accuracy: 0.7445\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2873 - accuracy: 0.8787 - val_loss: 0.6633 - val_accuracy: 0.7494\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.2778 - accuracy: 0.8842 - val_loss: 0.6687 - val_accuracy: 0.7470\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2769 - accuracy: 0.8769 - val_loss: 0.6742 - val_accuracy: 0.7494\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2671 - accuracy: 0.8927 - val_loss: 0.6800 - val_accuracy: 0.7518\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.2802 - accuracy: 0.8726 - val_loss: 0.6850 - val_accuracy: 0.7518\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2664 - accuracy: 0.8921 - val_loss: 0.6905 - val_accuracy: 0.7518\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2592 - accuracy: 0.8958 - val_loss: 0.6956 - val_accuracy: 0.7543\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.2623 - accuracy: 0.8885 - val_loss: 0.7002 - val_accuracy: 0.7494\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2628 - accuracy: 0.8927 - val_loss: 0.7055 - val_accuracy: 0.7470\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2554 - accuracy: 0.8964 - val_loss: 0.7111 - val_accuracy: 0.7445\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.2529 - accuracy: 0.8952 - val_loss: 0.7162 - val_accuracy: 0.7494\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.2405 - accuracy: 0.9031 - val_loss: 0.7222 - val_accuracy: 0.7518\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.2468 - accuracy: 0.8903 - val_loss: 0.7292 - val_accuracy: 0.7518\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.2498 - accuracy: 0.9019 - val_loss: 0.7361 - val_accuracy: 0.7518\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2279 - accuracy: 0.9068 - val_loss: 0.7440 - val_accuracy: 0.7518\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2208 - accuracy: 0.9159 - val_loss: 0.7523 - val_accuracy: 0.7518\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2238 - accuracy: 0.9062 - val_loss: 0.7632 - val_accuracy: 0.7470\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2250 - accuracy: 0.9080 - val_loss: 0.7735 - val_accuracy: 0.7470\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.2236 - accuracy: 0.9007 - val_loss: 0.7842 - val_accuracy: 0.7445\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2211 - accuracy: 0.9007 - val_loss: 0.7938 - val_accuracy: 0.7494\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2159 - accuracy: 0.9147 - val_loss: 0.8013 - val_accuracy: 0.7470\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.2067 - accuracy: 0.9153 - val_loss: 0.8083 - val_accuracy: 0.7470\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.2143 - accuracy: 0.9110 - val_loss: 0.8148 - val_accuracy: 0.7445\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.2108 - accuracy: 0.9202 - val_loss: 0.8228 - val_accuracy: 0.7445\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.2080 - accuracy: 0.9177 - val_loss: 0.8325 - val_accuracy: 0.7494\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.2040 - accuracy: 0.9049 - val_loss: 0.8429 - val_accuracy: 0.7470\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.2062 - accuracy: 0.9104 - val_loss: 0.8536 - val_accuracy: 0.7494\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.2033 - accuracy: 0.9165 - val_loss: 0.8647 - val_accuracy: 0.7494\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.1850 - accuracy: 0.9250 - val_loss: 0.8751 - val_accuracy: 0.7445\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1933 - accuracy: 0.9159 - val_loss: 0.8853 - val_accuracy: 0.7470\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.1840 - accuracy: 0.9281 - val_loss: 0.8930 - val_accuracy: 0.7470\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1697 - accuracy: 0.9360 - val_loss: 0.9027 - val_accuracy: 0.7567\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1843 - accuracy: 0.9244 - val_loss: 0.9141 - val_accuracy: 0.7518\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.1925 - accuracy: 0.9208 - val_loss: 0.9275 - val_accuracy: 0.7518\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1740 - accuracy: 0.9293 - val_loss: 0.9431 - val_accuracy: 0.7518\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1743 - accuracy: 0.9293 - val_loss: 0.9584 - val_accuracy: 0.7494\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1704 - accuracy: 0.9366 - val_loss: 0.9715 - val_accuracy: 0.7470\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1710 - accuracy: 0.9263 - val_loss: 0.9848 - val_accuracy: 0.7494\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1426 - accuracy: 0.9494 - val_loss: 0.9989 - val_accuracy: 0.7494\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1646 - accuracy: 0.9305 - val_loss: 1.0135 - val_accuracy: 0.7470\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1576 - accuracy: 0.9354 - val_loss: 1.0272 - val_accuracy: 0.7470\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.1585 - accuracy: 0.9324 - val_loss: 1.0424 - val_accuracy: 0.7421\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1490 - accuracy: 0.9317 - val_loss: 1.0592 - val_accuracy: 0.7445\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1470 - accuracy: 0.9445 - val_loss: 1.0717 - val_accuracy: 0.7421\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1495 - accuracy: 0.9366 - val_loss: 1.0830 - val_accuracy: 0.7470\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1481 - accuracy: 0.9391 - val_loss: 1.0958 - val_accuracy: 0.7543\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1411 - accuracy: 0.9525 - val_loss: 1.1099 - val_accuracy: 0.7518\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1422 - accuracy: 0.9409 - val_loss: 1.1255 - val_accuracy: 0.7518\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1430 - accuracy: 0.9439 - val_loss: 1.1393 - val_accuracy: 0.7445\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1427 - accuracy: 0.9470 - val_loss: 1.1518 - val_accuracy: 0.7518\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1314 - accuracy: 0.9482 - val_loss: 1.1670 - val_accuracy: 0.7470\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.1318 - accuracy: 0.9531 - val_loss: 1.1835 - val_accuracy: 0.7397\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1264 - accuracy: 0.9519 - val_loss: 1.2019 - val_accuracy: 0.7421\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1274 - accuracy: 0.9494 - val_loss: 1.2186 - val_accuracy: 0.7470\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.1198 - accuracy: 0.9573 - val_loss: 1.2343 - val_accuracy: 0.7494\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.1211 - accuracy: 0.9561 - val_loss: 1.2495 - val_accuracy: 0.7518\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1229 - accuracy: 0.9531 - val_loss: 1.2642 - val_accuracy: 0.7543\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1090 - accuracy: 0.9634 - val_loss: 1.2782 - val_accuracy: 0.7543\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1191 - accuracy: 0.9549 - val_loss: 1.2910 - val_accuracy: 0.7518\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1208 - accuracy: 0.9525 - val_loss: 1.3023 - val_accuracy: 0.7494\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1179 - accuracy: 0.9525 - val_loss: 1.3162 - val_accuracy: 0.7470\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1147 - accuracy: 0.9525 - val_loss: 1.3314 - val_accuracy: 0.7445\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.1088 - accuracy: 0.9580 - val_loss: 1.3468 - val_accuracy: 0.7494\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.1119 - accuracy: 0.9598 - val_loss: 1.3630 - val_accuracy: 0.7494\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.1090 - accuracy: 0.9592 - val_loss: 1.3766 - val_accuracy: 0.7494\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1069 - accuracy: 0.9616 - val_loss: 1.3899 - val_accuracy: 0.7518\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1115 - accuracy: 0.9610 - val_loss: 1.4033 - val_accuracy: 0.7445\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0998 - accuracy: 0.9653 - val_loss: 1.4171 - val_accuracy: 0.7470\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1011 - accuracy: 0.9659 - val_loss: 1.4294 - val_accuracy: 0.7494\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.1048 - accuracy: 0.9580 - val_loss: 1.4422 - val_accuracy: 0.7494\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0929 - accuracy: 0.9640 - val_loss: 1.4536 - val_accuracy: 0.7518\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1062 - accuracy: 0.9586 - val_loss: 1.4674 - val_accuracy: 0.7518\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1017 - accuracy: 0.9567 - val_loss: 1.4812 - val_accuracy: 0.7543\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1067 - accuracy: 0.9622 - val_loss: 1.4937 - val_accuracy: 0.7567\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0982 - accuracy: 0.9653 - val_loss: 1.5128 - val_accuracy: 0.7616\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0898 - accuracy: 0.9665 - val_loss: 1.5351 - val_accuracy: 0.7543\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0939 - accuracy: 0.9640 - val_loss: 1.5516 - val_accuracy: 0.7567\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0940 - accuracy: 0.9640 - val_loss: 1.5628 - val_accuracy: 0.7543\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0906 - accuracy: 0.9671 - val_loss: 1.5754 - val_accuracy: 0.7518\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0844 - accuracy: 0.9665 - val_loss: 1.5875 - val_accuracy: 0.7494\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0985 - accuracy: 0.9640 - val_loss: 1.5990 - val_accuracy: 0.7494\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0840 - accuracy: 0.9689 - val_loss: 1.6118 - val_accuracy: 0.7470\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0817 - accuracy: 0.9714 - val_loss: 1.6302 - val_accuracy: 0.7470\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0874 - accuracy: 0.9653 - val_loss: 1.6528 - val_accuracy: 0.7470\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.0893 - accuracy: 0.9653 - val_loss: 1.6714 - val_accuracy: 0.7445\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0832 - accuracy: 0.9738 - val_loss: 1.6851 - val_accuracy: 0.7470\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0828 - accuracy: 0.9677 - val_loss: 1.6950 - val_accuracy: 0.7494\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0747 - accuracy: 0.9732 - val_loss: 1.7061 - val_accuracy: 0.7543\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0818 - accuracy: 0.9732 - val_loss: 1.7208 - val_accuracy: 0.7543\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0863 - accuracy: 0.9640 - val_loss: 1.7475 - val_accuracy: 0.7518\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0712 - accuracy: 0.9701 - val_loss: 1.7804 - val_accuracy: 0.7470\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0797 - accuracy: 0.9701 - val_loss: 1.8034 - val_accuracy: 0.7445\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0724 - accuracy: 0.9726 - val_loss: 1.8156 - val_accuracy: 0.7470\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0755 - accuracy: 0.9659 - val_loss: 1.8246 - val_accuracy: 0.7543\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0671 - accuracy: 0.9732 - val_loss: 1.8389 - val_accuracy: 0.7518\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0770 - accuracy: 0.9750 - val_loss: 1.8577 - val_accuracy: 0.7518\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0708 - accuracy: 0.9732 - val_loss: 1.8763 - val_accuracy: 0.7470\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0753 - accuracy: 0.9671 - val_loss: 1.8892 - val_accuracy: 0.7494\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0684 - accuracy: 0.9750 - val_loss: 1.9056 - val_accuracy: 0.7518\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0820 - accuracy: 0.9665 - val_loss: 1.9115 - val_accuracy: 0.7494\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0738 - accuracy: 0.9701 - val_loss: 1.9215 - val_accuracy: 0.7518\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0715 - accuracy: 0.9720 - val_loss: 1.9342 - val_accuracy: 0.7518\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0729 - accuracy: 0.9683 - val_loss: 1.9544 - val_accuracy: 0.7470\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0798 - accuracy: 0.9707 - val_loss: 1.9822 - val_accuracy: 0.7543\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0699 - accuracy: 0.9732 - val_loss: 2.0001 - val_accuracy: 0.7518\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0638 - accuracy: 0.9714 - val_loss: 2.0114 - val_accuracy: 0.7470\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0579 - accuracy: 0.9787 - val_loss: 2.0217 - val_accuracy: 0.7518\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0697 - accuracy: 0.9768 - val_loss: 2.0271 - val_accuracy: 0.7591\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0701 - accuracy: 0.9707 - val_loss: 2.0333 - val_accuracy: 0.7567\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0652 - accuracy: 0.9762 - val_loss: 2.0402 - val_accuracy: 0.7567\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0593 - accuracy: 0.9817 - val_loss: 2.0547 - val_accuracy: 0.7640\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0574 - accuracy: 0.9793 - val_loss: 2.0748 - val_accuracy: 0.7640\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0611 - accuracy: 0.9781 - val_loss: 2.0981 - val_accuracy: 0.7543\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0637 - accuracy: 0.9762 - val_loss: 2.1193 - val_accuracy: 0.7543\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0598 - accuracy: 0.9762 - val_loss: 2.1343 - val_accuracy: 0.7567\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0661 - accuracy: 0.9714 - val_loss: 2.1475 - val_accuracy: 0.7567\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0767 - accuracy: 0.9665 - val_loss: 2.1509 - val_accuracy: 0.7518\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0578 - accuracy: 0.9762 - val_loss: 2.1559 - val_accuracy: 0.7543\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0584 - accuracy: 0.9793 - val_loss: 2.1635 - val_accuracy: 0.7543\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0557 - accuracy: 0.9768 - val_loss: 2.1769 - val_accuracy: 0.7543\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0644 - accuracy: 0.9726 - val_loss: 2.1981 - val_accuracy: 0.7494\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0643 - accuracy: 0.9787 - val_loss: 2.2293 - val_accuracy: 0.7518\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0594 - accuracy: 0.9781 - val_loss: 2.2647 - val_accuracy: 0.7494\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0555 - accuracy: 0.9768 - val_loss: 2.2891 - val_accuracy: 0.7470\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0619 - accuracy: 0.9756 - val_loss: 2.3094 - val_accuracy: 0.7445\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0523 - accuracy: 0.9793 - val_loss: 2.3233 - val_accuracy: 0.7470\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0514 - accuracy: 0.9787 - val_loss: 2.3335 - val_accuracy: 0.7470\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0515 - accuracy: 0.9799 - val_loss: 2.3443 - val_accuracy: 0.7470\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0588 - accuracy: 0.9775 - val_loss: 2.3567 - val_accuracy: 0.7470\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 5 (Combined):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.817121\n",
      "1  Precision  0.740964\n",
      "2     Recall  0.706897\n",
      "3   F1 Score  0.723529\n",
      "4    ROC AUC  0.869211\n",
      "\n",
      "Confusion Matrix for Method 5 (Combined):\n",
      " [[297  43]\n",
      " [ 51 123]]\n",
      "Loading BioBERT model and tokenizer...\n",
      "Predicting for sequence: GLWSKIKEVGKEAAKAAAKAAG\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "\n",
      "Predictions using Combined technique:\n",
      "Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load DataFrame\n",
    "display(allDataDF2)\n",
    "\n",
    "# Process data\n",
    "X1 = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1).values\n",
    "t = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1)\n",
    "display(t)\n",
    "y1 = allDataDF2['Value'].values\n",
    "\n",
    "# Reshape encoded columns\n",
    "X2 = np.array(list(allDataDF2['EncodedX'])).reshape((len(allDataDF2), -1))\n",
    "X3 = np.array(list(allDataDF2['vector'])).reshape((len(allDataDF2), -1))\n",
    "X4 = np.array(list(allDataDF2['Bert'])).reshape((len(allDataDF2), -1))\n",
    "\n",
    "# Concatenate arrays\n",
    "X = np.concatenate((X1, X2, X4, X3), axis=1)\n",
    "x11 = pd.DataFrame(X)\n",
    "display(x11)\n",
    "\n",
    "# Split data\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# Display shapes to verify\n",
    "print(X1.shape, X2.shape, X3.shape, X4.shape, X.shape)\n",
    "\n",
    "# Scale data\n",
    "scaler2 = StandardScaler()\n",
    "X4_train = scaler2.fit_transform(X4_train)\n",
    "X4_test = scaler2.transform(X4_test)\n",
    "\n",
    "# Define neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train neural network model\n",
    "input_dim = X4_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "nn_model.fit(X4_train, y4_train, epochs=200, batch_size=2000, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = nn_model.predict(X4_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method5 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 5 (Combined):\\n\", performance_metrics_method5)\n",
    "print(\"\\nConfusion Matrix for Method 5 (Combined):\\n\", conf_matrix)\n",
    "\n",
    "# Train Word2Vec model\n",
    "pos_sequences = posDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "neg_sequences = negDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "w2v_model = Word2Vec(sentences=all_sequences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate sequence vectors\n",
    "def sequence_vector(seq, model):\n",
    "    return np.mean([model.wv[char] for char in seq if char in model.wv], axis=0)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embeddings1(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_method3(sequence, nn_model, scaler, model, tokenizer, w2v_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    properties = calculate_all_properties(sequence)\n",
    "   \n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    \n",
    "    \n",
    "    sequence1 = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence1]\n",
    "    onehot_encoded = np.ravel([[0 if i != val else 1 for i in range(len(char_to_int))] for val in integer_encoded])\n",
    "    \n",
    "    \n",
    "    embedding = get_bert_embeddings1(sequence, tokenizer, model)\n",
    "    \n",
    "    \n",
    "    sequence = sequence.upper()\n",
    "    vector = sequence_vector(list(sequence), w2v_model)\n",
    "    \n",
    "    \n",
    "    total = np.concatenate((vector, embedding.flatten(), prop_values.flatten(), onehot_encoded))\n",
    "    total = total.reshape(1, -1)\n",
    "    \n",
    "    prediction = nn_model.predict(total)\n",
    "    result = 'Antimicrobial' if prediction[0] >= 0.5 else 'Non-antimicrobial'\n",
    "    \n",
    "    return result, embedding\n",
    "\n",
    "# Prediction\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "predictions, embed = predict_method3(sequence, nn_model, scaler, bert_model, tokenizer, w2v_model)\n",
    "print(\"\\nPredictions using Combined technique:\")\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2378716-e8b7-4aba-8ac1-4a48e9beca28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Add Method column\n",
    "performance_metrics_method1['Method'] = 'PhysicoChemical Properties'\n",
    "performance_metrics_method2['Method'] = 'One Hot Encoding'\n",
    "performance_metrics_method3['Method'] = 'Bert'\n",
    "performance_metrics_method4['Method'] = 'Word2Vec'\n",
    "performance_metrics_method5['Method'] = 'Combined'\n",
    "# Concatenate the metrics for graphing\n",
    "combined_metrics = pd.concat([performance_metrics_method1, performance_metrics_method2,performance_metrics_method2,performance_metrics_method3,performance_metrics_method4,performance_metrics_method5])\n",
    "\n",
    "# Plot the performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Value', hue='Method', data=combined_metrics)\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896358a-79cf-47a8-86a5-efd938892bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
