{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5993f04c-f821-4b57-93b9-6ff0e68f8b6c",
   "metadata": {},
   "source": [
    "**PhysicoChemical properties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22478c9f-e14f-4ae9-924e-c76c877e19c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start calculating properties for posDF and negDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 921/921 [03:07<00:00,  4.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1645/1645 [04:14<00:00,  6.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculating properties for posDF and negDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from lime import lime_tabular\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "posDF = pd.read_csv(r\"C:\\Users\\advik\\Downloads\\bio project\\PositiveSequences.csv\")\n",
    "allDataDF = pd.read_csv(r'C:\\Users\\advik\\Downloads\\bio project\\nlp_data.csv')\n",
    "\n",
    "# Process data for Method 1 (Physicochemical Properties)\n",
    "allDataDF = allDataDF.set_index(allDataDF['DADP ID'])\n",
    "posDF = posDF.set_index(posDF['DADP ID'])\n",
    "posID_List = posDF[\"DADP ID\"].tolist()\n",
    "negDF = allDataDF.drop(posID_List)\n",
    "\n",
    "invalid_chars = \"/\"\n",
    "negDF = negDF[~negDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "posDF = posDF[~posDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "\n",
    "def calculate_physicochemical_properties(sequence):\n",
    "    analyzed_seq = ProteinAnalysis(sequence)\n",
    "    properties = {\n",
    "        'MolecularWeight': analyzed_seq.molecular_weight(),\n",
    "        'IsoelectricPoint': analyzed_seq.isoelectric_point(),\n",
    "        'Aromaticity': analyzed_seq.aromaticity(),\n",
    "        'InstabilityIndex': analyzed_seq.instability_index(),\n",
    "        'Hydrophobicity': analyzed_seq.gravy(),\n",
    "        'ChargeAtPH7': analyzed_seq.charge_at_pH(7.0)\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "def calculate_rdkit_descriptors(sequence):\n",
    "    mol = Chem.MolFromSequence(sequence)\n",
    "    properties = {\n",
    "        'MolWt': Descriptors.MolWt(mol),\n",
    "        'LogP': Descriptors.MolLogP(mol),\n",
    "        'TPSA': Descriptors.TPSA(mol),\n",
    "        'NumHDonors': Descriptors.NumHDonors(mol),\n",
    "        'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
    "        'BalabanJ': Descriptors.BalabanJ(mol),\n",
    "        'BertzCT': Descriptors.BertzCT(mol),\n",
    "        'Chi0v': Descriptors.Chi0v(mol),\n",
    "        'Chi1v': Descriptors.Chi1v(mol),\n",
    "        'HallKierAlpha': Descriptors.HallKierAlpha(mol),\n",
    "        'Kappa1': Descriptors.Kappa1(mol),\n",
    "        'Kappa2': Descriptors.Kappa2(mol),\n",
    "        'Kappa3': Descriptors.Kappa3(mol)\n",
    "    }\n",
    "    return properties\n",
    "\n",
    "def calculate_all_properties(sequence):\n",
    "    physico_props = calculate_physicochemical_properties(sequence)\n",
    "    rdkit_props = calculate_rdkit_descriptors(sequence)\n",
    "    all_props = {**physico_props, **rdkit_props}\n",
    "    return all_props\n",
    "print(\"Start calculating properties for posDF and negDF\")\n",
    "posDF['Properties'] = posDF['Bioactive sequence'].progress_apply(calculate_all_properties)\n",
    "negDF['Properties'] = negDF['Bioactive sequence'].progress_apply(calculate_all_properties)\n",
    "print(\"Finished calculating properties for posDF and negDF\")\n",
    "pos_properties = pd.DataFrame(posDF['Properties'].tolist(), index=posDF.index)\n",
    "neg_properties = pd.DataFrame(negDF['Properties'].tolist(), index=negDF.index)\n",
    "pos_properties['Value'] = 1\n",
    "neg_properties['Value'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec604f2a-a67a-46d8-aaad-866f58626e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319   \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875   \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542   \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154   \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848   \n",
       "...               ...            ...         ...         ...         ...   \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037   \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541   \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362   \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276   \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679   \n",
       "\n",
       "           Value  \n",
       "DADP ID           \n",
       "SP_P31107      1  \n",
       "SP_2643        1  \n",
       "SP_2644        1  \n",
       "SP_2645        1  \n",
       "SP_2646        1  \n",
       "...          ...  \n",
       "SP_2852        0  \n",
       "SP_2853        0  \n",
       "SP_2854        0  \n",
       "SP_2855        0  \n",
       "SP_Q09022      0  \n",
       "\n",
       "[2566 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n",
      "Confusion Matrix:\n",
      "[[282  58]\n",
      " [ 84  90]]\n",
      "Accuracy: 0.7237\n",
      "Precision: 0.6081\n",
      "Recall: 0.5172\n",
      "F1 Score: 0.5590\n",
      "ROC AUC Score: 0.7639\n",
      "Performance Metrics:\n",
      "       Metric     Value\n",
      "0   Accuracy  0.723735\n",
      "1  Precision  0.608108\n",
      "2     Recall  0.517241\n",
      "3   F1 Score  0.559006\n",
      "4    ROC AUC  0.763886\n",
      "\n",
      "Confusion Matrix:\n",
      " [[282  58]\n",
      " [ 84  90]]\n",
      "\n",
      "Predicting using Deep Learning Model:\n",
      "Sequence: MFTLKKSMLLLFFLGTISLSLC\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Prediction: Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "all_properties = pd.concat([pos_properties, neg_properties])\n",
    "display(all_properties)\n",
    "allDataDF2=all_properties\n",
    "\n",
    "\n",
    "# Prepare data for Method 1\n",
    "X1 = all_properties.drop('Value', axis=1)\n",
    "y1 = all_properties['Value']\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X1_train = scaler.fit_transform(X1_train)\n",
    "X1_test = scaler.transform(X1_test)\n",
    "# Deep Learning Model\n",
    "def create_deep_learning_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the deep learning model\n",
    "def train_and_evaluate_dl_model(X_train, y_train, X_test, y_test, epochs=500, batch_size=2000):\n",
    "    input_shape = X_train.shape[1]\n",
    "    model = create_deep_learning_model(input_shape)\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    \n",
    "    return y_pred,y_pred_prob, model\n",
    "\n",
    "# Train and evaluate the deep learning model\n",
    "y_pred,y_pred_prob, dl_model = train_and_evaluate_dl_model(X1_train, y1_train, X1_test, y1_test, epochs=500, batch_size=2000)\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y1_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "precision = precision_score(y1_test, y_pred)\n",
    "recall = recall_score(y1_test, y_pred)\n",
    "f1 = f1_score(y1_test, y_pred)\n",
    "roc_auc = roc_auc_score(y1_test, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "performance_metrics_method1 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics:\\n\", performance_metrics_method1)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_dl_model(sequence, model, scaler):\n",
    "    properties = calculate_all_properties(sequence)\n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    prediction = model.predict(prop_values)\n",
    "    result = 'Antimicrobial' if prediction[0][0] > 0.5 else 'Non-antimicrobial'\n",
    "    return result\n",
    "\n",
    "# Example prediction\n",
    "sequence = 'MFTLKKSMLLLFFLGTISLSLC'\n",
    "\n",
    "# Prediction using deep learning model with progress bar\n",
    "print(\"\\nPredicting using Deep Learning Model:\")\n",
    "\n",
    "print(f\"Sequence: {sequence}\")\n",
    "prediction_dl = predict_dl_model(sequence, dl_model, scaler)\n",
    "print(\"Prediction:\", prediction_dl)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c92b4-0005-4f30-acfd-9efc58979526",
   "metadata": {},
   "source": [
    "**One Hot Encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c1ac7f-043c-4f8f-88c7-f1053634cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             DADP ID     Entry Name Uniprot code                Species  \\\n",
       "DADP ID                                                                   \n",
       "SP_P31107  SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "SP_2643      SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "SP_2644      SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "SP_2645      SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "SP_2646      SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...              ...            ...          ...                    ...   \n",
       "SP_2852      SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "SP_2853      SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "SP_2854      SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "SP_2855      SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "SP_Q09022  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "           Sequence length         signal sequence  \\\n",
       "DADP ID                                              \n",
       "SP_P31107               81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "SP_2643                 23                       /   \n",
       "SP_2644                 23                       /   \n",
       "SP_2645                 23                       /   \n",
       "SP_2646                 16                       /   \n",
       "...                    ...                     ...   \n",
       "SP_2852                 59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "SP_2853                 55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "SP_2854                 67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "SP_2855                 64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "SP_Q09022               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                          Bioactive sequence  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107                  GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "SP_2643                              GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2644                              GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "SP_2645                              GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "SP_2646                                     ILGKLLSTAAGLLSNL   \n",
       "...                                                      ...   \n",
       "SP_2852                                        VFLGNIVSMGKKI   \n",
       "SP_2853                                   DAAVEPELYHWGKVWLPN   \n",
       "SP_2854                                 CVDIGFSPTGKRPPFCPYPG   \n",
       "SP_2855                                    VADKRPYILREKKSIPY   \n",
       "SP_Q09022  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                                  Properties  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "SP_2643    {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "SP_2644    {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "SP_2645    {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "SP_2646    {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                      ...   \n",
       "SP_2852    {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "SP_2853    {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "SP_2854    {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "SP_2855    {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "SP_Q09022  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "                    TruncSequence             ModSequence  Value  \n",
       "DADP ID                                                           \n",
       "SP_P31107  GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "SP_2643    GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2644    GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2645    GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "SP_2646          ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                           ...                     ...    ...  \n",
       "SP_2852             VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "SP_2853        DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "SP_2854      CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "SP_2855         VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "SP_Q09022  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...  BalabanJ       BertzCT       Chi0v       Chi1v  \\\n",
       "DADP ID                ...                                                   \n",
       "SP_P31107          46  ...  2.195969   7612.627585  131.799954   75.452403   \n",
       "SP_2643            31  ...  2.246811   5191.774736   95.319148   54.422594   \n",
       "SP_2644            31  ...  2.254177   5211.853178   96.026255   54.922594   \n",
       "SP_2645            31  ...  2.253657   5096.168912   94.987870   54.392469   \n",
       "SP_2646            22  ...  6.186024   3100.024844   67.476541   37.978223   \n",
       "...               ...  ...       ...           ...         ...         ...   \n",
       "SP_2852            18  ...  3.276060   2766.535116   59.496327   34.969862   \n",
       "SP_2853            26  ...  0.992706   5867.905389   86.416638   50.512045   \n",
       "SP_2854            26  ...  0.884764   5233.121472   86.425115   51.704602   \n",
       "SP_2855            30  ...  1.659505   4729.413175   85.376209   50.244738   \n",
       "SP_Q09022         111  ...  2.570045  18204.737165  294.537783  172.867436   \n",
       "\n",
       "           HallKierAlpha      Kappa1      Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107         -20.68  199.359058  105.101422   75.403319      1   \n",
       "SP_2643           -15.02  142.034244   74.405793   54.137875      1   \n",
       "SP_2644           -15.02  143.033879   75.212983   54.431542      1   \n",
       "SP_2645           -14.53  141.524432   74.738555   53.425154      1   \n",
       "SP_2646            -9.21  101.790000   52.104489   40.528848      1   \n",
       "...                  ...         ...         ...         ...    ...   \n",
       "SP_2852            -8.01   88.001112   47.066173   33.026037      0   \n",
       "SP_2853           -15.99  121.218778   58.255568   37.036541      0   \n",
       "SP_2854           -13.86  121.347700   60.104035   35.961362      0   \n",
       "SP_2855           -13.69  125.600865   65.022901   43.867276      0   \n",
       "SP_Q09022         -40.18  448.837609  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
       "...                                                      ...  \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[2566 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 2s 480ms/step - loss: 0.7117 - accuracy: 0.4167 - val_loss: 0.6489 - val_accuracy: 0.6790\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6546 - accuracy: 0.6574 - val_loss: 0.6155 - val_accuracy: 0.6946\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.6235 - accuracy: 0.6696 - val_loss: 0.5956 - val_accuracy: 0.6946\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.6041 - accuracy: 0.6706 - val_loss: 0.5830 - val_accuracy: 0.6926\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.5909 - accuracy: 0.6696 - val_loss: 0.5749 - val_accuracy: 0.6946\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5811 - accuracy: 0.6720 - val_loss: 0.5694 - val_accuracy: 0.7043\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.5732 - accuracy: 0.6774 - val_loss: 0.5642 - val_accuracy: 0.7218\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5644 - accuracy: 0.7061 - val_loss: 0.5591 - val_accuracy: 0.7237\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.5552 - accuracy: 0.7086 - val_loss: 0.5549 - val_accuracy: 0.7218\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.5468 - accuracy: 0.7188 - val_loss: 0.5504 - val_accuracy: 0.7082\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.5384 - accuracy: 0.7164 - val_loss: 0.5455 - val_accuracy: 0.7082\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.5302 - accuracy: 0.7290 - val_loss: 0.5404 - val_accuracy: 0.7121\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.5222 - accuracy: 0.7339 - val_loss: 0.5358 - val_accuracy: 0.7237\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.5144 - accuracy: 0.7403 - val_loss: 0.5299 - val_accuracy: 0.7218\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.5057 - accuracy: 0.7442 - val_loss: 0.5228 - val_accuracy: 0.7315\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4974 - accuracy: 0.7476 - val_loss: 0.5171 - val_accuracy: 0.7315\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4909 - accuracy: 0.7471 - val_loss: 0.5129 - val_accuracy: 0.7374\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.4858 - accuracy: 0.7451 - val_loss: 0.5090 - val_accuracy: 0.7315\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4795 - accuracy: 0.7471 - val_loss: 0.5054 - val_accuracy: 0.7393\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 0.4718 - accuracy: 0.7534 - val_loss: 0.5019 - val_accuracy: 0.7412\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.4639 - accuracy: 0.7563 - val_loss: 0.4988 - val_accuracy: 0.7568\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4570 - accuracy: 0.7724 - val_loss: 0.4972 - val_accuracy: 0.7685\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.4521 - accuracy: 0.7900 - val_loss: 0.4944 - val_accuracy: 0.7724\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.4469 - accuracy: 0.7914 - val_loss: 0.4891 - val_accuracy: 0.7724\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4407 - accuracy: 0.7900 - val_loss: 0.4841 - val_accuracy: 0.7724\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.4355 - accuracy: 0.7909 - val_loss: 0.4800 - val_accuracy: 0.7821\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.4306 - accuracy: 0.7904 - val_loss: 0.4763 - val_accuracy: 0.7840\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.4254 - accuracy: 0.7895 - val_loss: 0.4735 - val_accuracy: 0.7899\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4198 - accuracy: 0.7973 - val_loss: 0.4716 - val_accuracy: 0.7821\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4146 - accuracy: 0.7992 - val_loss: 0.4697 - val_accuracy: 0.7782\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.4098 - accuracy: 0.8007 - val_loss: 0.4686 - val_accuracy: 0.7782\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.4052 - accuracy: 0.8041 - val_loss: 0.4690 - val_accuracy: 0.7821\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.4002 - accuracy: 0.8065 - val_loss: 0.4721 - val_accuracy: 0.7802\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.3958 - accuracy: 0.8065 - val_loss: 0.4820 - val_accuracy: 0.7568\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3986 - accuracy: 0.8119 - val_loss: 0.4941 - val_accuracy: 0.7393\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.4046 - accuracy: 0.7997 - val_loss: 0.4930 - val_accuracy: 0.7412\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.3992 - accuracy: 0.8056 - val_loss: 0.4807 - val_accuracy: 0.7665\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3852 - accuracy: 0.8226 - val_loss: 0.4704 - val_accuracy: 0.7860\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3747 - accuracy: 0.8299 - val_loss: 0.4702 - val_accuracy: 0.7899\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3755 - accuracy: 0.8285 - val_loss: 0.4741 - val_accuracy: 0.7802\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3782 - accuracy: 0.8158 - val_loss: 0.4695 - val_accuracy: 0.7879\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3698 - accuracy: 0.8309 - val_loss: 0.4649 - val_accuracy: 0.7918\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3583 - accuracy: 0.8319 - val_loss: 0.4728 - val_accuracy: 0.7821\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.3586 - accuracy: 0.8387 - val_loss: 0.4799 - val_accuracy: 0.7860\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3608 - accuracy: 0.8397 - val_loss: 0.4720 - val_accuracy: 0.7840\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3531 - accuracy: 0.8421 - val_loss: 0.4612 - val_accuracy: 0.7821\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.3453 - accuracy: 0.8445 - val_loss: 0.4580 - val_accuracy: 0.7821\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.3469 - accuracy: 0.8353 - val_loss: 0.4597 - val_accuracy: 0.8016\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.3493 - accuracy: 0.8372 - val_loss: 0.4571 - val_accuracy: 0.8074\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.3431 - accuracy: 0.8499 - val_loss: 0.4529 - val_accuracy: 0.8035\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.3326 - accuracy: 0.8548 - val_loss: 0.4546 - val_accuracy: 0.7821\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.3256 - accuracy: 0.8572 - val_loss: 0.4624 - val_accuracy: 0.7802\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.3254 - accuracy: 0.8592 - val_loss: 0.4682 - val_accuracy: 0.7724\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.3248 - accuracy: 0.8548 - val_loss: 0.4675 - val_accuracy: 0.7743\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3197 - accuracy: 0.8558 - val_loss: 0.4667 - val_accuracy: 0.7879\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.3174 - accuracy: 0.8504 - val_loss: 0.4711 - val_accuracy: 0.7879\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.3195 - accuracy: 0.8543 - val_loss: 0.4722 - val_accuracy: 0.7899\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.3142 - accuracy: 0.8587 - val_loss: 0.4725 - val_accuracy: 0.7840\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.3063 - accuracy: 0.8645 - val_loss: 0.4745 - val_accuracy: 0.7860\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.3033 - accuracy: 0.8684 - val_loss: 0.4730 - val_accuracy: 0.7802\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.3004 - accuracy: 0.8713 - val_loss: 0.4691 - val_accuracy: 0.7840\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2963 - accuracy: 0.8718 - val_loss: 0.4640 - val_accuracy: 0.7957\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2932 - accuracy: 0.8718 - val_loss: 0.4616 - val_accuracy: 0.7996\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2915 - accuracy: 0.8777 - val_loss: 0.4608 - val_accuracy: 0.7996\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2871 - accuracy: 0.8811 - val_loss: 0.4625 - val_accuracy: 0.8093\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.2807 - accuracy: 0.8860 - val_loss: 0.4673 - val_accuracy: 0.7938\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.2774 - accuracy: 0.8835 - val_loss: 0.4727 - val_accuracy: 0.7957\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2765 - accuracy: 0.8806 - val_loss: 0.4785 - val_accuracy: 0.7879\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.2752 - accuracy: 0.8796 - val_loss: 0.4802 - val_accuracy: 0.7899\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.2718 - accuracy: 0.8855 - val_loss: 0.4807 - val_accuracy: 0.7879\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.2688 - accuracy: 0.8879 - val_loss: 0.4817 - val_accuracy: 0.7957\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2666 - accuracy: 0.8879 - val_loss: 0.4814 - val_accuracy: 0.7977\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2650 - accuracy: 0.8860 - val_loss: 0.4819 - val_accuracy: 0.7938\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.2621 - accuracy: 0.8869 - val_loss: 0.4829 - val_accuracy: 0.7899\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2570 - accuracy: 0.8933 - val_loss: 0.4869 - val_accuracy: 0.7957\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2537 - accuracy: 0.8991 - val_loss: 0.4879 - val_accuracy: 0.7938\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2499 - accuracy: 0.8996 - val_loss: 0.4849 - val_accuracy: 0.7840\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2456 - accuracy: 0.8996 - val_loss: 0.4870 - val_accuracy: 0.7938\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2453 - accuracy: 0.8981 - val_loss: 0.4932 - val_accuracy: 0.7899\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.2447 - accuracy: 0.8991 - val_loss: 0.4996 - val_accuracy: 0.7918\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2412 - accuracy: 0.9020 - val_loss: 0.5085 - val_accuracy: 0.7821\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2380 - accuracy: 0.9025 - val_loss: 0.5194 - val_accuracy: 0.7840\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2378 - accuracy: 0.9069 - val_loss: 0.5207 - val_accuracy: 0.7821\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.2356 - accuracy: 0.9084 - val_loss: 0.5159 - val_accuracy: 0.7899\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.2327 - accuracy: 0.9103 - val_loss: 0.5131 - val_accuracy: 0.7957\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.2298 - accuracy: 0.9103 - val_loss: 0.5131 - val_accuracy: 0.7899\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.2286 - accuracy: 0.9113 - val_loss: 0.5140 - val_accuracy: 0.7899\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.2282 - accuracy: 0.9128 - val_loss: 0.5184 - val_accuracy: 0.7743\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.2283 - accuracy: 0.8996 - val_loss: 0.5245 - val_accuracy: 0.7763\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.2291 - accuracy: 0.8972 - val_loss: 0.5335 - val_accuracy: 0.7685\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2299 - accuracy: 0.8947 - val_loss: 0.5368 - val_accuracy: 0.7665\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.2268 - accuracy: 0.9011 - val_loss: 0.5265 - val_accuracy: 0.7918\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.2170 - accuracy: 0.9074 - val_loss: 0.5176 - val_accuracy: 0.7977\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2110 - accuracy: 0.9142 - val_loss: 0.5160 - val_accuracy: 0.8093\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.2081 - accuracy: 0.9137 - val_loss: 0.5214 - val_accuracy: 0.8035\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.2050 - accuracy: 0.9186 - val_loss: 0.5282 - val_accuracy: 0.8016\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.2042 - accuracy: 0.9181 - val_loss: 0.5312 - val_accuracy: 0.7938\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2038 - accuracy: 0.9191 - val_loss: 0.5350 - val_accuracy: 0.7996\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.2083 - accuracy: 0.9206 - val_loss: 0.5325 - val_accuracy: 0.8113\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.2022 - accuracy: 0.9215 - val_loss: 0.5358 - val_accuracy: 0.8191\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.1967 - accuracy: 0.9215 - val_loss: 0.5467 - val_accuracy: 0.8074\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1988 - accuracy: 0.9186 - val_loss: 0.5529 - val_accuracy: 0.7996\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1999 - accuracy: 0.9181 - val_loss: 0.5518 - val_accuracy: 0.7957\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1966 - accuracy: 0.9176 - val_loss: 0.5433 - val_accuracy: 0.7996\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1902 - accuracy: 0.9254 - val_loss: 0.5386 - val_accuracy: 0.8035\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1878 - accuracy: 0.9245 - val_loss: 0.5422 - val_accuracy: 0.8054\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1859 - accuracy: 0.9259 - val_loss: 0.5480 - val_accuracy: 0.8016\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1790 - accuracy: 0.9313 - val_loss: 0.5575 - val_accuracy: 0.8016\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.1760 - accuracy: 0.9298 - val_loss: 0.5646 - val_accuracy: 0.8016\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1759 - accuracy: 0.9323 - val_loss: 0.5685 - val_accuracy: 0.7977\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1759 - accuracy: 0.9337 - val_loss: 0.5674 - val_accuracy: 0.8132\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1743 - accuracy: 0.9381 - val_loss: 0.5643 - val_accuracy: 0.8113\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1749 - accuracy: 0.9347 - val_loss: 0.5636 - val_accuracy: 0.8191\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1758 - accuracy: 0.9279 - val_loss: 0.5641 - val_accuracy: 0.8035\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1713 - accuracy: 0.9308 - val_loss: 0.5767 - val_accuracy: 0.7918\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1787 - accuracy: 0.9308 - val_loss: 0.5720 - val_accuracy: 0.7996\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1737 - accuracy: 0.9293 - val_loss: 0.5618 - val_accuracy: 0.8152\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1657 - accuracy: 0.9357 - val_loss: 0.5623 - val_accuracy: 0.8191\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1659 - accuracy: 0.9371 - val_loss: 0.5648 - val_accuracy: 0.8171\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1624 - accuracy: 0.9386 - val_loss: 0.5723 - val_accuracy: 0.8191\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1607 - accuracy: 0.9357 - val_loss: 0.5819 - val_accuracy: 0.8152\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1625 - accuracy: 0.9357 - val_loss: 0.5857 - val_accuracy: 0.8191\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.1613 - accuracy: 0.9371 - val_loss: 0.5893 - val_accuracy: 0.8093\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1581 - accuracy: 0.9410 - val_loss: 0.5936 - val_accuracy: 0.8074\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1567 - accuracy: 0.9376 - val_loss: 0.5926 - val_accuracy: 0.8074\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1556 - accuracy: 0.9410 - val_loss: 0.5895 - val_accuracy: 0.8113\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1564 - accuracy: 0.9391 - val_loss: 0.5876 - val_accuracy: 0.8113\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1584 - accuracy: 0.9401 - val_loss: 0.5855 - val_accuracy: 0.8093\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1544 - accuracy: 0.9401 - val_loss: 0.5874 - val_accuracy: 0.8152\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1501 - accuracy: 0.9435 - val_loss: 0.5922 - val_accuracy: 0.8132\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.1484 - accuracy: 0.9435 - val_loss: 0.5879 - val_accuracy: 0.8191\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.1442 - accuracy: 0.9454 - val_loss: 0.5854 - val_accuracy: 0.8268\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 0.1489 - accuracy: 0.9459 - val_loss: 0.5876 - val_accuracy: 0.8210\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1538 - accuracy: 0.9386 - val_loss: 0.5874 - val_accuracy: 0.8191\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1481 - accuracy: 0.9435 - val_loss: 0.5976 - val_accuracy: 0.8132\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1464 - accuracy: 0.9435 - val_loss: 0.6066 - val_accuracy: 0.8035\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1476 - accuracy: 0.9415 - val_loss: 0.6070 - val_accuracy: 0.8093\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.1451 - accuracy: 0.9440 - val_loss: 0.6121 - val_accuracy: 0.8016\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1442 - accuracy: 0.9415 - val_loss: 0.6162 - val_accuracy: 0.7918\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1433 - accuracy: 0.9376 - val_loss: 0.6136 - val_accuracy: 0.7996\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1397 - accuracy: 0.9440 - val_loss: 0.6081 - val_accuracy: 0.8152\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1370 - accuracy: 0.9459 - val_loss: 0.6091 - val_accuracy: 0.8230\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1393 - accuracy: 0.9454 - val_loss: 0.6180 - val_accuracy: 0.8268\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.1421 - accuracy: 0.9459 - val_loss: 0.6329 - val_accuracy: 0.8171\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1461 - accuracy: 0.9425 - val_loss: 0.6355 - val_accuracy: 0.8132\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1422 - accuracy: 0.9430 - val_loss: 0.6350 - val_accuracy: 0.8132\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1347 - accuracy: 0.9479 - val_loss: 0.6378 - val_accuracy: 0.7996\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1316 - accuracy: 0.9474 - val_loss: 0.6316 - val_accuracy: 0.8074\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.1305 - accuracy: 0.9454 - val_loss: 0.6247 - val_accuracy: 0.8152\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1335 - accuracy: 0.9488 - val_loss: 0.6234 - val_accuracy: 0.8268\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.1320 - accuracy: 0.9498 - val_loss: 0.6312 - val_accuracy: 0.8230\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1250 - accuracy: 0.9542 - val_loss: 0.6544 - val_accuracy: 0.8054\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1286 - accuracy: 0.9513 - val_loss: 0.6760 - val_accuracy: 0.7996\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1358 - accuracy: 0.9527 - val_loss: 0.6686 - val_accuracy: 0.7977\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1289 - accuracy: 0.9542 - val_loss: 0.6566 - val_accuracy: 0.8035\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1233 - accuracy: 0.9527 - val_loss: 0.6578 - val_accuracy: 0.8093\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1240 - accuracy: 0.9532 - val_loss: 0.6643 - val_accuracy: 0.7957\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1227 - accuracy: 0.9498 - val_loss: 0.6679 - val_accuracy: 0.8016\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1221 - accuracy: 0.9493 - val_loss: 0.6691 - val_accuracy: 0.8093\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1202 - accuracy: 0.9508 - val_loss: 0.6790 - val_accuracy: 0.7938\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1190 - accuracy: 0.9527 - val_loss: 0.6912 - val_accuracy: 0.7938\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1232 - accuracy: 0.9493 - val_loss: 0.6867 - val_accuracy: 0.8074\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1196 - accuracy: 0.9542 - val_loss: 0.6826 - val_accuracy: 0.7996\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.1198 - accuracy: 0.9542 - val_loss: 0.6910 - val_accuracy: 0.8113\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1254 - accuracy: 0.9571 - val_loss: 0.6883 - val_accuracy: 0.8132\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1176 - accuracy: 0.9576 - val_loss: 0.6870 - val_accuracy: 0.8152\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1097 - accuracy: 0.9620 - val_loss: 0.6952 - val_accuracy: 0.8074\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1117 - accuracy: 0.9527 - val_loss: 0.7116 - val_accuracy: 0.8113\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.1234 - accuracy: 0.9483 - val_loss: 0.7261 - val_accuracy: 0.8016\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1304 - accuracy: 0.9493 - val_loss: 0.7271 - val_accuracy: 0.7899\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1292 - accuracy: 0.9454 - val_loss: 0.7196 - val_accuracy: 0.7918\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1238 - accuracy: 0.9474 - val_loss: 0.7109 - val_accuracy: 0.7899\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1182 - accuracy: 0.9527 - val_loss: 0.7054 - val_accuracy: 0.8035\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.1162 - accuracy: 0.9537 - val_loss: 0.7027 - val_accuracy: 0.8132\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1198 - accuracy: 0.9557 - val_loss: 0.7063 - val_accuracy: 0.8132\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1219 - accuracy: 0.9571 - val_loss: 0.7095 - val_accuracy: 0.8191\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.1184 - accuracy: 0.9576 - val_loss: 0.7137 - val_accuracy: 0.8191\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1159 - accuracy: 0.9600 - val_loss: 0.7218 - val_accuracy: 0.8074\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1147 - accuracy: 0.9581 - val_loss: 0.7343 - val_accuracy: 0.7860\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.1173 - accuracy: 0.9552 - val_loss: 0.7455 - val_accuracy: 0.7899\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.1199 - accuracy: 0.9532 - val_loss: 0.7504 - val_accuracy: 0.7879\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1180 - accuracy: 0.9547 - val_loss: 0.7482 - val_accuracy: 0.8054\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1134 - accuracy: 0.9552 - val_loss: 0.7403 - val_accuracy: 0.8191\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1068 - accuracy: 0.9591 - val_loss: 0.7340 - val_accuracy: 0.8152\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.1012 - accuracy: 0.9605 - val_loss: 0.7327 - val_accuracy: 0.8249\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0992 - accuracy: 0.9630 - val_loss: 0.7337 - val_accuracy: 0.8230\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0992 - accuracy: 0.9610 - val_loss: 0.7355 - val_accuracy: 0.8210\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0993 - accuracy: 0.9596 - val_loss: 0.7374 - val_accuracy: 0.8152\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1002 - accuracy: 0.9586 - val_loss: 0.7400 - val_accuracy: 0.8152\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.1013 - accuracy: 0.9581 - val_loss: 0.7447 - val_accuracy: 0.8210\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1041 - accuracy: 0.9581 - val_loss: 0.7498 - val_accuracy: 0.8268\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1072 - accuracy: 0.9581 - val_loss: 0.7536 - val_accuracy: 0.8288\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1084 - accuracy: 0.9591 - val_loss: 0.7577 - val_accuracy: 0.8268\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1074 - accuracy: 0.9586 - val_loss: 0.7647 - val_accuracy: 0.8132\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.1044 - accuracy: 0.9557 - val_loss: 0.7741 - val_accuracy: 0.8113\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1018 - accuracy: 0.9581 - val_loss: 0.7787 - val_accuracy: 0.8191\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 250ms/step - loss: 0.0999 - accuracy: 0.9586 - val_loss: 0.7854 - val_accuracy: 0.8171\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1003 - accuracy: 0.9581 - val_loss: 0.7947 - val_accuracy: 0.8171\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1021 - accuracy: 0.9566 - val_loss: 0.7967 - val_accuracy: 0.8191\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.1020 - accuracy: 0.9561 - val_loss: 0.7969 - val_accuracy: 0.8249\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1033 - accuracy: 0.9586 - val_loss: 0.8060 - val_accuracy: 0.8132\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1103 - accuracy: 0.9576 - val_loss: 0.8126 - val_accuracy: 0.8093\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.1148 - accuracy: 0.9557 - val_loss: 0.8040 - val_accuracy: 0.8093\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1075 - accuracy: 0.9571 - val_loss: 0.7879 - val_accuracy: 0.8113\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0972 - accuracy: 0.9591 - val_loss: 0.7727 - val_accuracy: 0.8152\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0944 - accuracy: 0.9630 - val_loss: 0.7603 - val_accuracy: 0.8210\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0973 - accuracy: 0.9600 - val_loss: 0.7511 - val_accuracy: 0.8171\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0999 - accuracy: 0.9591 - val_loss: 0.7494 - val_accuracy: 0.8113\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1014 - accuracy: 0.9600 - val_loss: 0.7504 - val_accuracy: 0.8093\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.1002 - accuracy: 0.9615 - val_loss: 0.7508 - val_accuracy: 0.8132\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0985 - accuracy: 0.9600 - val_loss: 0.7549 - val_accuracy: 0.8093\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0973 - accuracy: 0.9615 - val_loss: 0.7631 - val_accuracy: 0.8113\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0973 - accuracy: 0.9630 - val_loss: 0.7674 - val_accuracy: 0.8171\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0979 - accuracy: 0.9610 - val_loss: 0.7755 - val_accuracy: 0.8210\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.1017 - accuracy: 0.9581 - val_loss: 0.7854 - val_accuracy: 0.8191\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.0997 - accuracy: 0.9615 - val_loss: 0.8004 - val_accuracy: 0.8152\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0966 - accuracy: 0.9605 - val_loss: 0.8159 - val_accuracy: 0.8016\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0961 - accuracy: 0.9566 - val_loss: 0.8128 - val_accuracy: 0.8132\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0934 - accuracy: 0.9600 - val_loss: 0.8126 - val_accuracy: 0.8093\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0982 - accuracy: 0.9537 - val_loss: 0.8122 - val_accuracy: 0.8093\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0997 - accuracy: 0.9571 - val_loss: 0.8048 - val_accuracy: 0.8152\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0964 - accuracy: 0.9635 - val_loss: 0.7996 - val_accuracy: 0.8191\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0935 - accuracy: 0.9644 - val_loss: 0.7994 - val_accuracy: 0.8191\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0930 - accuracy: 0.9644 - val_loss: 0.8036 - val_accuracy: 0.8210\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0982 - accuracy: 0.9678 - val_loss: 0.7989 - val_accuracy: 0.8307\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0986 - accuracy: 0.9698 - val_loss: 0.7914 - val_accuracy: 0.8288\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0946 - accuracy: 0.9698 - val_loss: 0.7875 - val_accuracy: 0.8152\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0916 - accuracy: 0.9678 - val_loss: 0.7863 - val_accuracy: 0.8132\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0909 - accuracy: 0.9649 - val_loss: 0.7831 - val_accuracy: 0.8113\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0893 - accuracy: 0.9625 - val_loss: 0.7801 - val_accuracy: 0.8171\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0881 - accuracy: 0.9654 - val_loss: 0.7826 - val_accuracy: 0.8191\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0878 - accuracy: 0.9664 - val_loss: 0.7888 - val_accuracy: 0.8210\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0880 - accuracy: 0.9673 - val_loss: 0.7972 - val_accuracy: 0.8230\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0894 - accuracy: 0.9659 - val_loss: 0.8038 - val_accuracy: 0.8152\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0896 - accuracy: 0.9678 - val_loss: 0.8045 - val_accuracy: 0.8152\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0867 - accuracy: 0.9654 - val_loss: 0.8011 - val_accuracy: 0.8093\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0839 - accuracy: 0.9673 - val_loss: 0.8005 - val_accuracy: 0.8113\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0847 - accuracy: 0.9630 - val_loss: 0.8037 - val_accuracy: 0.8132\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0860 - accuracy: 0.9625 - val_loss: 0.8129 - val_accuracy: 0.8074\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0846 - accuracy: 0.9630 - val_loss: 0.8263 - val_accuracy: 0.8093\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0857 - accuracy: 0.9620 - val_loss: 0.8320 - val_accuracy: 0.8054\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0855 - accuracy: 0.9586 - val_loss: 0.8254 - val_accuracy: 0.8152\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0825 - accuracy: 0.9659 - val_loss: 0.8314 - val_accuracy: 0.8288\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0943 - accuracy: 0.9639 - val_loss: 0.8412 - val_accuracy: 0.8268\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.8385 - val_accuracy: 0.8191\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0863 - accuracy: 0.9659 - val_loss: 0.8591 - val_accuracy: 0.8074\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0866 - accuracy: 0.9664 - val_loss: 0.8815 - val_accuracy: 0.8016\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0964 - accuracy: 0.9639 - val_loss: 0.8760 - val_accuracy: 0.8093\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0954 - accuracy: 0.9654 - val_loss: 0.8668 - val_accuracy: 0.8152\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0966 - accuracy: 0.9605 - val_loss: 0.8639 - val_accuracy: 0.8132\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.1028 - accuracy: 0.9605 - val_loss: 0.8551 - val_accuracy: 0.8152\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.1006 - accuracy: 0.9610 - val_loss: 0.8412 - val_accuracy: 0.8171\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0936 - accuracy: 0.9615 - val_loss: 0.8305 - val_accuracy: 0.8249\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0880 - accuracy: 0.9639 - val_loss: 0.8281 - val_accuracy: 0.8132\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0854 - accuracy: 0.9615 - val_loss: 0.8353 - val_accuracy: 0.8152\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0868 - accuracy: 0.9635 - val_loss: 0.8402 - val_accuracy: 0.8093\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0864 - accuracy: 0.9625 - val_loss: 0.8413 - val_accuracy: 0.8093\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0835 - accuracy: 0.9610 - val_loss: 0.8414 - val_accuracy: 0.8016\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0811 - accuracy: 0.9610 - val_loss: 0.8397 - val_accuracy: 0.8113\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0797 - accuracy: 0.9644 - val_loss: 0.8433 - val_accuracy: 0.8132\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0794 - accuracy: 0.9673 - val_loss: 0.8508 - val_accuracy: 0.8132\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.0796 - accuracy: 0.9649 - val_loss: 0.8601 - val_accuracy: 0.8152\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0817 - accuracy: 0.9664 - val_loss: 0.8722 - val_accuracy: 0.8113\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0885 - accuracy: 0.9625 - val_loss: 0.8751 - val_accuracy: 0.8132\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0894 - accuracy: 0.9630 - val_loss: 0.8665 - val_accuracy: 0.8113\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0836 - accuracy: 0.9659 - val_loss: 0.8579 - val_accuracy: 0.8249\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0820 - accuracy: 0.9698 - val_loss: 0.8540 - val_accuracy: 0.8288\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0867 - accuracy: 0.9683 - val_loss: 0.8489 - val_accuracy: 0.8210\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0839 - accuracy: 0.9678 - val_loss: 0.8677 - val_accuracy: 0.8035\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0894 - accuracy: 0.9630 - val_loss: 0.8934 - val_accuracy: 0.8054\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.1011 - accuracy: 0.9591 - val_loss: 0.8775 - val_accuracy: 0.8054\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0906 - accuracy: 0.9600 - val_loss: 0.8549 - val_accuracy: 0.8249\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0812 - accuracy: 0.9639 - val_loss: 0.8584 - val_accuracy: 0.8191\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0853 - accuracy: 0.9625 - val_loss: 0.8657 - val_accuracy: 0.8210\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0867 - accuracy: 0.9688 - val_loss: 0.8763 - val_accuracy: 0.8171\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0890 - accuracy: 0.9678 - val_loss: 0.8872 - val_accuracy: 0.8113\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0912 - accuracy: 0.9659 - val_loss: 0.8974 - val_accuracy: 0.8074\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0921 - accuracy: 0.9659 - val_loss: 0.8864 - val_accuracy: 0.8171\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0842 - accuracy: 0.9698 - val_loss: 0.8674 - val_accuracy: 0.8210\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0798 - accuracy: 0.9688 - val_loss: 0.8649 - val_accuracy: 0.8152\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0837 - accuracy: 0.9649 - val_loss: 0.8634 - val_accuracy: 0.8191\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0845 - accuracy: 0.9625 - val_loss: 0.8670 - val_accuracy: 0.8132\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0854 - accuracy: 0.9615 - val_loss: 0.8840 - val_accuracy: 0.8074\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0907 - accuracy: 0.9600 - val_loss: 0.9102 - val_accuracy: 0.7977\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1035 - accuracy: 0.9586 - val_loss: 0.9111 - val_accuracy: 0.7938\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1038 - accuracy: 0.9620 - val_loss: 0.8886 - val_accuracy: 0.8074\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0925 - accuracy: 0.9620 - val_loss: 0.8701 - val_accuracy: 0.8093\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0848 - accuracy: 0.9659 - val_loss: 0.8625 - val_accuracy: 0.8210\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0821 - accuracy: 0.9683 - val_loss: 0.8607 - val_accuracy: 0.8230\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0819 - accuracy: 0.9693 - val_loss: 0.8639 - val_accuracy: 0.8230\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0811 - accuracy: 0.9703 - val_loss: 0.8731 - val_accuracy: 0.8171\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0812 - accuracy: 0.9688 - val_loss: 0.8866 - val_accuracy: 0.8113\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0855 - accuracy: 0.9630 - val_loss: 0.8978 - val_accuracy: 0.7977\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0886 - accuracy: 0.9605 - val_loss: 0.9014 - val_accuracy: 0.7977\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0885 - accuracy: 0.9635 - val_loss: 0.8967 - val_accuracy: 0.7977\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0854 - accuracy: 0.9649 - val_loss: 0.8899 - val_accuracy: 0.8074\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0815 - accuracy: 0.9654 - val_loss: 0.8884 - val_accuracy: 0.8152\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0806 - accuracy: 0.9678 - val_loss: 0.8876 - val_accuracy: 0.8171\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0806 - accuracy: 0.9669 - val_loss: 0.8784 - val_accuracy: 0.8230\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0780 - accuracy: 0.9712 - val_loss: 0.8752 - val_accuracy: 0.8249\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0817 - accuracy: 0.9703 - val_loss: 0.8759 - val_accuracy: 0.8268\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0864 - accuracy: 0.9693 - val_loss: 0.8724 - val_accuracy: 0.8230\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 229ms/step - loss: 0.0843 - accuracy: 0.9678 - val_loss: 0.8756 - val_accuracy: 0.8152\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0829 - accuracy: 0.9688 - val_loss: 0.8798 - val_accuracy: 0.8074\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0846 - accuracy: 0.9669 - val_loss: 0.8814 - val_accuracy: 0.7996\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0863 - accuracy: 0.9659 - val_loss: 0.8741 - val_accuracy: 0.8074\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0852 - accuracy: 0.9659 - val_loss: 0.8654 - val_accuracy: 0.8093\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0848 - accuracy: 0.9664 - val_loss: 0.8637 - val_accuracy: 0.7977\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0864 - accuracy: 0.9591 - val_loss: 0.8614 - val_accuracy: 0.8093\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0851 - accuracy: 0.9620 - val_loss: 0.8594 - val_accuracy: 0.8171\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0836 - accuracy: 0.9630 - val_loss: 0.8621 - val_accuracy: 0.8152\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0847 - accuracy: 0.9625 - val_loss: 0.8717 - val_accuracy: 0.8074\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0855 - accuracy: 0.9678 - val_loss: 0.8908 - val_accuracy: 0.8054\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0892 - accuracy: 0.9639 - val_loss: 0.9059 - val_accuracy: 0.8054\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0925 - accuracy: 0.9664 - val_loss: 0.8984 - val_accuracy: 0.8074\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0854 - accuracy: 0.9630 - val_loss: 0.8901 - val_accuracy: 0.8132\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0783 - accuracy: 0.9639 - val_loss: 0.8860 - val_accuracy: 0.8191\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0752 - accuracy: 0.9698 - val_loss: 0.8859 - val_accuracy: 0.8249\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0757 - accuracy: 0.9678 - val_loss: 0.8891 - val_accuracy: 0.8210\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0784 - accuracy: 0.9644 - val_loss: 0.8931 - val_accuracy: 0.8230\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0791 - accuracy: 0.9654 - val_loss: 0.9022 - val_accuracy: 0.8113\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0818 - accuracy: 0.9635 - val_loss: 0.9059 - val_accuracy: 0.8132\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0837 - accuracy: 0.9659 - val_loss: 0.9007 - val_accuracy: 0.8230\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0827 - accuracy: 0.9644 - val_loss: 0.8947 - val_accuracy: 0.8249\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0826 - accuracy: 0.9635 - val_loss: 0.8916 - val_accuracy: 0.8268\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0841 - accuracy: 0.9635 - val_loss: 0.8897 - val_accuracy: 0.8288\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0849 - accuracy: 0.9635 - val_loss: 0.8894 - val_accuracy: 0.8249\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0837 - accuracy: 0.9644 - val_loss: 0.8912 - val_accuracy: 0.8249\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0807 - accuracy: 0.9669 - val_loss: 0.8988 - val_accuracy: 0.8268\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0781 - accuracy: 0.9683 - val_loss: 0.9097 - val_accuracy: 0.8288\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0772 - accuracy: 0.9698 - val_loss: 0.9177 - val_accuracy: 0.8210\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0788 - accuracy: 0.9678 - val_loss: 0.9197 - val_accuracy: 0.8171\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0830 - accuracy: 0.9644 - val_loss: 0.9254 - val_accuracy: 0.8113\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0905 - accuracy: 0.9615 - val_loss: 0.9258 - val_accuracy: 0.8074\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0926 - accuracy: 0.9581 - val_loss: 0.9136 - val_accuracy: 0.8210\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0883 - accuracy: 0.9600 - val_loss: 0.9017 - val_accuracy: 0.8268\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0816 - accuracy: 0.9698 - val_loss: 0.8983 - val_accuracy: 0.8210\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0801 - accuracy: 0.9703 - val_loss: 0.8987 - val_accuracy: 0.8152\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0801 - accuracy: 0.9669 - val_loss: 0.9019 - val_accuracy: 0.8132\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0805 - accuracy: 0.9639 - val_loss: 0.9083 - val_accuracy: 0.8210\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0806 - accuracy: 0.9669 - val_loss: 0.9072 - val_accuracy: 0.8230\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0782 - accuracy: 0.9673 - val_loss: 0.9033 - val_accuracy: 0.8191\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.0770 - accuracy: 0.9678 - val_loss: 0.9055 - val_accuracy: 0.8191\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0770 - accuracy: 0.9683 - val_loss: 0.9116 - val_accuracy: 0.8132\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0773 - accuracy: 0.9659 - val_loss: 0.9193 - val_accuracy: 0.8113\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0747 - accuracy: 0.9664 - val_loss: 0.9336 - val_accuracy: 0.7996\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0764 - accuracy: 0.9620 - val_loss: 0.9419 - val_accuracy: 0.8074\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0783 - accuracy: 0.9654 - val_loss: 0.9348 - val_accuracy: 0.8171\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0758 - accuracy: 0.9673 - val_loss: 0.9288 - val_accuracy: 0.8171\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0754 - accuracy: 0.9678 - val_loss: 0.9188 - val_accuracy: 0.8249\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0751 - accuracy: 0.9698 - val_loss: 0.9116 - val_accuracy: 0.8288\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0747 - accuracy: 0.9708 - val_loss: 0.9134 - val_accuracy: 0.8230\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0747 - accuracy: 0.9703 - val_loss: 0.9167 - val_accuracy: 0.8132\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0745 - accuracy: 0.9688 - val_loss: 0.9192 - val_accuracy: 0.8191\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0727 - accuracy: 0.9698 - val_loss: 0.9273 - val_accuracy: 0.8171\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0707 - accuracy: 0.9712 - val_loss: 0.9524 - val_accuracy: 0.8016\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0811 - accuracy: 0.9649 - val_loss: 0.9595 - val_accuracy: 0.7996\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0865 - accuracy: 0.9654 - val_loss: 0.9364 - val_accuracy: 0.8016\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0818 - accuracy: 0.9615 - val_loss: 0.9353 - val_accuracy: 0.8152\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.1016 - accuracy: 0.9552 - val_loss: 0.9374 - val_accuracy: 0.8171\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.1096 - accuracy: 0.9571 - val_loss: 0.9267 - val_accuracy: 0.8152\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0934 - accuracy: 0.9610 - val_loss: 0.9260 - val_accuracy: 0.8171\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0792 - accuracy: 0.9669 - val_loss: 0.9397 - val_accuracy: 0.8171\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0766 - accuracy: 0.9698 - val_loss: 0.9570 - val_accuracy: 0.8074\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0826 - accuracy: 0.9693 - val_loss: 0.9619 - val_accuracy: 0.8074\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0866 - accuracy: 0.9664 - val_loss: 0.9544 - val_accuracy: 0.8132\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0859 - accuracy: 0.9664 - val_loss: 0.9381 - val_accuracy: 0.8171\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0824 - accuracy: 0.9659 - val_loss: 0.9364 - val_accuracy: 0.8288\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0869 - accuracy: 0.9664 - val_loss: 0.9422 - val_accuracy: 0.8249\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0913 - accuracy: 0.9639 - val_loss: 0.9345 - val_accuracy: 0.8132\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0823 - accuracy: 0.9639 - val_loss: 0.9339 - val_accuracy: 0.8132\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0750 - accuracy: 0.9664 - val_loss: 0.9438 - val_accuracy: 0.8054\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 253ms/step - loss: 0.0768 - accuracy: 0.9625 - val_loss: 0.9509 - val_accuracy: 0.8035\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0807 - accuracy: 0.9620 - val_loss: 0.9554 - val_accuracy: 0.8054\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0839 - accuracy: 0.9630 - val_loss: 0.9460 - val_accuracy: 0.7996\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0806 - accuracy: 0.9630 - val_loss: 0.9230 - val_accuracy: 0.8191\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0734 - accuracy: 0.9693 - val_loss: 0.9103 - val_accuracy: 0.8230\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0744 - accuracy: 0.9678 - val_loss: 0.9127 - val_accuracy: 0.8210\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0779 - accuracy: 0.9673 - val_loss: 0.9154 - val_accuracy: 0.8249\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0769 - accuracy: 0.9693 - val_loss: 0.9171 - val_accuracy: 0.8268\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0748 - accuracy: 0.9712 - val_loss: 0.9227 - val_accuracy: 0.8288\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0742 - accuracy: 0.9703 - val_loss: 0.9367 - val_accuracy: 0.8191\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0782 - accuracy: 0.9683 - val_loss: 0.9512 - val_accuracy: 0.8132\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0824 - accuracy: 0.9688 - val_loss: 0.9535 - val_accuracy: 0.8191\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0810 - accuracy: 0.9669 - val_loss: 0.9455 - val_accuracy: 0.8171\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0791 - accuracy: 0.9673 - val_loss: 0.9431 - val_accuracy: 0.8230\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0822 - accuracy: 0.9688 - val_loss: 0.9474 - val_accuracy: 0.8230\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0798 - accuracy: 0.9678 - val_loss: 0.9560 - val_accuracy: 0.8191\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0760 - accuracy: 0.9683 - val_loss: 0.9678 - val_accuracy: 0.8230\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.0812 - accuracy: 0.9659 - val_loss: 0.9752 - val_accuracy: 0.8210\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0880 - accuracy: 0.9649 - val_loss: 0.9773 - val_accuracy: 0.8191\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0887 - accuracy: 0.9639 - val_loss: 0.9799 - val_accuracy: 0.8171\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 138ms/step - loss: 0.0852 - accuracy: 0.9644 - val_loss: 0.9739 - val_accuracy: 0.8171\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0807 - accuracy: 0.9654 - val_loss: 0.9553 - val_accuracy: 0.8171\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0753 - accuracy: 0.9693 - val_loss: 0.9425 - val_accuracy: 0.8171\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0738 - accuracy: 0.9669 - val_loss: 0.9370 - val_accuracy: 0.8132\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0746 - accuracy: 0.9635 - val_loss: 0.9375 - val_accuracy: 0.8132\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0773 - accuracy: 0.9639 - val_loss: 0.9353 - val_accuracy: 0.8152\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0772 - accuracy: 0.9620 - val_loss: 0.9293 - val_accuracy: 0.8171\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0734 - accuracy: 0.9644 - val_loss: 0.9304 - val_accuracy: 0.8210\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0736 - accuracy: 0.9688 - val_loss: 0.9367 - val_accuracy: 0.8191\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0770 - accuracy: 0.9688 - val_loss: 0.9442 - val_accuracy: 0.8113\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0795 - accuracy: 0.9654 - val_loss: 0.9459 - val_accuracy: 0.8230\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0792 - accuracy: 0.9683 - val_loss: 0.9398 - val_accuracy: 0.8268\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.0783 - accuracy: 0.9683 - val_loss: 0.9385 - val_accuracy: 0.8268\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0812 - accuracy: 0.9673 - val_loss: 0.9396 - val_accuracy: 0.8230\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0798 - accuracy: 0.9664 - val_loss: 0.9380 - val_accuracy: 0.8191\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0727 - accuracy: 0.9688 - val_loss: 0.9437 - val_accuracy: 0.8093\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0717 - accuracy: 0.9673 - val_loss: 0.9490 - val_accuracy: 0.8074\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0740 - accuracy: 0.9669 - val_loss: 0.9532 - val_accuracy: 0.8074\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0769 - accuracy: 0.9669 - val_loss: 0.9565 - val_accuracy: 0.8054\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0801 - accuracy: 0.9673 - val_loss: 0.9464 - val_accuracy: 0.8016\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0767 - accuracy: 0.9654 - val_loss: 0.9303 - val_accuracy: 0.8132\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.0708 - accuracy: 0.9688 - val_loss: 0.9262 - val_accuracy: 0.8191\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0755 - accuracy: 0.9678 - val_loss: 0.9319 - val_accuracy: 0.8210\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0858 - accuracy: 0.9659 - val_loss: 0.9285 - val_accuracy: 0.8230\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0836 - accuracy: 0.9698 - val_loss: 0.9302 - val_accuracy: 0.8191\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0802 - accuracy: 0.9703 - val_loss: 0.9422 - val_accuracy: 0.8074\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0821 - accuracy: 0.9669 - val_loss: 0.9559 - val_accuracy: 0.8093\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0859 - accuracy: 0.9669 - val_loss: 0.9540 - val_accuracy: 0.8093\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0835 - accuracy: 0.9669 - val_loss: 0.9438 - val_accuracy: 0.8093\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0772 - accuracy: 0.9683 - val_loss: 0.9369 - val_accuracy: 0.8191\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0723 - accuracy: 0.9703 - val_loss: 0.9336 - val_accuracy: 0.8230\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0700 - accuracy: 0.9717 - val_loss: 0.9361 - val_accuracy: 0.8171\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0706 - accuracy: 0.9717 - val_loss: 0.9377 - val_accuracy: 0.8152\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0719 - accuracy: 0.9693 - val_loss: 0.9347 - val_accuracy: 0.8132\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0719 - accuracy: 0.9688 - val_loss: 0.9315 - val_accuracy: 0.8152\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0718 - accuracy: 0.9693 - val_loss: 0.9300 - val_accuracy: 0.8191\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0725 - accuracy: 0.9693 - val_loss: 0.9308 - val_accuracy: 0.8191\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0747 - accuracy: 0.9693 - val_loss: 0.9331 - val_accuracy: 0.8171\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0750 - accuracy: 0.9654 - val_loss: 0.9378 - val_accuracy: 0.8210\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0749 - accuracy: 0.9649 - val_loss: 0.9432 - val_accuracy: 0.8268\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0748 - accuracy: 0.9659 - val_loss: 0.9467 - val_accuracy: 0.8210\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0742 - accuracy: 0.9669 - val_loss: 0.9514 - val_accuracy: 0.8230\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0743 - accuracy: 0.9669 - val_loss: 0.9566 - val_accuracy: 0.8152\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0749 - accuracy: 0.9625 - val_loss: 0.9561 - val_accuracy: 0.8171\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0732 - accuracy: 0.9639 - val_loss: 0.9499 - val_accuracy: 0.8171\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0698 - accuracy: 0.9669 - val_loss: 0.9451 - val_accuracy: 0.8230\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.0685 - accuracy: 0.9703 - val_loss: 0.9466 - val_accuracy: 0.8191\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0713 - accuracy: 0.9664 - val_loss: 0.9498 - val_accuracy: 0.8210\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0740 - accuracy: 0.9673 - val_loss: 0.9448 - val_accuracy: 0.8230\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 0.0687 - accuracy: 0.9678 - val_loss: 0.9444 - val_accuracy: 0.8249\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0654 - accuracy: 0.9693 - val_loss: 0.9529 - val_accuracy: 0.8249\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0682 - accuracy: 0.9688 - val_loss: 0.9591 - val_accuracy: 0.8230\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0709 - accuracy: 0.9688 - val_loss: 0.9630 - val_accuracy: 0.8191\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0723 - accuracy: 0.9717 - val_loss: 0.9613 - val_accuracy: 0.8210\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0716 - accuracy: 0.9717 - val_loss: 0.9520 - val_accuracy: 0.8210\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0682 - accuracy: 0.9717 - val_loss: 0.9461 - val_accuracy: 0.8249\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0676 - accuracy: 0.9698 - val_loss: 0.9489 - val_accuracy: 0.8210\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0691 - accuracy: 0.9639 - val_loss: 0.9537 - val_accuracy: 0.8288\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 206ms/step - loss: 0.0708 - accuracy: 0.9678 - val_loss: 0.9589 - val_accuracy: 0.8288\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0726 - accuracy: 0.9678 - val_loss: 0.9572 - val_accuracy: 0.8230\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0679 - accuracy: 0.9688 - val_loss: 0.9627 - val_accuracy: 0.8268\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0656 - accuracy: 0.9703 - val_loss: 0.9763 - val_accuracy: 0.8171\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0684 - accuracy: 0.9698 - val_loss: 0.9858 - val_accuracy: 0.8132\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0716 - accuracy: 0.9678 - val_loss: 0.9801 - val_accuracy: 0.8210\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0695 - accuracy: 0.9698 - val_loss: 0.9660 - val_accuracy: 0.8268\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0663 - accuracy: 0.9708 - val_loss: 0.9634 - val_accuracy: 0.8249\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0705 - accuracy: 0.9688 - val_loss: 0.9636 - val_accuracy: 0.8230\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0728 - accuracy: 0.9673 - val_loss: 0.9585 - val_accuracy: 0.8210\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 0.0674 - accuracy: 0.9669 - val_loss: 0.9586 - val_accuracy: 0.8191\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0631 - accuracy: 0.9708 - val_loss: 0.9617 - val_accuracy: 0.8210\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.0633 - accuracy: 0.9727 - val_loss: 0.9636 - val_accuracy: 0.8210\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0674 - accuracy: 0.9688 - val_loss: 0.9703 - val_accuracy: 0.8191\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0695 - accuracy: 0.9673 - val_loss: 0.9787 - val_accuracy: 0.8132\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0691 - accuracy: 0.9722 - val_loss: 0.9815 - val_accuracy: 0.8113\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0689 - accuracy: 0.9722 - val_loss: 0.9798 - val_accuracy: 0.8132\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0689 - accuracy: 0.9722 - val_loss: 0.9790 - val_accuracy: 0.8132\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0686 - accuracy: 0.9717 - val_loss: 0.9770 - val_accuracy: 0.8152\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.0682 - accuracy: 0.9722 - val_loss: 0.9768 - val_accuracy: 0.8152\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.0703 - accuracy: 0.9698 - val_loss: 0.9753 - val_accuracy: 0.8093\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0711 - accuracy: 0.9693 - val_loss: 0.9729 - val_accuracy: 0.8113\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0707 - accuracy: 0.9703 - val_loss: 0.9717 - val_accuracy: 0.8152\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 0.0696 - accuracy: 0.9698 - val_loss: 0.9747 - val_accuracy: 0.8132\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0694 - accuracy: 0.9698 - val_loss: 0.9765 - val_accuracy: 0.8152\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0686 - accuracy: 0.9712 - val_loss: 0.9735 - val_accuracy: 0.8210\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.0677 - accuracy: 0.9712 - val_loss: 0.9711 - val_accuracy: 0.8230\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0686 - accuracy: 0.9717 - val_loss: 0.9719 - val_accuracy: 0.8191\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0727 - accuracy: 0.9673 - val_loss: 0.9740 - val_accuracy: 0.8191\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0752 - accuracy: 0.9659 - val_loss: 0.9761 - val_accuracy: 0.8191\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0746 - accuracy: 0.9664 - val_loss: 0.9780 - val_accuracy: 0.8249\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0736 - accuracy: 0.9664 - val_loss: 0.9789 - val_accuracy: 0.8249\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.0713 - accuracy: 0.9659 - val_loss: 0.9811 - val_accuracy: 0.8230\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0675 - accuracy: 0.9717 - val_loss: 0.9888 - val_accuracy: 0.8171\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0643 - accuracy: 0.9717 - val_loss: 1.0002 - val_accuracy: 0.8171\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0642 - accuracy: 0.9727 - val_loss: 1.0195 - val_accuracy: 0.8093\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0692 - accuracy: 0.9683 - val_loss: 1.0371 - val_accuracy: 0.7977\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0751 - accuracy: 0.9664 - val_loss: 1.0311 - val_accuracy: 0.8035\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0728 - accuracy: 0.9664 - val_loss: 1.0168 - val_accuracy: 0.8132\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0682 - accuracy: 0.9703 - val_loss: 1.0013 - val_accuracy: 0.8249\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0656 - accuracy: 0.9732 - val_loss: 0.9953 - val_accuracy: 0.8230\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0668 - accuracy: 0.9693 - val_loss: 0.9977 - val_accuracy: 0.8249\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.0689 - accuracy: 0.9688 - val_loss: 1.0006 - val_accuracy: 0.8210\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.0698 - accuracy: 0.9698 - val_loss: 1.0066 - val_accuracy: 0.8132\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0704 - accuracy: 0.9669 - val_loss: 1.0191 - val_accuracy: 0.8132\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.0740 - accuracy: 0.9669 - val_loss: 1.0267 - val_accuracy: 0.8035\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0772 - accuracy: 0.9635 - val_loss: 1.0289 - val_accuracy: 0.8054\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.0802 - accuracy: 0.9600 - val_loss: 1.0235 - val_accuracy: 0.8152\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.0770 - accuracy: 0.9644 - val_loss: 1.0131 - val_accuracy: 0.8210\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0740 - accuracy: 0.9664 - val_loss: 1.0075 - val_accuracy: 0.8249\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 2 (One-hot Encoding):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.824903\n",
      "1  Precision  0.733333\n",
      "2     Recall  0.758621\n",
      "3   F1 Score  0.745763\n",
      "4    ROC AUC  0.849434\n",
      "\n",
      "Confusion Matrix for Method 2 (One-hot Encoding):\n",
      " [[292  48]\n",
      " [ 42 132]]\n",
      "MFTLKKSMLLLFFLGTISLSLC\n",
      "neither\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Prediction for Method 2 (Deep Learning): Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assuming you have already defined posDF, negDF, char_to_int, and alphabet as in your original code\n",
    "\n",
    "# Process data for Method 2 (One-hot Encoding)\n",
    "allDataDF = allDataDF.reset_index(drop=True)\n",
    "posDF = posDF.reset_index(drop=True)\n",
    "negDF = negDF.reset_index(drop=True)\n",
    "\n",
    "posDF['Bioactive sequence'] = (posDF['Bioactive sequence'].str.upper())\n",
    "negDF['Bioactive sequence'] = (negDF['Bioactive sequence'].str.upper())\n",
    "\n",
    "posDF['TruncSequence'] = (posDF['Bioactive sequence'].str.slice(0,22))\n",
    "negDF['TruncSequence'] = (negDF['Bioactive sequence'].str.slice(0,22))\n",
    "\n",
    "posDF['ModSequence'] = (posDF['TruncSequence'].str.pad(22,\"right\",\"#\"))\n",
    "negDF['ModSequence'] = (negDF['TruncSequence'].str.pad(22,\"right\",\"#\"))\n",
    "\n",
    "posDF['Value'] = 1\n",
    "negDF['Value'] = 0\n",
    "\n",
    "\n",
    "a=pd.concat([posDF,negDF])\n",
    "a=a.set_index(a['DADP ID'])\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWY#/ '\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "# Encode sequences using one-hot encoding\n",
    "def one_hot_encode_sequence(sequence):\n",
    "    integer_encoded = [char_to_int[char] for char in sequence]\n",
    "    onehot_encoded = np.zeros((len(sequence), len(alphabet)))\n",
    "    for i, value in enumerate(integer_encoded):\n",
    "        onehot_encoded[i, value] = 1\n",
    "    return onehot_encoded\n",
    "display(a)\n",
    "allDataDF2['EncodedX'] = a['ModSequence'].apply(one_hot_encode_sequence)\n",
    "display(allDataDF2)\n",
    "# Prepare data for training\n",
    "X = np.array(list(allDataDF2['EncodedX']))\n",
    "y = allDataDF2['Value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define a simple feedforward neural network model\n",
    "def create_nn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the neural network model\n",
    "model = create_nn_model(input_shape=X.shape[1:])\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=2000, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method2 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 2 (One-hot Encoding):\\n\", performance_metrics_method2)\n",
    "print(\"\\nConfusion Matrix for Method 2 (One-hot Encoding):\\n\", conf_matrix)\n",
    "\n",
    "# Function to predict using the trained neural network model\n",
    "def predict_method2(sequence, model):\n",
    "    sequence = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence]\n",
    "    onehot_encoded = np.zeros((len(sequence), len(alphabet)))\n",
    "    for i, value in enumerate(integer_encoded):\n",
    "        onehot_encoded[i, value] = 1\n",
    "    onehot_encoded = np.array(onehot_encoded).reshape(1, 22, len(alphabet))  # Reshape for the model\n",
    "    prediction = model.predict(onehot_encoded)\n",
    "    if prediction[0] >= 0.5:\n",
    "        return 'Antimicrobial'\n",
    "    else:\n",
    "        return 'Non-antimicrobial'\n",
    "\n",
    "# Example of making predictions\n",
    "sequence='MFTLKKSMLLLFFLGTISLSLC'\n",
    "print(sequence)\n",
    "if sequence in posDF[\"Bioactive sequence\"]:\n",
    "    print(\"positive\")\n",
    "elif sequence in negDF:\n",
    "    print(\"negative\")\n",
    "else:\n",
    "    print(\"neither\")\n",
    "    \n",
    "\n",
    "prediction = predict_method2(sequence, model)\n",
    "print(f\"Prediction for Method 2 (Deep Learning): {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f1abc-64f6-4f18-9a8e-cf9ed08a12ad",
   "metadata": {},
   "source": [
    "**Bert Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5db40e-894e-4cbf-85b8-d79fb5aec9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DADP ID     Entry Name Uniprot code                Species  \\\n",
       "0     SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "1       SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "2       SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "3       SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "4       SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...         ...            ...          ...                    ...   \n",
       "1640    SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "1641    SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "1642    SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "1643    SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "1644  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "      Sequence length         signal sequence  \\\n",
       "0                  81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "1                  23                       /   \n",
       "2                  23                       /   \n",
       "3                  23                       /   \n",
       "4                  16                       /   \n",
       "...               ...                     ...   \n",
       "1640               59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "1641               55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "1642               67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "1643               64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "1644               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                     Bioactive sequence  \\\n",
       "0                     GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "1                               GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "2                               GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "3                               GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "4                                      ILGKLLSTAAGLLSNL   \n",
       "...                                                 ...   \n",
       "1640                                      VFLGNIVSMGKKI   \n",
       "1641                                 DAAVEPELYHWGKVWLPN   \n",
       "1642                               CVDIGFSPTGKRPPFCPYPG   \n",
       "1643                                  VADKRPYILREKKSIPY   \n",
       "1644  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                             Properties  \\\n",
       "0     {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "1     {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "2     {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "3     {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "4     {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                 ...   \n",
       "1640  {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "1641  {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "1642  {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "1643  {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "1644  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "               TruncSequence             ModSequence  Value  \n",
       "0     GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "1     GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "2     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "3     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "4           ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                      ...                     ...    ...  \n",
       "1640           VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "1641      DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "1642    CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "1643       VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "1644  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BioBERT model and tokenizer...\n",
      "Generating embeddings for sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding sequences:   0%|                                                            | 2/2566 [00:02<37:49,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV\n",
      "Embedding shape: (1, 768)\n",
      "Embedding: [[-1.64003409e-02 -4.82710510e-01  2.49254555e-01 -1.09041380e-02\n",
      "   7.28634894e-01 -6.47612751e-01  5.33651933e-02  6.01541884e-02\n",
      "   1.70116290e-01  1.52809873e-01  6.76362962e-02 -3.91306877e-01\n",
      "   2.00200170e-01 -1.46960109e-01 -2.70447642e-01  1.64351575e-02\n",
      "  -3.26772660e-01 -5.08496761e-01 -6.19464889e-02 -2.41161324e-02\n",
      "  -2.76165307e-01 -1.97354943e-01 -1.28901646e-01  6.33836761e-02\n",
      "  -7.28787035e-02  9.17148683e-03 -1.29213050e-01  2.32727125e-01\n",
      "  -3.41059491e-02  5.04042029e-01 -1.14261426e-01 -3.74896735e-01\n",
      "  -3.62805247e-01 -8.44919980e-01  1.53892905e-01  1.49310201e-01\n",
      "  -3.13910916e-02 -1.67167813e-01  2.06563830e-01 -7.79478401e-02\n",
      "   2.59182304e-01  2.83368021e-01  9.26770031e-01  9.09246728e-02\n",
      "   2.39800140e-01 -2.97584422e-02 -3.81289199e-02 -4.01842356e-01\n",
      "   6.95645362e-02  4.24234152e-01 -2.79195487e-01  3.44893396e-01\n",
      "   4.80712652e-02  2.03084499e-01  3.26647237e-02 -7.62905851e-02\n",
      "  -4.92505938e-01 -6.87148347e-02  9.33533758e-02  2.80629188e-01\n",
      "   4.63714421e-01  8.73696685e-01  1.02981257e+00 -5.69522381e-01\n",
      "  -3.87014151e-01  4.33567584e-01  2.59703398e-02  3.32449228e-01\n",
      "  -2.81213373e-02 -5.18061034e-02 -5.19406438e-01 -3.57699215e-01\n",
      "  -2.03453451e-01 -2.34661371e-01 -3.22974026e-01  4.39279646e-01\n",
      "   8.12059045e-01 -2.29620606e-01 -3.03594284e-02 -4.21073973e-01\n",
      "   2.30230376e-01 -6.78885952e-02  5.24203300e-01  1.45139962e-01\n",
      "  -2.00255699e-02  8.59785229e-02  3.48286748e-01  2.23969296e-01\n",
      "  -1.21705174e-01  1.53178915e-01  2.31319994e-01  1.67289883e-01\n",
      "  -4.23715591e-01  2.60757711e-02  2.26204153e-02  2.78430760e-01\n",
      "  -2.30804868e-02 -2.27607433e-02  4.78556544e-01  1.02654457e-01\n",
      "   2.71262228e-01  3.51161718e-01  1.34500906e-01  4.18255419e-01\n",
      "   1.30037695e-01 -1.21921621e-01  3.21175396e-01 -2.23258898e-01\n",
      "  -9.98982191e-02  5.41192889e-02  4.95859802e-01 -1.45267636e-01\n",
      "   1.66579157e-01 -2.85401195e-01 -1.72845721e-01  2.15685457e-01\n",
      "  -2.97062516e-01 -5.28271571e-02  2.73609549e-01 -1.79285914e-01\n",
      "   4.15746272e-01 -5.27933165e-02  9.44848824e-03  2.77196169e-02\n",
      "   2.17274427e-01 -2.05512762e-01  1.36599347e-01  5.96080534e-02\n",
      "  -1.31294653e-01  1.51329175e-01 -1.59273148e-01  1.17665902e-01\n",
      "  -1.81695983e-01 -4.99983728e-01  3.25479925e-01 -1.90261342e-02\n",
      "  -7.54253745e-01  2.19560891e-01 -5.06306469e-01 -2.25222200e-01\n",
      "   2.23297626e-01 -5.72084114e-02  1.67503908e-01 -2.67478019e-01\n",
      "  -2.87734836e-01 -3.43581259e-01 -2.39790767e-01 -3.91018242e-01\n",
      "   6.54581726e-01 -4.03676510e-01 -3.89921010e-01 -8.68996605e-02\n",
      "  -2.36088231e-01 -2.46641323e-01  4.60999086e-02 -2.06793144e-01\n",
      "   2.50632823e-01  3.30883414e-01  2.59830058e-03  6.14218652e-01\n",
      "   3.36680114e-01  1.98538378e-01 -4.50240284e-01  3.77394587e-01\n",
      "  -1.09299123e-01  3.91387343e-01  2.83318430e-01  1.65441051e-01\n",
      "  -3.18492383e-01  3.36919963e-01  2.29316518e-01 -3.58706623e-01\n",
      "   1.10764980e-01  8.70121002e-01  5.66058457e-01  2.63886154e-01\n",
      "  -5.37948590e-03 -4.72802073e-01 -4.48753178e-01  1.19649008e-01\n",
      "  -1.17332954e-02 -1.74811766e-01 -1.70688957e-01 -1.02806211e-01\n",
      "   1.02928616e-01  8.03418905e-02  2.93123186e-01 -4.35834736e-01\n",
      "   8.07677135e-02  4.81469631e-01 -6.70536980e-02 -6.44497499e-02\n",
      "   7.04802126e-02 -2.27294832e-01 -1.93789646e-01 -3.53011429e-01\n",
      "  -5.27552009e-01  2.30557829e-01  6.43702149e-02  1.18900038e-01\n",
      "  -4.81676459e-02  2.72338092e-01 -4.48655307e-01  3.53000909e-01\n",
      "   5.64118505e-01  1.41916290e-01  2.51963675e-01  6.94609061e-02\n",
      "  -8.71846825e-02  3.24353069e-01 -1.00836203e-01  2.72442430e-01\n",
      "   3.56450737e-01  2.03076005e-01 -4.95974630e-01 -2.78541923e-01\n",
      "  -5.33652306e-02 -2.65108168e-01 -1.89601809e-01 -8.94655734e-02\n",
      "  -3.80511433e-01  1.21126339e-01 -9.39186439e-02  1.47225887e-01\n",
      "  -1.88323408e-01  2.40825325e-01  1.68419331e-01 -1.12176225e-01\n",
      "   7.82238007e-01 -1.05786487e-01  2.13067025e-01 -9.49328095e-02\n",
      "  -4.78597075e-01 -2.07954019e-01 -2.36499190e-01 -6.08335733e-01\n",
      "   2.03321770e-01 -3.26503694e-01 -3.38162512e-01 -1.43479168e-01\n",
      "   1.57580391e-01 -1.93155721e-01 -6.36705756e-02  1.44481589e-03\n",
      "  -1.33347228e-01  2.68800944e-01 -3.17861885e-01  3.18718910e-01\n",
      "   1.25608802e-01  1.93304233e-02  7.65366912e-01 -1.46732047e-01\n",
      "   2.15564460e-01  3.79094362e-01 -5.25868610e-02  2.29893595e-01\n",
      "  -7.92285427e-02  1.77079886e-01 -1.83717519e-01 -2.05646425e-01\n",
      "  -2.22441241e-01 -3.66842449e-01  3.69438469e-01  1.00258484e-01\n",
      "   3.05972070e-01  3.45058858e-01  1.43970773e-01 -4.20839004e-02\n",
      "  -4.73827660e-01 -1.16497472e-01 -3.23148727e-01  1.72412157e-01\n",
      "  -2.08704859e-01  2.38257796e-02  5.60933113e-01 -3.49931121e-01\n",
      "  -4.34415251e-01  1.10159613e-01  5.57218730e-01 -3.79327625e-01\n",
      "  -1.45372134e-02 -4.20112044e-01  9.88156721e-02  2.63102353e-01\n",
      "  -3.41233343e-01 -1.72946781e-01  2.17428118e-01  4.85941231e-01\n",
      "   1.17742859e-01 -1.63134485e-01  1.58303604e-01  1.63433939e-01\n",
      "  -9.93495621e-03  1.32355422e-01  2.33467907e-01  1.90930679e-01\n",
      "  -3.03264558e-02  3.41879278e-01  2.26501554e-01 -2.85040915e-01\n",
      "  -9.46656838e-02 -1.87640071e-01  1.20637253e-01  1.64712518e-01\n",
      "  -1.31472200e-01  2.14573339e-01 -6.09389059e-02 -1.01145655e-02\n",
      "   3.91925871e-01  2.98554957e-01 -1.11580804e-01  3.31197828e-01\n",
      "   1.76808432e-01 -2.55336463e-01  1.09789208e-01  6.78438306e-01\n",
      "   6.27605319e-02 -6.16012104e-02 -1.91079095e-01 -1.86340794e-01\n",
      "   5.02566211e-02  5.73267460e-01 -3.93331021e-01 -1.36839464e-01\n",
      "  -3.34180593e-01 -6.91046491e-02  1.66049451e-01 -3.67128760e-01\n",
      "  -5.45554638e-01 -2.83265412e-01 -8.87355059e-02 -1.53464288e-01\n",
      "   3.21482494e-02 -4.49454606e-01  3.23594928e-01 -3.04524422e-01\n",
      "  -6.24529235e-02 -1.48338705e-01 -2.94645965e-01  1.02758862e-01\n",
      "  -2.77663972e-02 -1.59768417e-01 -1.91362917e-01  2.62224734e-01\n",
      "  -2.55818516e-01 -3.11551511e-01 -1.86605453e-01  7.09620565e-02\n",
      "  -1.65994659e-01  2.49349266e-01 -1.31043300e-01  4.86724749e-02\n",
      "  -2.84670353e-01  4.55094039e-01  7.26821542e-01 -2.68644184e-01\n",
      "  -3.54816735e-01  4.85965669e-01  2.42982835e-01 -1.51281923e-01\n",
      "  -1.33383095e-01  1.81295767e-01  3.15533370e-01  3.76008153e-02\n",
      "   1.25981405e-01 -4.69311059e-01  3.29818636e-01  1.54262081e-01\n",
      "  -4.42143381e-01  1.85550258e-01  4.60503668e-01 -2.39960119e-01\n",
      "   2.40492553e-01  2.10199147e-01 -2.83194005e-01  6.61097467e-04\n",
      "   1.23950340e-01 -4.16454613e-01 -1.30853221e-01 -6.49896979e-01\n",
      "  -5.64791918e-01  3.32899868e-01  2.03123361e-01 -3.88354063e-04\n",
      "  -1.38105586e-01 -2.05863267e-03  2.34906763e-01  5.01994729e-01\n",
      "   3.64093393e-01  1.69165224e-01 -5.93554862e-02 -1.09114148e-01\n",
      "   3.82529534e-02 -8.29861462e-01  2.72085845e-01 -2.10565567e-01\n",
      "  -1.02890909e-01  1.43581361e-01  2.79628336e-01 -3.24829400e-01\n",
      "  -7.65102580e-02  5.35817862e-01 -1.62376195e-01 -3.88465934e-02\n",
      "  -1.59207910e-01 -1.46823050e-02  8.51192102e-02 -3.60138893e-01\n",
      "  -4.04286161e-02 -1.26875013e-01 -3.71843159e-01 -6.92612529e-01\n",
      "   3.77769917e-01  3.68672580e-01  5.66107988e-01 -3.40746492e-02\n",
      "   1.92986771e-01  5.99353790e-01 -5.11055887e-01  1.07030906e-01\n",
      "   2.74289846e-01  8.39815140e-02 -2.12958410e-01  4.22647968e-02\n",
      "   3.30490798e-01 -1.45351261e-01 -5.54383755e-01  7.27400035e-02\n",
      "  -3.00445825e-01  3.77484232e-01 -2.58035958e-02  1.96821094e-01\n",
      "   5.52675016e-02 -1.25040859e-01  5.05269878e-02 -8.35235417e-02\n",
      "   3.39868724e-01 -4.17065382e-01  4.03910279e-02 -2.44317561e-01\n",
      "   1.07356742e-01 -3.95538092e-01  3.16612661e-01 -4.80681658e-01\n",
      "   1.37501925e-01 -2.95745552e-01 -1.32596761e-01 -3.27217567e-04\n",
      "   3.14221442e-01  3.17523450e-01  3.87254238e-01  5.57496369e-01\n",
      "  -2.33228710e-02 -2.89946850e-02 -1.38014611e-02 -6.53989494e-01\n",
      "   9.42042023e-02 -3.78498197e-01 -3.52117389e-01 -3.10935795e-01\n",
      "  -1.22845054e-01 -5.16671062e-01 -2.00064704e-01  2.60457456e-01\n",
      "   4.41903114e-01  8.54933858e-02 -9.03007388e-02  5.02986670e-01\n",
      "   2.11076136e-03 -5.10823019e-02  3.10165614e-01  5.82528234e-01\n",
      "   1.86313912e-01  6.59858704e-01 -2.80341387e-01 -6.99331751e-03\n",
      "  -1.92157343e-01  3.61472890e-02 -1.76903576e-01  6.95914850e-02\n",
      "   1.40665531e-01  3.60881180e-01 -1.66920736e-01  3.55992854e-01\n",
      "  -4.19237375e-01  4.75794792e-01  4.69290130e-02 -1.37300596e-01\n",
      "   1.11848488e-01 -2.51817200e-02 -1.40645459e-01 -1.47531480e-02\n",
      "  -9.48466659e-02  2.73072720e-01  2.44653560e-02  7.33996916e-04\n",
      "  -2.78816130e-02 -2.12786153e-01 -2.35722214e-01 -1.14705071e-01\n",
      "  -2.47785956e-01  5.51830838e-03 -3.92451793e-01  4.14590351e-02\n",
      "  -2.52511084e-01  1.61845163e-01  2.88358152e-01 -3.10438816e-02\n",
      "   4.64073479e-01 -3.63943189e-01 -1.85232505e-01  3.37846220e-01\n",
      "   3.21000516e-01 -7.20554614e-04 -2.81546405e-03 -3.88722956e-01\n",
      "  -3.62940133e-01  9.64944996e-03  4.13027942e-01 -2.42494941e-01\n",
      "  -2.76208788e-01 -4.04574573e-01 -4.28979009e-01 -5.91866910e-01\n",
      "  -3.76135290e-01 -2.47574657e-01 -2.47372359e-01 -5.82553744e-01\n",
      "   1.54925570e-01 -5.12567401e-01 -4.27423239e-01  2.96844065e-01\n",
      "  -5.12302756e-01 -1.80125445e-01 -4.23329473e-02  7.96196833e-02\n",
      "  -2.76697010e-01 -1.37815803e-01 -3.40058953e-01  8.06401849e-01\n",
      "  -1.25470668e-01  3.59844238e-01 -7.24588484e-02  1.02231815e-01\n",
      "   2.14679435e-01  5.54944992e-01 -3.83526415e-01  2.40727946e-01\n",
      "   6.20963797e-02  2.89308876e-01  7.52661675e-02  2.16998070e-01\n",
      "   1.13193527e-01  2.03761190e-01  1.96365029e-01 -4.42474991e-01\n",
      "  -5.33894658e-01 -8.58926201e+00 -2.92451885e-02 -1.58291817e-01\n",
      "  -3.48334640e-01  6.66174144e-02 -2.74790943e-01  2.12331057e-01\n",
      "  -3.25475663e-01 -2.10324556e-01  7.96175078e-02 -6.68469012e-01\n",
      "   1.12953521e-01  2.46119112e-01 -4.08804357e-01  2.02097774e-01\n",
      "  -1.51490318e-02  3.20530027e-01 -2.04893157e-01  4.80153143e-01\n",
      "  -3.22802126e-01 -2.24535272e-01  3.27471010e-02  2.63910115e-01\n",
      "  -3.32639702e-02  4.02758747e-01  2.92285860e-01 -8.05698782e-02\n",
      "   7.57329047e-01  1.21529952e-01 -2.73500048e-02 -7.79089332e-01\n",
      "  -1.62480310e-01  3.10304135e-01  2.00903058e-01 -3.71160328e-01\n",
      "   2.81329937e-02  3.72705609e-01 -3.64656955e-01  1.09729931e-01\n",
      "  -2.03583866e-01  1.58804864e-01 -2.97934711e-01  6.71295077e-02\n",
      "   1.70671493e-01  2.64702559e-01  2.11844206e-01  2.25563675e-01\n",
      "   3.86612684e-01 -2.69972682e-01  2.83501416e-01  2.62707710e-01\n",
      "   4.97025073e-01 -1.02322683e-01 -5.32655954e-01 -1.62199393e-01\n",
      "   2.47249693e-01 -5.26982665e-01  5.24339378e-01 -9.16092023e-02\n",
      "  -3.24983299e-01 -1.81029290e-02  1.66945383e-01 -1.68297574e-01\n",
      "  -1.58478349e-01 -1.13397874e-01  3.15888613e-01 -3.95438612e-01\n",
      "  -3.03851157e-01 -3.77259031e-02  9.96890664e-02 -4.03561652e-01\n",
      "   2.35031128e-01  3.63694757e-01  4.88871574e-01  1.42041013e-01\n",
      "  -4.67888355e-01 -3.25319350e-01  2.36944959e-01  8.86314213e-01\n",
      "   5.04088163e-01 -7.89883792e-01 -1.72655016e-01 -2.54145209e-02\n",
      "  -3.25324237e-01 -9.64408088e-03 -2.55575001e-01 -1.83248371e-01\n",
      "   3.51095587e-01  2.31729537e-01 -1.20018221e-01  2.18523428e-01\n",
      "  -3.36666912e-01 -1.70587786e-02  2.39727825e-01 -5.07556140e-01\n",
      "  -1.13659620e-01 -2.93289572e-01 -5.70242517e-02 -3.97179902e-01\n",
      "  -2.49347299e-01 -9.01850462e-02  5.02324164e-01  1.40607372e-01\n",
      "  -2.36753225e-02  1.17642954e-02 -2.54781187e-01 -7.96463192e-02\n",
      "  -2.42033213e-01  5.78561723e-02  8.64051431e-02  1.83538586e-01\n",
      "  -5.34611456e-02  1.32681783e-02 -1.74065217e-01  2.94749588e-01\n",
      "   3.33616197e-01  4.05905634e-01  1.93764135e-01  3.22997957e-01\n",
      "  -3.73488967e-03 -1.17702581e-01 -3.95532325e-02 -1.22108258e-01\n",
      "  -6.23045921e-01  1.46333143e-01  1.16850272e-01 -1.34916618e-01\n",
      "  -5.80624938e-02  1.37720361e-01 -2.76611805e-01  4.56170559e-01\n",
      "  -1.68050498e-01 -3.45749050e-01 -4.51550782e-02 -1.52641416e-01\n",
      "   5.17635755e-02  1.62691310e-01  3.19137096e-01 -4.13682640e-01\n",
      "  -3.62632096e-01 -1.27217740e-01  1.70231387e-02  3.86574179e-01\n",
      "   3.09289783e-01  1.02399215e-01  4.32819128e-03 -2.46804982e-01\n",
      "   1.21136591e-01 -7.12259188e-02  2.82539129e-01 -2.01454446e-01\n",
      "   6.93570793e-01  1.80779025e-01 -2.27622673e-01  5.48453815e-02\n",
      "   5.70640445e-01  2.03180939e-01 -2.10896447e-01 -1.70503914e-01\n",
      "  -3.22827691e-04  4.98328537e-01  6.67827189e-01  8.46041813e-02\n",
      "   5.31485260e-01  5.02188563e-01 -3.26934755e-01  1.61333472e-01\n",
      "   5.01764119e-01 -2.35691950e-01 -3.07621688e-01 -3.19027573e-01\n",
      "   1.68065995e-01  1.98655620e-01 -7.21843094e-02  2.99447328e-01\n",
      "  -8.50660168e-03 -1.77459195e-01 -6.97741956e-02 -6.71077380e-03\n",
      "   4.05107811e-02  1.17405519e-01 -3.39172259e-02 -6.57981187e-02\n",
      "  -3.62121135e-01  4.44906890e-01  5.23720622e-01 -1.05864853e-02\n",
      "   1.48474630e-02  9.57417190e-02 -1.07464336e-01 -1.46982104e-01\n",
      "  -8.81997943e-02 -2.85884321e-01 -1.53922975e-01  3.30416024e-01\n",
      "   3.81363034e-02 -2.72856653e-01  2.69623250e-01 -2.28136301e-01\n",
      "   4.03578043e-01 -1.03221692e-01 -8.57222453e-02  9.04977918e-02\n",
      "  -4.45217609e-01 -6.99968994e-01 -2.87689686e-01  4.83651340e-01\n",
      "  -1.54991731e-01  1.25976503e-01 -3.59323084e-01 -5.95465064e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding sequences: 100%|█████████████████████████████████████████████████████████| 2566/2566 [06:38<00:00,  6.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DADP ID</th>\n",
       "      <th>Entry Name</th>\n",
       "      <th>Uniprot code</th>\n",
       "      <th>Species</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>signal sequence</th>\n",
       "      <th>Bioactive sequence</th>\n",
       "      <th>Properties</th>\n",
       "      <th>TruncSequence</th>\n",
       "      <th>ModSequence</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SP_P31107</td>\n",
       "      <td>Adenoregulin</td>\n",
       "      <td>P31107</td>\n",
       "      <td>Phyllomedusa bicolor</td>\n",
       "      <td>81</td>\n",
       "      <td>MAFLKKSLFLVLFLGLVSLSIC</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV</td>\n",
       "      <td>{'MolecularWeight': 3181.683499999999, 'Isoele...</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>GLWSKIKEVGKEAAKAAAKAAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SP_2643</td>\n",
       "      <td>Alyteserin-1a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2279.679399999999, 'Isoele...</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKDIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SP_2644</td>\n",
       "      <td>Alyteserin-1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAN</td>\n",
       "      <td>{'MolecularWeight': 2293.705999999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SP_2645</td>\n",
       "      <td>Alyteserin-1c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>23</td>\n",
       "      <td>/</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVAS</td>\n",
       "      <td>{'MolecularWeight': 2266.680699999999, 'Isoele...</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>GLKEIFKAGLGSLVKGIAAHVA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SP_2646</td>\n",
       "      <td>Alyteserin-2a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alytes obstetricans</td>\n",
       "      <td>16</td>\n",
       "      <td>/</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>{'MolecularWeight': 1583.9103000000002, 'Isoel...</td>\n",
       "      <td>ILGKLLSTAAGLLSNL</td>\n",
       "      <td>ILGKLLSTAAGLLSNL######</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>SP_2852</td>\n",
       "      <td>Wuchuanin-C1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>59</td>\n",
       "      <td>MFTLKKSMLLLFFLGTISLSLC</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>{'MolecularWeight': 1405.7469999999996, 'Isoel...</td>\n",
       "      <td>VFLGNIVSMGKKI</td>\n",
       "      <td>VFLGNIVSMGKKI#########</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>SP_2853</td>\n",
       "      <td>Wuchuanin-D1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>55</td>\n",
       "      <td>MFTLKKSLLLLFFLGTINLSLC</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>{'MolecularWeight': 2124.3526000000006, 'Isoel...</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN</td>\n",
       "      <td>DAAVEPELYHWGKVWLPN####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>SP_2854</td>\n",
       "      <td>Wuchuanin-E1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>67</td>\n",
       "      <td>MFTLKKSLLLIVLLGIISLSLC</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>{'MolecularWeight': 2138.467000000001, 'Isoele...</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG</td>\n",
       "      <td>CVDIGFSPTGKRPPFCPYPG##</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>SP_2855</td>\n",
       "      <td>Wuchuanin-F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Odorrana wuchuanensis</td>\n",
       "      <td>64</td>\n",
       "      <td>MFTLKKSLLLLFLLGTISLSLC</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>{'MolecularWeight': 2076.4408000000003, 'Isoel...</td>\n",
       "      <td>VADKRPYILREKKSIPY</td>\n",
       "      <td>VADKRPYILREKKSIPY#####</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>SP_Q09022</td>\n",
       "      <td>Xenoxin-1</td>\n",
       "      <td>Q09022</td>\n",
       "      <td>Xenopus laevis</td>\n",
       "      <td>84</td>\n",
       "      <td>MRYAIVFFLVCVITLGEA</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...</td>\n",
       "      <td>{'MolecularWeight': 7235.609599999999, 'Isoele...</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>LKCVNLQANGIKMTQECAKEDT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DADP ID     Entry Name Uniprot code                Species  \\\n",
       "0     SP_P31107   Adenoregulin       P31107   Phyllomedusa bicolor   \n",
       "1       SP_2643  Alyteserin-1a          NaN    Alytes obstetricans   \n",
       "2       SP_2644  Alyteserin-1b          NaN    Alytes obstetricans   \n",
       "3       SP_2645  Alyteserin-1c          NaN    Alytes obstetricans   \n",
       "4       SP_2646  Alyteserin-2a          NaN    Alytes obstetricans   \n",
       "...         ...            ...          ...                    ...   \n",
       "1640    SP_2852   Wuchuanin-C1          NaN  Odorrana wuchuanensis   \n",
       "1641    SP_2853   Wuchuanin-D1          NaN  Odorrana wuchuanensis   \n",
       "1642    SP_2854   Wuchuanin-E1          NaN  Odorrana wuchuanensis   \n",
       "1643    SP_2855   Wuchuanin-F1          NaN  Odorrana wuchuanensis   \n",
       "1644  SP_Q09022      Xenoxin-1       Q09022         Xenopus laevis   \n",
       "\n",
       "      Sequence length         signal sequence  \\\n",
       "0                  81  MAFLKKSLFLVLFLGLVSLSIC   \n",
       "1                  23                       /   \n",
       "2                  23                       /   \n",
       "3                  23                       /   \n",
       "4                  16                       /   \n",
       "...               ...                     ...   \n",
       "1640               59  MFTLKKSMLLLFFLGTISLSLC   \n",
       "1641               55  MFTLKKSLLLLFFLGTINLSLC   \n",
       "1642               67  MFTLKKSLLLIVLLGIISLSLC   \n",
       "1643               64  MFTLKKSLLLLFLLGTISLSLC   \n",
       "1644               84      MRYAIVFFLVCVITLGEA   \n",
       "\n",
       "                                     Bioactive sequence  \\\n",
       "0                     GLWSKIKEVGKEAAKAAAKAAGKAALGAVSEAV   \n",
       "1                               GLKDIFKAGLGSLVKGIAAHVAN   \n",
       "2                               GLKEIFKAGLGSLVKGIAAHVAN   \n",
       "3                               GLKEIFKAGLGSLVKGIAAHVAS   \n",
       "4                                      ILGKLLSTAAGLLSNL   \n",
       "...                                                 ...   \n",
       "1640                                      VFLGNIVSMGKKI   \n",
       "1641                                 DAAVEPELYHWGKVWLPN   \n",
       "1642                               CVDIGFSPTGKRPPFCPYPG   \n",
       "1643                                  VADKRPYILREKKSIPY   \n",
       "1644  LKCVNLQANGIKMTQECAKEDTKCLTLRSLKKTLKFCASGRTCTTM...   \n",
       "\n",
       "                                             Properties  \\\n",
       "0     {'MolecularWeight': 3181.683499999999, 'Isoele...   \n",
       "1     {'MolecularWeight': 2279.679399999999, 'Isoele...   \n",
       "2     {'MolecularWeight': 2293.705999999999, 'Isoele...   \n",
       "3     {'MolecularWeight': 2266.680699999999, 'Isoele...   \n",
       "4     {'MolecularWeight': 1583.9103000000002, 'Isoel...   \n",
       "...                                                 ...   \n",
       "1640  {'MolecularWeight': 1405.7469999999996, 'Isoel...   \n",
       "1641  {'MolecularWeight': 2124.3526000000006, 'Isoel...   \n",
       "1642  {'MolecularWeight': 2138.467000000001, 'Isoele...   \n",
       "1643  {'MolecularWeight': 2076.4408000000003, 'Isoel...   \n",
       "1644  {'MolecularWeight': 7235.609599999999, 'Isoele...   \n",
       "\n",
       "               TruncSequence             ModSequence  Value  \n",
       "0     GLWSKIKEVGKEAAKAAAKAAG  GLWSKIKEVGKEAAKAAAKAAG      1  \n",
       "1     GLKDIFKAGLGSLVKGIAAHVA  GLKDIFKAGLGSLVKGIAAHVA      1  \n",
       "2     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "3     GLKEIFKAGLGSLVKGIAAHVA  GLKEIFKAGLGSLVKGIAAHVA      1  \n",
       "4           ILGKLLSTAAGLLSNL  ILGKLLSTAAGLLSNL######      1  \n",
       "...                      ...                     ...    ...  \n",
       "1640           VFLGNIVSMGKKI  VFLGNIVSMGKKI#########      0  \n",
       "1641      DAAVEPELYHWGKVWLPN  DAAVEPELYHWGKVWLPN####      0  \n",
       "1642    CVDIGFSPTGKRPPFCPYPG  CVDIGFSPTGKRPPFCPYPG##      0  \n",
       "1643       VADKRPYILREKKSIPY  VADKRPYILREKKSIPY#####      0  \n",
       "1644  LKCVNLQANGIKMTQECAKEDT  LKCVNLQANGIKMTQECAKEDT      0  \n",
       "\n",
       "[2566 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into training and testing sets...\n",
      "Training the neural network...\n",
      "Epoch 1/500, Loss: 0.6949748396873474\n",
      "Epoch 2/500, Loss: 0.6648867726325989\n",
      "Epoch 3/500, Loss: 0.6532174944877625\n",
      "Epoch 4/500, Loss: 0.65052330493927\n",
      "Epoch 5/500, Loss: 0.6497530937194824\n",
      "Epoch 6/500, Loss: 0.6482367515563965\n",
      "Epoch 7/500, Loss: 0.6453698873519897\n",
      "Epoch 8/500, Loss: 0.6415627002716064\n",
      "Epoch 9/500, Loss: 0.6374520659446716\n",
      "Epoch 10/500, Loss: 0.6337497234344482\n",
      "Epoch 11/500, Loss: 0.6310984492301941\n",
      "Epoch 12/500, Loss: 0.6295077204704285\n",
      "Epoch 13/500, Loss: 0.6286216378211975\n",
      "Epoch 14/500, Loss: 0.6279727816581726\n",
      "Epoch 15/500, Loss: 0.6271641254425049\n",
      "Epoch 16/500, Loss: 0.6259685158729553\n",
      "Epoch 17/500, Loss: 0.6245596408843994\n",
      "Epoch 18/500, Loss: 0.6232953667640686\n",
      "Epoch 19/500, Loss: 0.6224016547203064\n",
      "Epoch 20/500, Loss: 0.6219089031219482\n",
      "Epoch 21/500, Loss: 0.6215516328811646\n",
      "Epoch 22/500, Loss: 0.6210127472877502\n",
      "Epoch 23/500, Loss: 0.6202263832092285\n",
      "Epoch 24/500, Loss: 0.6192882061004639\n",
      "Epoch 25/500, Loss: 0.6183393597602844\n",
      "Epoch 26/500, Loss: 0.6175296902656555\n",
      "Epoch 27/500, Loss: 0.6168627738952637\n",
      "Epoch 28/500, Loss: 0.616223156452179\n",
      "Epoch 29/500, Loss: 0.6154732704162598\n",
      "Epoch 30/500, Loss: 0.6145845055580139\n",
      "Epoch 31/500, Loss: 0.6136144995689392\n",
      "Epoch 32/500, Loss: 0.6126465201377869\n",
      "Epoch 33/500, Loss: 0.6117528676986694\n",
      "Epoch 34/500, Loss: 0.6109346151351929\n",
      "Epoch 35/500, Loss: 0.6101015210151672\n",
      "Epoch 36/500, Loss: 0.6092163324356079\n",
      "Epoch 37/500, Loss: 0.6082974672317505\n",
      "Epoch 38/500, Loss: 0.6074134707450867\n",
      "Epoch 39/500, Loss: 0.6065779328346252\n",
      "Epoch 40/500, Loss: 0.6057387590408325\n",
      "Epoch 41/500, Loss: 0.6048383116722107\n",
      "Epoch 42/500, Loss: 0.6039137840270996\n",
      "Epoch 43/500, Loss: 0.6029839515686035\n",
      "Epoch 44/500, Loss: 0.6020001769065857\n",
      "Epoch 45/500, Loss: 0.6009959578514099\n",
      "Epoch 46/500, Loss: 0.5999819040298462\n",
      "Epoch 47/500, Loss: 0.5989397168159485\n",
      "Epoch 48/500, Loss: 0.5979006886482239\n",
      "Epoch 49/500, Loss: 0.5968474745750427\n",
      "Epoch 50/500, Loss: 0.5957739949226379\n",
      "Epoch 51/500, Loss: 0.5946589112281799\n",
      "Epoch 52/500, Loss: 0.5935200452804565\n",
      "Epoch 53/500, Loss: 0.5923678278923035\n",
      "Epoch 54/500, Loss: 0.5912283658981323\n",
      "Epoch 55/500, Loss: 0.5900636315345764\n",
      "Epoch 56/500, Loss: 0.5888779163360596\n",
      "Epoch 57/500, Loss: 0.587705671787262\n",
      "Epoch 58/500, Loss: 0.5865439176559448\n",
      "Epoch 59/500, Loss: 0.5853514075279236\n",
      "Epoch 60/500, Loss: 0.5841485857963562\n",
      "Epoch 61/500, Loss: 0.5829620361328125\n",
      "Epoch 62/500, Loss: 0.5817936062812805\n",
      "Epoch 63/500, Loss: 0.5805984735488892\n",
      "Epoch 64/500, Loss: 0.5793942213058472\n",
      "Epoch 65/500, Loss: 0.5782290101051331\n",
      "Epoch 66/500, Loss: 0.5770715475082397\n",
      "Epoch 67/500, Loss: 0.575883150100708\n",
      "Epoch 68/500, Loss: 0.5746966600418091\n",
      "Epoch 69/500, Loss: 0.5735419988632202\n",
      "Epoch 70/500, Loss: 0.5723698139190674\n",
      "Epoch 71/500, Loss: 0.5711979866027832\n",
      "Epoch 72/500, Loss: 0.5700555443763733\n",
      "Epoch 73/500, Loss: 0.5689136385917664\n",
      "Epoch 74/500, Loss: 0.5677725672721863\n",
      "Epoch 75/500, Loss: 0.5666746497154236\n",
      "Epoch 76/500, Loss: 0.5655622482299805\n",
      "Epoch 77/500, Loss: 0.5644479393959045\n",
      "Epoch 78/500, Loss: 0.5633546710014343\n",
      "Epoch 79/500, Loss: 0.5622585415840149\n",
      "Epoch 80/500, Loss: 0.5611638426780701\n",
      "Epoch 81/500, Loss: 0.5601011514663696\n",
      "Epoch 82/500, Loss: 0.5590259432792664\n",
      "Epoch 83/500, Loss: 0.5579613447189331\n",
      "Epoch 84/500, Loss: 0.5569183230400085\n",
      "Epoch 85/500, Loss: 0.555865466594696\n",
      "Epoch 86/500, Loss: 0.5548216700553894\n",
      "Epoch 87/500, Loss: 0.5537585020065308\n",
      "Epoch 88/500, Loss: 0.5527622103691101\n",
      "Epoch 89/500, Loss: 0.5516942739486694\n",
      "Epoch 90/500, Loss: 0.5506616234779358\n",
      "Epoch 91/500, Loss: 0.5496353507041931\n",
      "Epoch 92/500, Loss: 0.548629105091095\n",
      "Epoch 93/500, Loss: 0.547594428062439\n",
      "Epoch 94/500, Loss: 0.5465927720069885\n",
      "Epoch 95/500, Loss: 0.5455731153488159\n",
      "Epoch 96/500, Loss: 0.5445651412010193\n",
      "Epoch 97/500, Loss: 0.543551504611969\n",
      "Epoch 98/500, Loss: 0.5425354838371277\n",
      "Epoch 99/500, Loss: 0.5415433645248413\n",
      "Epoch 100/500, Loss: 0.5405224561691284\n",
      "Epoch 101/500, Loss: 0.5395213961601257\n",
      "Epoch 102/500, Loss: 0.538528323173523\n",
      "Epoch 103/500, Loss: 0.5375545620918274\n",
      "Epoch 104/500, Loss: 0.53658127784729\n",
      "Epoch 105/500, Loss: 0.5355955958366394\n",
      "Epoch 106/500, Loss: 0.5346466898918152\n",
      "Epoch 107/500, Loss: 0.5337015390396118\n",
      "Epoch 108/500, Loss: 0.5327556133270264\n",
      "Epoch 109/500, Loss: 0.5318218469619751\n",
      "Epoch 110/500, Loss: 0.5308799147605896\n",
      "Epoch 111/500, Loss: 0.5299579501152039\n",
      "Epoch 112/500, Loss: 0.529072105884552\n",
      "Epoch 113/500, Loss: 0.528202474117279\n",
      "Epoch 114/500, Loss: 0.527334988117218\n",
      "Epoch 115/500, Loss: 0.5265080332756042\n",
      "Epoch 116/500, Loss: 0.5256122350692749\n",
      "Epoch 117/500, Loss: 0.5247147679328918\n",
      "Epoch 118/500, Loss: 0.5238423943519592\n",
      "Epoch 119/500, Loss: 0.522975742816925\n",
      "Epoch 120/500, Loss: 0.5221186280250549\n",
      "Epoch 121/500, Loss: 0.5212909579277039\n",
      "Epoch 122/500, Loss: 0.5204672813415527\n",
      "Epoch 123/500, Loss: 0.5196377635002136\n",
      "Epoch 124/500, Loss: 0.518853485584259\n",
      "Epoch 125/500, Loss: 0.5180433392524719\n",
      "Epoch 126/500, Loss: 0.5173100233078003\n",
      "Epoch 127/500, Loss: 0.5166313052177429\n",
      "Epoch 128/500, Loss: 0.5160012245178223\n",
      "Epoch 129/500, Loss: 0.5152776837348938\n",
      "Epoch 130/500, Loss: 0.5144079923629761\n",
      "Epoch 131/500, Loss: 0.5134541988372803\n",
      "Epoch 132/500, Loss: 0.5126669406890869\n",
      "Epoch 133/500, Loss: 0.5120736360549927\n",
      "Epoch 134/500, Loss: 0.5114532709121704\n",
      "Epoch 135/500, Loss: 0.5106954574584961\n",
      "Epoch 136/500, Loss: 0.5098850727081299\n",
      "Epoch 137/500, Loss: 0.5091153383255005\n",
      "Epoch 138/500, Loss: 0.5084853768348694\n",
      "Epoch 139/500, Loss: 0.5079359412193298\n",
      "Epoch 140/500, Loss: 0.5072798728942871\n",
      "Epoch 141/500, Loss: 0.5065348744392395\n",
      "Epoch 142/500, Loss: 0.505781352519989\n",
      "Epoch 143/500, Loss: 0.5051054954528809\n",
      "Epoch 144/500, Loss: 0.5045483112335205\n",
      "Epoch 145/500, Loss: 0.5040243268013\n",
      "Epoch 146/500, Loss: 0.5034541487693787\n",
      "Epoch 147/500, Loss: 0.5027300715446472\n",
      "Epoch 148/500, Loss: 0.5019538998603821\n",
      "Epoch 149/500, Loss: 0.5012788772583008\n",
      "Epoch 150/500, Loss: 0.5006638765335083\n",
      "Epoch 151/500, Loss: 0.5001940131187439\n",
      "Epoch 152/500, Loss: 0.49967092275619507\n",
      "Epoch 153/500, Loss: 0.4990299344062805\n",
      "Epoch 154/500, Loss: 0.4983202815055847\n",
      "Epoch 155/500, Loss: 0.49764466285705566\n",
      "Epoch 156/500, Loss: 0.4970102608203888\n",
      "Epoch 157/500, Loss: 0.4964393079280853\n",
      "Epoch 158/500, Loss: 0.4959067702293396\n",
      "Epoch 159/500, Loss: 0.49535202980041504\n",
      "Epoch 160/500, Loss: 0.4948374629020691\n",
      "Epoch 161/500, Loss: 0.4943116009235382\n",
      "Epoch 162/500, Loss: 0.4937463402748108\n",
      "Epoch 163/500, Loss: 0.4931480884552002\n",
      "Epoch 164/500, Loss: 0.4925231337547302\n",
      "Epoch 165/500, Loss: 0.4919353127479553\n",
      "Epoch 166/500, Loss: 0.49138519167900085\n",
      "Epoch 167/500, Loss: 0.4907996952533722\n",
      "Epoch 168/500, Loss: 0.4902513325214386\n",
      "Epoch 169/500, Loss: 0.4897185266017914\n",
      "Epoch 170/500, Loss: 0.4891727864742279\n",
      "Epoch 171/500, Loss: 0.48864367604255676\n",
      "Epoch 172/500, Loss: 0.488228976726532\n",
      "Epoch 173/500, Loss: 0.48783692717552185\n",
      "Epoch 174/500, Loss: 0.4877053499221802\n",
      "Epoch 175/500, Loss: 0.4876147508621216\n",
      "Epoch 176/500, Loss: 0.4869564175605774\n",
      "Epoch 177/500, Loss: 0.48586195707321167\n",
      "Epoch 178/500, Loss: 0.4850594103336334\n",
      "Epoch 179/500, Loss: 0.48471301794052124\n",
      "Epoch 180/500, Loss: 0.4846198856830597\n",
      "Epoch 181/500, Loss: 0.4841125011444092\n",
      "Epoch 182/500, Loss: 0.483212411403656\n",
      "Epoch 183/500, Loss: 0.4825831949710846\n",
      "Epoch 184/500, Loss: 0.4823013246059418\n",
      "Epoch 185/500, Loss: 0.48199060559272766\n",
      "Epoch 186/500, Loss: 0.48153579235076904\n",
      "Epoch 187/500, Loss: 0.4808485209941864\n",
      "Epoch 188/500, Loss: 0.4801863729953766\n",
      "Epoch 189/500, Loss: 0.4798058271408081\n",
      "Epoch 190/500, Loss: 0.4794567823410034\n",
      "Epoch 191/500, Loss: 0.4790482223033905\n",
      "Epoch 192/500, Loss: 0.4785069525241852\n",
      "Epoch 193/500, Loss: 0.47794660925865173\n",
      "Epoch 194/500, Loss: 0.47746652364730835\n",
      "Epoch 195/500, Loss: 0.47705334424972534\n",
      "Epoch 196/500, Loss: 0.47670087218284607\n",
      "Epoch 197/500, Loss: 0.47632336616516113\n",
      "Epoch 198/500, Loss: 0.4758385717868805\n",
      "Epoch 199/500, Loss: 0.47534409165382385\n",
      "Epoch 200/500, Loss: 0.47485557198524475\n",
      "Epoch 201/500, Loss: 0.4743878245353699\n",
      "Epoch 202/500, Loss: 0.47391945123672485\n",
      "Epoch 203/500, Loss: 0.47349637746810913\n",
      "Epoch 204/500, Loss: 0.47310352325439453\n",
      "Epoch 205/500, Loss: 0.4727356433868408\n",
      "Epoch 206/500, Loss: 0.47246164083480835\n",
      "Epoch 207/500, Loss: 0.4721912145614624\n",
      "Epoch 208/500, Loss: 0.4718964397907257\n",
      "Epoch 209/500, Loss: 0.47146743535995483\n",
      "Epoch 210/500, Loss: 0.47091731429100037\n",
      "Epoch 211/500, Loss: 0.47033020853996277\n",
      "Epoch 212/500, Loss: 0.4697481095790863\n",
      "Epoch 213/500, Loss: 0.4693008065223694\n",
      "Epoch 214/500, Loss: 0.46899205446243286\n",
      "Epoch 215/500, Loss: 0.46871480345726013\n",
      "Epoch 216/500, Loss: 0.4684494137763977\n",
      "Epoch 217/500, Loss: 0.4681003987789154\n",
      "Epoch 218/500, Loss: 0.46761542558670044\n",
      "Epoch 219/500, Loss: 0.467086523771286\n",
      "Epoch 220/500, Loss: 0.4665365219116211\n",
      "Epoch 221/500, Loss: 0.46609923243522644\n",
      "Epoch 222/500, Loss: 0.46575722098350525\n",
      "Epoch 223/500, Loss: 0.46535199880599976\n",
      "Epoch 224/500, Loss: 0.4649781882762909\n",
      "Epoch 225/500, Loss: 0.46471935510635376\n",
      "Epoch 226/500, Loss: 0.4644032418727875\n",
      "Epoch 227/500, Loss: 0.4641120135784149\n",
      "Epoch 228/500, Loss: 0.4638345539569855\n",
      "Epoch 229/500, Loss: 0.46347272396087646\n",
      "Epoch 230/500, Loss: 0.4629653990268707\n",
      "Epoch 231/500, Loss: 0.4623807370662689\n",
      "Epoch 232/500, Loss: 0.46186092495918274\n",
      "Epoch 233/500, Loss: 0.46146926283836365\n",
      "Epoch 234/500, Loss: 0.461068332195282\n",
      "Epoch 235/500, Loss: 0.46069058775901794\n",
      "Epoch 236/500, Loss: 0.46039873361587524\n",
      "Epoch 237/500, Loss: 0.46012431383132935\n",
      "Epoch 238/500, Loss: 0.459834486246109\n",
      "Epoch 239/500, Loss: 0.45963218808174133\n",
      "Epoch 240/500, Loss: 0.45929306745529175\n",
      "Epoch 241/500, Loss: 0.45886197686195374\n",
      "Epoch 242/500, Loss: 0.45840340852737427\n",
      "Epoch 243/500, Loss: 0.4578704833984375\n",
      "Epoch 244/500, Loss: 0.457385390996933\n",
      "Epoch 245/500, Loss: 0.45699170231819153\n",
      "Epoch 246/500, Loss: 0.4566272497177124\n",
      "Epoch 247/500, Loss: 0.4562213122844696\n",
      "Epoch 248/500, Loss: 0.4558899998664856\n",
      "Epoch 249/500, Loss: 0.4555731415748596\n",
      "Epoch 250/500, Loss: 0.45522797107696533\n",
      "Epoch 251/500, Loss: 0.45498278737068176\n",
      "Epoch 252/500, Loss: 0.45480039715766907\n",
      "Epoch 253/500, Loss: 0.4546504318714142\n",
      "Epoch 254/500, Loss: 0.45461127161979675\n",
      "Epoch 255/500, Loss: 0.454505980014801\n",
      "Epoch 256/500, Loss: 0.4540879428386688\n",
      "Epoch 257/500, Loss: 0.45325079560279846\n",
      "Epoch 258/500, Loss: 0.4525398910045624\n",
      "Epoch 259/500, Loss: 0.45202621817588806\n",
      "Epoch 260/500, Loss: 0.4517143666744232\n",
      "Epoch 261/500, Loss: 0.4516342878341675\n",
      "Epoch 262/500, Loss: 0.45144206285476685\n",
      "Epoch 263/500, Loss: 0.4510866403579712\n",
      "Epoch 264/500, Loss: 0.45059412717819214\n",
      "Epoch 265/500, Loss: 0.45012038946151733\n",
      "Epoch 266/500, Loss: 0.4496751129627228\n",
      "Epoch 267/500, Loss: 0.4492649734020233\n",
      "Epoch 268/500, Loss: 0.4490189850330353\n",
      "Epoch 269/500, Loss: 0.4488603472709656\n",
      "Epoch 270/500, Loss: 0.4486881494522095\n",
      "Epoch 271/500, Loss: 0.44853106141090393\n",
      "Epoch 272/500, Loss: 0.448305606842041\n",
      "Epoch 273/500, Loss: 0.4478185176849365\n",
      "Epoch 274/500, Loss: 0.4472156763076782\n",
      "Epoch 275/500, Loss: 0.44672226905822754\n",
      "Epoch 276/500, Loss: 0.446294367313385\n",
      "Epoch 277/500, Loss: 0.44595298171043396\n",
      "Epoch 278/500, Loss: 0.44572097063064575\n",
      "Epoch 279/500, Loss: 0.44555529952049255\n",
      "Epoch 280/500, Loss: 0.44537070393562317\n",
      "Epoch 281/500, Loss: 0.4451550245285034\n",
      "Epoch 282/500, Loss: 0.4449019432067871\n",
      "Epoch 283/500, Loss: 0.44456222653388977\n",
      "Epoch 284/500, Loss: 0.4440235495567322\n",
      "Epoch 285/500, Loss: 0.44340959191322327\n",
      "Epoch 286/500, Loss: 0.4429793953895569\n",
      "Epoch 287/500, Loss: 0.4426180422306061\n",
      "Epoch 288/500, Loss: 0.4423207938671112\n",
      "Epoch 289/500, Loss: 0.44214534759521484\n",
      "Epoch 290/500, Loss: 0.4419964551925659\n",
      "Epoch 291/500, Loss: 0.44179895520210266\n",
      "Epoch 292/500, Loss: 0.4414960443973541\n",
      "Epoch 293/500, Loss: 0.441070020198822\n",
      "Epoch 294/500, Loss: 0.4406014084815979\n",
      "Epoch 295/500, Loss: 0.44014671444892883\n",
      "Epoch 296/500, Loss: 0.43974384665489197\n",
      "Epoch 297/500, Loss: 0.4393942058086395\n",
      "Epoch 298/500, Loss: 0.4391157925128937\n",
      "Epoch 299/500, Loss: 0.4388618767261505\n",
      "Epoch 300/500, Loss: 0.4386071562767029\n",
      "Epoch 301/500, Loss: 0.43844807147979736\n",
      "Epoch 302/500, Loss: 0.4382646977901459\n",
      "Epoch 303/500, Loss: 0.4380878210067749\n",
      "Epoch 304/500, Loss: 0.4378281235694885\n",
      "Epoch 305/500, Loss: 0.4374693036079407\n",
      "Epoch 306/500, Loss: 0.43707147240638733\n",
      "Epoch 307/500, Loss: 0.43662214279174805\n",
      "Epoch 308/500, Loss: 0.436242014169693\n",
      "Epoch 309/500, Loss: 0.4359048008918762\n",
      "Epoch 310/500, Loss: 0.4356086850166321\n",
      "Epoch 311/500, Loss: 0.4354094862937927\n",
      "Epoch 312/500, Loss: 0.43521642684936523\n",
      "Epoch 313/500, Loss: 0.43511608242988586\n",
      "Epoch 314/500, Loss: 0.4351194500923157\n",
      "Epoch 315/500, Loss: 0.43506765365600586\n",
      "Epoch 316/500, Loss: 0.4348655045032501\n",
      "Epoch 317/500, Loss: 0.4343341290950775\n",
      "Epoch 318/500, Loss: 0.43376481533050537\n",
      "Epoch 319/500, Loss: 0.43327805399894714\n",
      "Epoch 320/500, Loss: 0.432918906211853\n",
      "Epoch 321/500, Loss: 0.43277543783187866\n",
      "Epoch 322/500, Loss: 0.43267348408699036\n",
      "Epoch 323/500, Loss: 0.43258097767829895\n",
      "Epoch 324/500, Loss: 0.4323744475841522\n",
      "Epoch 325/500, Loss: 0.43203479051589966\n",
      "Epoch 326/500, Loss: 0.4316544234752655\n",
      "Epoch 327/500, Loss: 0.4312564432621002\n",
      "Epoch 328/500, Loss: 0.4308781921863556\n",
      "Epoch 329/500, Loss: 0.4306403696537018\n",
      "Epoch 330/500, Loss: 0.4305226504802704\n",
      "Epoch 331/500, Loss: 0.43047934770584106\n",
      "Epoch 332/500, Loss: 0.43047404289245605\n",
      "Epoch 333/500, Loss: 0.4302753508090973\n",
      "Epoch 334/500, Loss: 0.4298918843269348\n",
      "Epoch 335/500, Loss: 0.4293658435344696\n",
      "Epoch 336/500, Loss: 0.428932785987854\n",
      "Epoch 337/500, Loss: 0.4287157356739044\n",
      "Epoch 338/500, Loss: 0.4285680949687958\n",
      "Epoch 339/500, Loss: 0.42854052782058716\n",
      "Epoch 340/500, Loss: 0.42839962244033813\n",
      "Epoch 341/500, Loss: 0.42814940214157104\n",
      "Epoch 342/500, Loss: 0.42778050899505615\n",
      "Epoch 343/500, Loss: 0.42744147777557373\n",
      "Epoch 344/500, Loss: 0.42711931467056274\n",
      "Epoch 345/500, Loss: 0.42687004804611206\n",
      "Epoch 346/500, Loss: 0.42664068937301636\n",
      "Epoch 347/500, Loss: 0.4264443814754486\n",
      "Epoch 348/500, Loss: 0.42630547285079956\n",
      "Epoch 349/500, Loss: 0.4262058734893799\n",
      "Epoch 350/500, Loss: 0.4261775612831116\n",
      "Epoch 351/500, Loss: 0.4260646402835846\n",
      "Epoch 352/500, Loss: 0.42592084407806396\n",
      "Epoch 353/500, Loss: 0.4256254732608795\n",
      "Epoch 354/500, Loss: 0.4252350330352783\n",
      "Epoch 355/500, Loss: 0.4248153865337372\n",
      "Epoch 356/500, Loss: 0.4245111346244812\n",
      "Epoch 357/500, Loss: 0.42431044578552246\n",
      "Epoch 358/500, Loss: 0.4241318106651306\n",
      "Epoch 359/500, Loss: 0.4240052402019501\n",
      "Epoch 360/500, Loss: 0.42386114597320557\n",
      "Epoch 361/500, Loss: 0.42367303371429443\n",
      "Epoch 362/500, Loss: 0.4234899878501892\n",
      "Epoch 363/500, Loss: 0.4232625663280487\n",
      "Epoch 364/500, Loss: 0.4230622351169586\n",
      "Epoch 365/500, Loss: 0.42281198501586914\n",
      "Epoch 366/500, Loss: 0.4226011037826538\n",
      "Epoch 367/500, Loss: 0.4224032759666443\n",
      "Epoch 368/500, Loss: 0.4221796691417694\n",
      "Epoch 369/500, Loss: 0.4219800531864166\n",
      "Epoch 370/500, Loss: 0.42178526520729065\n",
      "Epoch 371/500, Loss: 0.4215560555458069\n",
      "Epoch 372/500, Loss: 0.42139196395874023\n",
      "Epoch 373/500, Loss: 0.42121484875679016\n",
      "Epoch 374/500, Loss: 0.42101621627807617\n",
      "Epoch 375/500, Loss: 0.42085662484169006\n",
      "Epoch 376/500, Loss: 0.4206670820713043\n",
      "Epoch 377/500, Loss: 0.4204777479171753\n",
      "Epoch 378/500, Loss: 0.42031803727149963\n",
      "Epoch 379/500, Loss: 0.42013388872146606\n",
      "Epoch 380/500, Loss: 0.4199812412261963\n",
      "Epoch 381/500, Loss: 0.4198688864707947\n",
      "Epoch 382/500, Loss: 0.4198502004146576\n",
      "Epoch 383/500, Loss: 0.41987520456314087\n",
      "Epoch 384/500, Loss: 0.41996338963508606\n",
      "Epoch 385/500, Loss: 0.41991254687309265\n",
      "Epoch 386/500, Loss: 0.41964736580848694\n",
      "Epoch 387/500, Loss: 0.41909244656562805\n",
      "Epoch 388/500, Loss: 0.4186294674873352\n",
      "Epoch 389/500, Loss: 0.4184314012527466\n",
      "Epoch 390/500, Loss: 0.41836369037628174\n",
      "Epoch 391/500, Loss: 0.4183211028575897\n",
      "Epoch 392/500, Loss: 0.41822150349617004\n",
      "Epoch 393/500, Loss: 0.4179827868938446\n",
      "Epoch 394/500, Loss: 0.4176985025405884\n",
      "Epoch 395/500, Loss: 0.4174368679523468\n",
      "Epoch 396/500, Loss: 0.4171874225139618\n",
      "Epoch 397/500, Loss: 0.41700324416160583\n",
      "Epoch 398/500, Loss: 0.4168960154056549\n",
      "Epoch 399/500, Loss: 0.4167841672897339\n",
      "Epoch 400/500, Loss: 0.41665250062942505\n",
      "Epoch 401/500, Loss: 0.41651445627212524\n",
      "Epoch 402/500, Loss: 0.4162977635860443\n",
      "Epoch 403/500, Loss: 0.4160809814929962\n",
      "Epoch 404/500, Loss: 0.4158551096916199\n",
      "Epoch 405/500, Loss: 0.4156371057033539\n",
      "Epoch 406/500, Loss: 0.4154205918312073\n",
      "Epoch 407/500, Loss: 0.4152328670024872\n",
      "Epoch 408/500, Loss: 0.41506028175354004\n",
      "Epoch 409/500, Loss: 0.4148699641227722\n",
      "Epoch 410/500, Loss: 0.41468486189842224\n",
      "Epoch 411/500, Loss: 0.41453316807746887\n",
      "Epoch 412/500, Loss: 0.4143999516963959\n",
      "Epoch 413/500, Loss: 0.41430872678756714\n",
      "Epoch 414/500, Loss: 0.4142264723777771\n",
      "Epoch 415/500, Loss: 0.414186954498291\n",
      "Epoch 416/500, Loss: 0.41411787271499634\n",
      "Epoch 417/500, Loss: 0.41403353214263916\n",
      "Epoch 418/500, Loss: 0.41385307908058167\n",
      "Epoch 419/500, Loss: 0.4136400818824768\n",
      "Epoch 420/500, Loss: 0.41333794593811035\n",
      "Epoch 421/500, Loss: 0.4130089282989502\n",
      "Epoch 422/500, Loss: 0.412681519985199\n",
      "Epoch 423/500, Loss: 0.4124455749988556\n",
      "Epoch 424/500, Loss: 0.4122999310493469\n",
      "Epoch 425/500, Loss: 0.4122087061405182\n",
      "Epoch 426/500, Loss: 0.4121552109718323\n",
      "Epoch 427/500, Loss: 0.41211652755737305\n",
      "Epoch 428/500, Loss: 0.4120504558086395\n",
      "Epoch 429/500, Loss: 0.41190364956855774\n",
      "Epoch 430/500, Loss: 0.41171616315841675\n",
      "Epoch 431/500, Loss: 0.4114573001861572\n",
      "Epoch 432/500, Loss: 0.41121941804885864\n",
      "Epoch 433/500, Loss: 0.4109783470630646\n",
      "Epoch 434/500, Loss: 0.4107436239719391\n",
      "Epoch 435/500, Loss: 0.4105775058269501\n",
      "Epoch 436/500, Loss: 0.41041114926338196\n",
      "Epoch 437/500, Loss: 0.410282701253891\n",
      "Epoch 438/500, Loss: 0.41014549136161804\n",
      "Epoch 439/500, Loss: 0.41005030274391174\n",
      "Epoch 440/500, Loss: 0.40999746322631836\n",
      "Epoch 441/500, Loss: 0.4099116623401642\n",
      "Epoch 442/500, Loss: 0.4098129868507385\n",
      "Epoch 443/500, Loss: 0.409733384847641\n",
      "Epoch 444/500, Loss: 0.4095659554004669\n",
      "Epoch 445/500, Loss: 0.4093630611896515\n",
      "Epoch 446/500, Loss: 0.40917912125587463\n",
      "Epoch 447/500, Loss: 0.4089621901512146\n",
      "Epoch 448/500, Loss: 0.40876543521881104\n",
      "Epoch 449/500, Loss: 0.408599853515625\n",
      "Epoch 450/500, Loss: 0.40842199325561523\n",
      "Epoch 451/500, Loss: 0.4082392752170563\n",
      "Epoch 452/500, Loss: 0.408121794462204\n",
      "Epoch 453/500, Loss: 0.407964289188385\n",
      "Epoch 454/500, Loss: 0.4077932834625244\n",
      "Epoch 455/500, Loss: 0.40767014026641846\n",
      "Epoch 456/500, Loss: 0.4075252413749695\n",
      "Epoch 457/500, Loss: 0.4073648154735565\n",
      "Epoch 458/500, Loss: 0.4072301983833313\n",
      "Epoch 459/500, Loss: 0.40709662437438965\n",
      "Epoch 460/500, Loss: 0.40695255994796753\n",
      "Epoch 461/500, Loss: 0.4068093001842499\n",
      "Epoch 462/500, Loss: 0.4066714644432068\n",
      "Epoch 463/500, Loss: 0.406536728143692\n",
      "Epoch 464/500, Loss: 0.4064106345176697\n",
      "Epoch 465/500, Loss: 0.40627187490463257\n",
      "Epoch 466/500, Loss: 0.4061613380908966\n",
      "Epoch 467/500, Loss: 0.40607962012290955\n",
      "Epoch 468/500, Loss: 0.4060453474521637\n",
      "Epoch 469/500, Loss: 0.4061773717403412\n",
      "Epoch 470/500, Loss: 0.4063504934310913\n",
      "Epoch 471/500, Loss: 0.40664219856262207\n",
      "Epoch 472/500, Loss: 0.40648385882377625\n",
      "Epoch 473/500, Loss: 0.40624913573265076\n",
      "Epoch 474/500, Loss: 0.4055550694465637\n",
      "Epoch 475/500, Loss: 0.4050367772579193\n",
      "Epoch 476/500, Loss: 0.40486931800842285\n",
      "Epoch 477/500, Loss: 0.4049544632434845\n",
      "Epoch 478/500, Loss: 0.40519779920578003\n",
      "Epoch 479/500, Loss: 0.4052557647228241\n",
      "Epoch 480/500, Loss: 0.40509697794914246\n",
      "Epoch 481/500, Loss: 0.40457940101623535\n",
      "Epoch 482/500, Loss: 0.4041630029678345\n",
      "Epoch 483/500, Loss: 0.4040589928627014\n",
      "Epoch 484/500, Loss: 0.4041557013988495\n",
      "Epoch 485/500, Loss: 0.40430954098701477\n",
      "Epoch 486/500, Loss: 0.40414413809776306\n",
      "Epoch 487/500, Loss: 0.40376153588294983\n",
      "Epoch 488/500, Loss: 0.40342214703559875\n",
      "Epoch 489/500, Loss: 0.4033389091491699\n",
      "Epoch 490/500, Loss: 0.4034089148044586\n",
      "Epoch 491/500, Loss: 0.4033874571323395\n",
      "Epoch 492/500, Loss: 0.4032057821750641\n",
      "Epoch 493/500, Loss: 0.402926504611969\n",
      "Epoch 494/500, Loss: 0.40269824862480164\n",
      "Epoch 495/500, Loss: 0.4025838077068329\n",
      "Epoch 496/500, Loss: 0.4025666117668152\n",
      "Epoch 497/500, Loss: 0.4025406241416931\n",
      "Epoch 498/500, Loss: 0.4024322032928467\n",
      "Epoch 499/500, Loss: 0.4022318720817566\n",
      "Epoch 500/500, Loss: 0.40200045704841614\n",
      "Evaluating the model...\n",
      "Test Accuracy: 77.82%\n",
      "Precision: 0.69\n",
      "Recall: 0.63\n",
      "F1 Score: 0.66\n",
      "Confusion Matrix:\n",
      "[[291  49]\n",
      " [ 65 109]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       340\n",
      "           1       0.69      0.63      0.66       174\n",
      "\n",
      "    accuracy                           0.78       514\n",
      "   macro avg       0.75      0.74      0.75       514\n",
      "weighted avg       0.77      0.78      0.78       514\n",
      "\n",
      "Predicting for sequence: MFTLKKSMLLLFFLGTISLSLC\n",
      "Prediction: Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming posDF and negDF are already defined and concatenated into allDataDF\n",
    "allDataDF = pd.concat([posDF, negDF])\n",
    "display(allDataDF)\n",
    "\n",
    "# Define the neural network\n",
    "class BioBERTClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BioBERTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "def get_bert_embeddings(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "sequences = allDataDF['Bioactive sequence'].tolist()\n",
    "labels = allDataDF['Value'].tolist()\n",
    "\n",
    "# Load BioBERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Get embeddings for each sequence\n",
    "print(\"Generating embeddings for sequences...\")\n",
    "embeddings = []\n",
    "for seq in tqdm(sequences, desc=\"Embedding sequences\"):\n",
    "    embedding = get_bert_embeddings(seq, tokenizer, bert_model)\n",
    "    embeddings.append(embedding)\n",
    "    # Print features for the first sequence\n",
    "    if len(embeddings) <= 1:\n",
    "        print(f\"Sequence: {seq}\")\n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"Embedding: {embedding}\")\n",
    "\n",
    "allDataDF2['Bert'] = embeddings\n",
    "display(allDataDF)\n",
    "\n",
    "X = np.array(embeddings).squeeze()\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "print(\"Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Number of classes\n",
    "\n",
    "classifier = BioBERTClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "print(\"Training the neural network...\")\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "classifier.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = classifier(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "    precision = precision_score(y_test, predicted.numpy())\n",
    "    recall = recall_score(y_test, predicted.numpy())\n",
    "    f1 = f1_score(y_test, predicted.numpy())\n",
    "    roc_auc=roc_auc_score(y_test, predicted.numpy())\n",
    "    confusion_mat = confusion_matrix(y_test, predicted.numpy())\n",
    "    class_report = classification_report(y_test, predicted.numpy())\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method3 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "\n",
    "# Function to predict antimicrobial property using BioBERT embeddings\n",
    "def predict_method(sequence, classifier, tokenizer, bert_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    embedding = get_bert_embeddings(sequence, tokenizer, bert_model)\n",
    "    embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier(embedding_tensor)\n",
    "        _, prediction = torch.max(output.data, 1)\n",
    "        result = 'Antimicrobial' if prediction.item() == 1 else 'Non-antimicrobial'\n",
    "    return result, embedding\n",
    "\n",
    "sequence = \"MFTLKKSMLLLFFLGTISLSLC\"\n",
    "prediction, embed = predict_method(sequence, classifier, tokenizer, bert_model)\n",
    "print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c4bde-833f-49f0-bf6c-da6dad7fb3fc",
   "metadata": {},
   "source": [
    "**Word2Vec Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec9addb-a389-4502-a34f-c16085bb31e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sequences for Word2Vec...\n",
      "Training Word2Vec model...\n",
      "Generating sequence vectors...\n",
      "Preparing data for classification...\n",
      "Building the neural network model...\n",
      "Training the model...\n",
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 204ms/step - loss: 0.6720 - accuracy: 0.6360 - val_loss: 0.6566 - val_accuracy: 0.6615\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6641 - accuracy: 0.6360 - val_loss: 0.6488 - val_accuracy: 0.6615\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6586 - accuracy: 0.6360 - val_loss: 0.6434 - val_accuracy: 0.6615\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6549 - accuracy: 0.6360 - val_loss: 0.6394 - val_accuracy: 0.6615\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6521 - accuracy: 0.6360 - val_loss: 0.6364 - val_accuracy: 0.6615\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6500 - accuracy: 0.6360 - val_loss: 0.6340 - val_accuracy: 0.6615\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6479 - accuracy: 0.6360 - val_loss: 0.6322 - val_accuracy: 0.6615\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6456 - accuracy: 0.6360 - val_loss: 0.6306 - val_accuracy: 0.6615\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6434 - accuracy: 0.6360 - val_loss: 0.6291 - val_accuracy: 0.6615\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6413 - accuracy: 0.6360 - val_loss: 0.6280 - val_accuracy: 0.6615\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6395 - accuracy: 0.6360 - val_loss: 0.6272 - val_accuracy: 0.6615\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6378 - accuracy: 0.6360 - val_loss: 0.6264 - val_accuracy: 0.6615\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6363 - accuracy: 0.6360 - val_loss: 0.6251 - val_accuracy: 0.6615\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6347 - accuracy: 0.6360 - val_loss: 0.6233 - val_accuracy: 0.6615\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6329 - accuracy: 0.6360 - val_loss: 0.6220 - val_accuracy: 0.6615\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6313 - accuracy: 0.6360 - val_loss: 0.6208 - val_accuracy: 0.6615\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6299 - accuracy: 0.6360 - val_loss: 0.6196 - val_accuracy: 0.6615\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6284 - accuracy: 0.6360 - val_loss: 0.6189 - val_accuracy: 0.6615\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6272 - accuracy: 0.6360 - val_loss: 0.6184 - val_accuracy: 0.6615\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6261 - accuracy: 0.6360 - val_loss: 0.6175 - val_accuracy: 0.6615\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6249 - accuracy: 0.6360 - val_loss: 0.6160 - val_accuracy: 0.6615\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6235 - accuracy: 0.6360 - val_loss: 0.6137 - val_accuracy: 0.6615\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6218 - accuracy: 0.6360 - val_loss: 0.6112 - val_accuracy: 0.6615\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6208 - accuracy: 0.6360 - val_loss: 0.6097 - val_accuracy: 0.6615\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6201 - accuracy: 0.6360 - val_loss: 0.6088 - val_accuracy: 0.6615\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6195 - accuracy: 0.6360 - val_loss: 0.6080 - val_accuracy: 0.6615\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6190 - accuracy: 0.6360 - val_loss: 0.6073 - val_accuracy: 0.6615\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6188 - accuracy: 0.6360 - val_loss: 0.6068 - val_accuracy: 0.6615\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6185 - accuracy: 0.6360 - val_loss: 0.6062 - val_accuracy: 0.6615\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6181 - accuracy: 0.6360 - val_loss: 0.6057 - val_accuracy: 0.6615\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6174 - accuracy: 0.6360 - val_loss: 0.6052 - val_accuracy: 0.6615\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6166 - accuracy: 0.6360 - val_loss: 0.6047 - val_accuracy: 0.6615\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6159 - accuracy: 0.6360 - val_loss: 0.6043 - val_accuracy: 0.6615\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6153 - accuracy: 0.6360 - val_loss: 0.6040 - val_accuracy: 0.6615\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6147 - accuracy: 0.6360 - val_loss: 0.6036 - val_accuracy: 0.6615\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6138 - accuracy: 0.6360 - val_loss: 0.6034 - val_accuracy: 0.6615\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6129 - accuracy: 0.6355 - val_loss: 0.6032 - val_accuracy: 0.6556\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6121 - accuracy: 0.6350 - val_loss: 0.6032 - val_accuracy: 0.6537\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6114 - accuracy: 0.6326 - val_loss: 0.6033 - val_accuracy: 0.6556\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6108 - accuracy: 0.6330 - val_loss: 0.6034 - val_accuracy: 0.6342\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6104 - accuracy: 0.6282 - val_loss: 0.6038 - val_accuracy: 0.6381\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6101 - accuracy: 0.6306 - val_loss: 0.6046 - val_accuracy: 0.6479\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6101 - accuracy: 0.6369 - val_loss: 0.6054 - val_accuracy: 0.6556\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6102 - accuracy: 0.6350 - val_loss: 0.6050 - val_accuracy: 0.6537\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6099 - accuracy: 0.6360 - val_loss: 0.6045 - val_accuracy: 0.6537\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6094 - accuracy: 0.6360 - val_loss: 0.6042 - val_accuracy: 0.6518\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6091 - accuracy: 0.6360 - val_loss: 0.6028 - val_accuracy: 0.6440\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6083 - accuracy: 0.6355 - val_loss: 0.6012 - val_accuracy: 0.6459\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6075 - accuracy: 0.6369 - val_loss: 0.6003 - val_accuracy: 0.6362\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6071 - accuracy: 0.6291 - val_loss: 0.5996 - val_accuracy: 0.6362\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6069 - accuracy: 0.6291 - val_loss: 0.5992 - val_accuracy: 0.6362\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6066 - accuracy: 0.6311 - val_loss: 0.5993 - val_accuracy: 0.6362\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6063 - accuracy: 0.6301 - val_loss: 0.5995 - val_accuracy: 0.6459\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6061 - accuracy: 0.6379 - val_loss: 0.6000 - val_accuracy: 0.6459\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6061 - accuracy: 0.6345 - val_loss: 0.6003 - val_accuracy: 0.6440\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6061 - accuracy: 0.6360 - val_loss: 0.5999 - val_accuracy: 0.6440\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6059 - accuracy: 0.6355 - val_loss: 0.5994 - val_accuracy: 0.6440\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6057 - accuracy: 0.6369 - val_loss: 0.5992 - val_accuracy: 0.6459\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6056 - accuracy: 0.6374 - val_loss: 0.5992 - val_accuracy: 0.6420\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6055 - accuracy: 0.6350 - val_loss: 0.5989 - val_accuracy: 0.6459\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6054 - accuracy: 0.6374 - val_loss: 0.5982 - val_accuracy: 0.6362\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6055 - accuracy: 0.6296 - val_loss: 0.5978 - val_accuracy: 0.6381\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6058 - accuracy: 0.6335 - val_loss: 0.5977 - val_accuracy: 0.6401\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6059 - accuracy: 0.6335 - val_loss: 0.5976 - val_accuracy: 0.6381\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6058 - accuracy: 0.6340 - val_loss: 0.5978 - val_accuracy: 0.6362\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6053 - accuracy: 0.6291 - val_loss: 0.5984 - val_accuracy: 0.6420\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6050 - accuracy: 0.6345 - val_loss: 0.5988 - val_accuracy: 0.6459\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6049 - accuracy: 0.6355 - val_loss: 0.5988 - val_accuracy: 0.6440\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6048 - accuracy: 0.6345 - val_loss: 0.5983 - val_accuracy: 0.6459\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6047 - accuracy: 0.6350 - val_loss: 0.5975 - val_accuracy: 0.6459\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6050 - accuracy: 0.6418 - val_loss: 0.5973 - val_accuracy: 0.6381\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6056 - accuracy: 0.6340 - val_loss: 0.5973 - val_accuracy: 0.6342\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6062 - accuracy: 0.6291 - val_loss: 0.5973 - val_accuracy: 0.6342\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6063 - accuracy: 0.6287 - val_loss: 0.5972 - val_accuracy: 0.6420\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6059 - accuracy: 0.6335 - val_loss: 0.5973 - val_accuracy: 0.6381\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6050 - accuracy: 0.6301 - val_loss: 0.5981 - val_accuracy: 0.6459\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6043 - accuracy: 0.6365 - val_loss: 0.5998 - val_accuracy: 0.6634\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6045 - accuracy: 0.6569 - val_loss: 0.6022 - val_accuracy: 0.6518\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6056 - accuracy: 0.6477 - val_loss: 0.6048 - val_accuracy: 0.6381\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6072 - accuracy: 0.6321 - val_loss: 0.6059 - val_accuracy: 0.6381\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6078 - accuracy: 0.6291 - val_loss: 0.6052 - val_accuracy: 0.6381\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6073 - accuracy: 0.6326 - val_loss: 0.6031 - val_accuracy: 0.6440\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6060 - accuracy: 0.6418 - val_loss: 0.6003 - val_accuracy: 0.6595\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6045 - accuracy: 0.6550 - val_loss: 0.5983 - val_accuracy: 0.6615\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6040 - accuracy: 0.6433 - val_loss: 0.5974 - val_accuracy: 0.6440\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6041 - accuracy: 0.6355 - val_loss: 0.5973 - val_accuracy: 0.6420\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6042 - accuracy: 0.6360 - val_loss: 0.5974 - val_accuracy: 0.6440\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6040 - accuracy: 0.6355 - val_loss: 0.5978 - val_accuracy: 0.6459\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6039 - accuracy: 0.6326 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6038 - accuracy: 0.6457 - val_loss: 0.5990 - val_accuracy: 0.6634\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6040 - accuracy: 0.6569 - val_loss: 0.6002 - val_accuracy: 0.6556\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6044 - accuracy: 0.6540 - val_loss: 0.6007 - val_accuracy: 0.6576\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6045 - accuracy: 0.6481 - val_loss: 0.6004 - val_accuracy: 0.6537\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6044 - accuracy: 0.6501 - val_loss: 0.6004 - val_accuracy: 0.6518\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6044 - accuracy: 0.6496 - val_loss: 0.5998 - val_accuracy: 0.6595\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6041 - accuracy: 0.6540 - val_loss: 0.5983 - val_accuracy: 0.6576\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.6037 - accuracy: 0.6486 - val_loss: 0.5974 - val_accuracy: 0.6440\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 0.6038 - accuracy: 0.6340 - val_loss: 0.5970 - val_accuracy: 0.6459\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.6042 - accuracy: 0.6365 - val_loss: 0.5969 - val_accuracy: 0.6459\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6047 - accuracy: 0.6394 - val_loss: 0.5969 - val_accuracy: 0.6381\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6055 - accuracy: 0.6311 - val_loss: 0.5971 - val_accuracy: 0.6401\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6060 - accuracy: 0.6335 - val_loss: 0.5970 - val_accuracy: 0.6401\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6057 - accuracy: 0.6321 - val_loss: 0.5969 - val_accuracy: 0.6479\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6049 - accuracy: 0.6408 - val_loss: 0.5969 - val_accuracy: 0.6459\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6044 - accuracy: 0.6374 - val_loss: 0.5969 - val_accuracy: 0.6479\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6043 - accuracy: 0.6374 - val_loss: 0.5970 - val_accuracy: 0.6459\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6040 - accuracy: 0.6350 - val_loss: 0.5973 - val_accuracy: 0.6440\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6037 - accuracy: 0.6345 - val_loss: 0.5973 - val_accuracy: 0.6479\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6037 - accuracy: 0.6340 - val_loss: 0.5973 - val_accuracy: 0.6479\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6036 - accuracy: 0.6340 - val_loss: 0.5978 - val_accuracy: 0.6595\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6035 - accuracy: 0.6452 - val_loss: 0.5987 - val_accuracy: 0.6595\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6036 - accuracy: 0.6540 - val_loss: 0.5992 - val_accuracy: 0.6654\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6037 - accuracy: 0.6516 - val_loss: 0.5990 - val_accuracy: 0.6673\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6036 - accuracy: 0.6520 - val_loss: 0.5983 - val_accuracy: 0.6634\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6034 - accuracy: 0.6594 - val_loss: 0.5974 - val_accuracy: 0.6556\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6036 - accuracy: 0.6365 - val_loss: 0.5970 - val_accuracy: 0.6459\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6041 - accuracy: 0.6345 - val_loss: 0.5970 - val_accuracy: 0.6479\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6045 - accuracy: 0.6374 - val_loss: 0.5970 - val_accuracy: 0.6479\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6045 - accuracy: 0.6374 - val_loss: 0.5970 - val_accuracy: 0.6440\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6043 - accuracy: 0.6369 - val_loss: 0.5970 - val_accuracy: 0.6420\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6041 - accuracy: 0.6365 - val_loss: 0.5970 - val_accuracy: 0.6459\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6041 - accuracy: 0.6350 - val_loss: 0.5970 - val_accuracy: 0.6459\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6041 - accuracy: 0.6350 - val_loss: 0.5971 - val_accuracy: 0.6440\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6039 - accuracy: 0.6345 - val_loss: 0.5974 - val_accuracy: 0.6556\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6036 - accuracy: 0.6360 - val_loss: 0.5979 - val_accuracy: 0.6576\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6034 - accuracy: 0.6472 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6034 - accuracy: 0.6579 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6034 - accuracy: 0.6564 - val_loss: 0.5979 - val_accuracy: 0.6576\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6034 - accuracy: 0.6477 - val_loss: 0.5979 - val_accuracy: 0.6576\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6034 - accuracy: 0.6462 - val_loss: 0.5980 - val_accuracy: 0.6576\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6034 - accuracy: 0.6462 - val_loss: 0.5983 - val_accuracy: 0.6615\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6034 - accuracy: 0.6589 - val_loss: 0.5984 - val_accuracy: 0.6634\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6034 - accuracy: 0.6574 - val_loss: 0.5983 - val_accuracy: 0.6634\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6034 - accuracy: 0.6589 - val_loss: 0.5978 - val_accuracy: 0.6595\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6033 - accuracy: 0.6452 - val_loss: 0.5971 - val_accuracy: 0.6459\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6035 - accuracy: 0.6350 - val_loss: 0.5967 - val_accuracy: 0.6459\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6042 - accuracy: 0.6374 - val_loss: 0.5967 - val_accuracy: 0.6381\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6048 - accuracy: 0.6296 - val_loss: 0.5968 - val_accuracy: 0.6381\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6053 - accuracy: 0.6335 - val_loss: 0.5968 - val_accuracy: 0.6401\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6055 - accuracy: 0.6326 - val_loss: 0.5967 - val_accuracy: 0.6362\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6047 - accuracy: 0.6301 - val_loss: 0.5968 - val_accuracy: 0.6459\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6040 - accuracy: 0.6360 - val_loss: 0.5971 - val_accuracy: 0.6440\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6036 - accuracy: 0.6350 - val_loss: 0.5976 - val_accuracy: 0.6459\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6034 - accuracy: 0.6330 - val_loss: 0.5984 - val_accuracy: 0.6576\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6035 - accuracy: 0.6467 - val_loss: 0.5996 - val_accuracy: 0.6654\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6039 - accuracy: 0.6525 - val_loss: 0.5999 - val_accuracy: 0.6595\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6040 - accuracy: 0.6530 - val_loss: 0.5992 - val_accuracy: 0.6615\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6038 - accuracy: 0.6559 - val_loss: 0.5992 - val_accuracy: 0.6615\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6038 - accuracy: 0.6530 - val_loss: 0.5992 - val_accuracy: 0.6615\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6037 - accuracy: 0.6525 - val_loss: 0.5983 - val_accuracy: 0.6576\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6034 - accuracy: 0.6462 - val_loss: 0.5974 - val_accuracy: 0.6479\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6034 - accuracy: 0.6340 - val_loss: 0.5969 - val_accuracy: 0.6440\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6037 - accuracy: 0.6369 - val_loss: 0.5968 - val_accuracy: 0.6479\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6037 - accuracy: 0.6374 - val_loss: 0.5969 - val_accuracy: 0.6420\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6036 - accuracy: 0.6350 - val_loss: 0.5973 - val_accuracy: 0.6479\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6034 - accuracy: 0.6340 - val_loss: 0.5983 - val_accuracy: 0.6576\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6034 - accuracy: 0.6472 - val_loss: 0.5990 - val_accuracy: 0.6634\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6036 - accuracy: 0.6545 - val_loss: 0.5994 - val_accuracy: 0.6634\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6037 - accuracy: 0.6516 - val_loss: 0.5998 - val_accuracy: 0.6576\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6038 - accuracy: 0.6550 - val_loss: 0.5998 - val_accuracy: 0.6576\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6038 - accuracy: 0.6540 - val_loss: 0.5994 - val_accuracy: 0.6576\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6037 - accuracy: 0.6525 - val_loss: 0.5998 - val_accuracy: 0.6537\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6038 - accuracy: 0.6506 - val_loss: 0.5996 - val_accuracy: 0.6537\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6037 - accuracy: 0.6530 - val_loss: 0.5996 - val_accuracy: 0.6537\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6036 - accuracy: 0.6530 - val_loss: 0.6004 - val_accuracy: 0.6576\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6040 - accuracy: 0.6472 - val_loss: 0.6004 - val_accuracy: 0.6576\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6040 - accuracy: 0.6452 - val_loss: 0.5992 - val_accuracy: 0.6595\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6034 - accuracy: 0.6550 - val_loss: 0.5977 - val_accuracy: 0.6576\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.6031 - accuracy: 0.6462 - val_loss: 0.5968 - val_accuracy: 0.6459\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 0.6034 - accuracy: 0.6360 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 0.6035 - accuracy: 0.6350 - val_loss: 0.5967 - val_accuracy: 0.6440\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6034 - accuracy: 0.6360 - val_loss: 0.5969 - val_accuracy: 0.6479\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6032 - accuracy: 0.6340 - val_loss: 0.5972 - val_accuracy: 0.6518\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6031 - accuracy: 0.6340 - val_loss: 0.5972 - val_accuracy: 0.6518\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6031 - accuracy: 0.6340 - val_loss: 0.5970 - val_accuracy: 0.6440\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6031 - accuracy: 0.6316 - val_loss: 0.5966 - val_accuracy: 0.6440\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6036 - accuracy: 0.6374 - val_loss: 0.5965 - val_accuracy: 0.6459\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6035 - accuracy: 0.6365 - val_loss: 0.5970 - val_accuracy: 0.6440\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6031 - accuracy: 0.6350 - val_loss: 0.5982 - val_accuracy: 0.6634\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6031 - accuracy: 0.6589 - val_loss: 0.6003 - val_accuracy: 0.6556\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6039 - accuracy: 0.6452 - val_loss: 0.6022 - val_accuracy: 0.6459\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6050 - accuracy: 0.6389 - val_loss: 0.6028 - val_accuracy: 0.6420\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6053 - accuracy: 0.6355 - val_loss: 0.6021 - val_accuracy: 0.6459\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6049 - accuracy: 0.6379 - val_loss: 0.6013 - val_accuracy: 0.6537\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6044 - accuracy: 0.6452 - val_loss: 0.5998 - val_accuracy: 0.6537\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6037 - accuracy: 0.6516 - val_loss: 0.5985 - val_accuracy: 0.6634\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6032 - accuracy: 0.6569 - val_loss: 0.5978 - val_accuracy: 0.6615\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6031 - accuracy: 0.6477 - val_loss: 0.5976 - val_accuracy: 0.6518\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6030 - accuracy: 0.6321 - val_loss: 0.5978 - val_accuracy: 0.6615\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6030 - accuracy: 0.6457 - val_loss: 0.5983 - val_accuracy: 0.6634\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6031 - accuracy: 0.6584 - val_loss: 0.5988 - val_accuracy: 0.6634\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6033 - accuracy: 0.6550 - val_loss: 0.5987 - val_accuracy: 0.6595\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6032 - accuracy: 0.6550 - val_loss: 0.5979 - val_accuracy: 0.6576\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6030 - accuracy: 0.6472 - val_loss: 0.5970 - val_accuracy: 0.6440\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6030 - accuracy: 0.6345 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6031 - accuracy: 0.6355 - val_loss: 0.5968 - val_accuracy: 0.6459\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6030 - accuracy: 0.6360 - val_loss: 0.5974 - val_accuracy: 0.6518\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6028 - accuracy: 0.6321 - val_loss: 0.5986 - val_accuracy: 0.6634\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6031 - accuracy: 0.6550 - val_loss: 0.6008 - val_accuracy: 0.6518\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6040 - accuracy: 0.6428 - val_loss: 0.6024 - val_accuracy: 0.6381\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6049 - accuracy: 0.6316 - val_loss: 0.6032 - val_accuracy: 0.6362\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6053 - accuracy: 0.6330 - val_loss: 0.6036 - val_accuracy: 0.6362\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6056 - accuracy: 0.6326 - val_loss: 0.6030 - val_accuracy: 0.6362\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6052 - accuracy: 0.6340 - val_loss: 0.6017 - val_accuracy: 0.6459\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6044 - accuracy: 0.6399 - val_loss: 0.5996 - val_accuracy: 0.6576\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 0.6033 - accuracy: 0.6506 - val_loss: 0.5976 - val_accuracy: 0.6576\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6028 - accuracy: 0.6481 - val_loss: 0.5968 - val_accuracy: 0.6459\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6029 - accuracy: 0.6345 - val_loss: 0.5964 - val_accuracy: 0.6440\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6035 - accuracy: 0.6369 - val_loss: 0.5964 - val_accuracy: 0.6362\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6041 - accuracy: 0.6287 - val_loss: 0.5964 - val_accuracy: 0.6381\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6042 - accuracy: 0.6301 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6040 - accuracy: 0.6413 - val_loss: 0.5964 - val_accuracy: 0.6479\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6038 - accuracy: 0.6404 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6034 - accuracy: 0.6374 - val_loss: 0.5968 - val_accuracy: 0.6440\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6028 - accuracy: 0.6340 - val_loss: 0.5981 - val_accuracy: 0.6634\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6027 - accuracy: 0.6594 - val_loss: 0.6000 - val_accuracy: 0.6537\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6035 - accuracy: 0.6442 - val_loss: 0.6015 - val_accuracy: 0.6537\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6042 - accuracy: 0.6491 - val_loss: 0.6019 - val_accuracy: 0.6459\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 0.6044 - accuracy: 0.6379 - val_loss: 0.6018 - val_accuracy: 0.6459\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 191ms/step - loss: 0.6043 - accuracy: 0.6394 - val_loss: 0.6009 - val_accuracy: 0.6537\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6038 - accuracy: 0.6452 - val_loss: 0.5994 - val_accuracy: 0.6537\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6032 - accuracy: 0.6520 - val_loss: 0.5982 - val_accuracy: 0.6634\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6027 - accuracy: 0.6589 - val_loss: 0.5974 - val_accuracy: 0.6595\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6026 - accuracy: 0.6457 - val_loss: 0.5969 - val_accuracy: 0.6459\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6027 - accuracy: 0.6340 - val_loss: 0.5967 - val_accuracy: 0.6440\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6028 - accuracy: 0.6365 - val_loss: 0.5968 - val_accuracy: 0.6440\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6027 - accuracy: 0.6311 - val_loss: 0.5971 - val_accuracy: 0.6537\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6026 - accuracy: 0.6345 - val_loss: 0.5976 - val_accuracy: 0.6576\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6026 - accuracy: 0.6477 - val_loss: 0.5990 - val_accuracy: 0.6615\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6030 - accuracy: 0.6506 - val_loss: 0.6000 - val_accuracy: 0.6537\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6034 - accuracy: 0.6462 - val_loss: 0.5996 - val_accuracy: 0.6576\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6032 - accuracy: 0.6506 - val_loss: 0.5993 - val_accuracy: 0.6595\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6031 - accuracy: 0.6540 - val_loss: 0.5988 - val_accuracy: 0.6654\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6029 - accuracy: 0.6511 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6027 - accuracy: 0.6569 - val_loss: 0.5980 - val_accuracy: 0.6634\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6026 - accuracy: 0.6589 - val_loss: 0.5983 - val_accuracy: 0.6595\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6027 - accuracy: 0.6545 - val_loss: 0.5985 - val_accuracy: 0.6615\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6028 - accuracy: 0.6555 - val_loss: 0.5983 - val_accuracy: 0.6615\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6027 - accuracy: 0.6550 - val_loss: 0.5983 - val_accuracy: 0.6595\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6027 - accuracy: 0.6545 - val_loss: 0.5985 - val_accuracy: 0.6634\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6028 - accuracy: 0.6555 - val_loss: 0.5985 - val_accuracy: 0.6615\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6028 - accuracy: 0.6555 - val_loss: 0.5984 - val_accuracy: 0.6595\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6027 - accuracy: 0.6555 - val_loss: 0.5980 - val_accuracy: 0.6634\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6026 - accuracy: 0.6589 - val_loss: 0.5979 - val_accuracy: 0.6615\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6026 - accuracy: 0.6535 - val_loss: 0.5978 - val_accuracy: 0.6556\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6026 - accuracy: 0.6462 - val_loss: 0.5975 - val_accuracy: 0.6595\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6025 - accuracy: 0.6447 - val_loss: 0.5969 - val_accuracy: 0.6459\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6025 - accuracy: 0.6330 - val_loss: 0.5966 - val_accuracy: 0.6440\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6026 - accuracy: 0.6355 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6027 - accuracy: 0.6355 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6026 - accuracy: 0.6355 - val_loss: 0.5973 - val_accuracy: 0.6595\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6024 - accuracy: 0.6452 - val_loss: 0.5981 - val_accuracy: 0.6634\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6026 - accuracy: 0.6589 - val_loss: 0.5990 - val_accuracy: 0.6634\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6029 - accuracy: 0.6535 - val_loss: 0.5996 - val_accuracy: 0.6556\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6031 - accuracy: 0.6506 - val_loss: 0.6001 - val_accuracy: 0.6498\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6034 - accuracy: 0.6442 - val_loss: 0.6011 - val_accuracy: 0.6518\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6039 - accuracy: 0.6457 - val_loss: 0.6012 - val_accuracy: 0.6479\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6039 - accuracy: 0.6442 - val_loss: 0.6000 - val_accuracy: 0.6537\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6033 - accuracy: 0.6457 - val_loss: 0.5992 - val_accuracy: 0.6556\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6029 - accuracy: 0.6535 - val_loss: 0.5988 - val_accuracy: 0.6615\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 218ms/step - loss: 0.6028 - accuracy: 0.6520 - val_loss: 0.5981 - val_accuracy: 0.6615\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6025 - accuracy: 0.6569 - val_loss: 0.5973 - val_accuracy: 0.6595\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6024 - accuracy: 0.6438 - val_loss: 0.5967 - val_accuracy: 0.6440\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6025 - accuracy: 0.6321 - val_loss: 0.5963 - val_accuracy: 0.6440\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6028 - accuracy: 0.6369 - val_loss: 0.5961 - val_accuracy: 0.6459\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6031 - accuracy: 0.6384 - val_loss: 0.5961 - val_accuracy: 0.6459\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6035 - accuracy: 0.6413 - val_loss: 0.5962 - val_accuracy: 0.6381\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6040 - accuracy: 0.6306 - val_loss: 0.5963 - val_accuracy: 0.6401\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6044 - accuracy: 0.6316 - val_loss: 0.5962 - val_accuracy: 0.6362\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6040 - accuracy: 0.6301 - val_loss: 0.5962 - val_accuracy: 0.6459\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6031 - accuracy: 0.6384 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6025 - accuracy: 0.6355 - val_loss: 0.5974 - val_accuracy: 0.6615\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6023 - accuracy: 0.6477 - val_loss: 0.5984 - val_accuracy: 0.6634\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6025 - accuracy: 0.6545 - val_loss: 0.6000 - val_accuracy: 0.6537\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6032 - accuracy: 0.6452 - val_loss: 0.6021 - val_accuracy: 0.6420\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6044 - accuracy: 0.6384 - val_loss: 0.6033 - val_accuracy: 0.6362\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6051 - accuracy: 0.6330 - val_loss: 0.6026 - val_accuracy: 0.6342\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6047 - accuracy: 0.6335 - val_loss: 0.6007 - val_accuracy: 0.6537\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6037 - accuracy: 0.6462 - val_loss: 0.5990 - val_accuracy: 0.6654\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6028 - accuracy: 0.6520 - val_loss: 0.5974 - val_accuracy: 0.6615\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6025 - accuracy: 0.6447 - val_loss: 0.5965 - val_accuracy: 0.6459\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6025 - accuracy: 0.6360 - val_loss: 0.5961 - val_accuracy: 0.6479\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6030 - accuracy: 0.6408 - val_loss: 0.5960 - val_accuracy: 0.6420\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6041 - accuracy: 0.6321 - val_loss: 0.5965 - val_accuracy: 0.6556\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6057 - accuracy: 0.6330 - val_loss: 0.5969 - val_accuracy: 0.6556\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6065 - accuracy: 0.6330 - val_loss: 0.5967 - val_accuracy: 0.6556\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6060 - accuracy: 0.6326 - val_loss: 0.5962 - val_accuracy: 0.6304\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6049 - accuracy: 0.6291 - val_loss: 0.5960 - val_accuracy: 0.6401\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6037 - accuracy: 0.6326 - val_loss: 0.5962 - val_accuracy: 0.6440\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6028 - accuracy: 0.6365 - val_loss: 0.5969 - val_accuracy: 0.6420\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6024 - accuracy: 0.6316 - val_loss: 0.5980 - val_accuracy: 0.6556\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6025 - accuracy: 0.6462 - val_loss: 0.5997 - val_accuracy: 0.6518\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6032 - accuracy: 0.6525 - val_loss: 0.6014 - val_accuracy: 0.6518\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 0.6041 - accuracy: 0.6467 - val_loss: 0.6020 - val_accuracy: 0.6556\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 0.6044 - accuracy: 0.6472 - val_loss: 0.6013 - val_accuracy: 0.6518\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 0.6040 - accuracy: 0.6462 - val_loss: 0.5994 - val_accuracy: 0.6615\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6031 - accuracy: 0.6540 - val_loss: 0.5976 - val_accuracy: 0.6615\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6025 - accuracy: 0.6472 - val_loss: 0.5968 - val_accuracy: 0.6440\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6023 - accuracy: 0.6316 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6024 - accuracy: 0.6360 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6024 - accuracy: 0.6360 - val_loss: 0.5966 - val_accuracy: 0.6440\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6023 - accuracy: 0.6350 - val_loss: 0.5970 - val_accuracy: 0.6518\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6022 - accuracy: 0.6345 - val_loss: 0.5977 - val_accuracy: 0.6634\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6023 - accuracy: 0.6589 - val_loss: 0.5987 - val_accuracy: 0.6634\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6026 - accuracy: 0.6520 - val_loss: 0.5993 - val_accuracy: 0.6576\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6029 - accuracy: 0.6516 - val_loss: 0.5991 - val_accuracy: 0.6537\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6028 - accuracy: 0.6520 - val_loss: 0.5984 - val_accuracy: 0.6654\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6025 - accuracy: 0.6520 - val_loss: 0.5979 - val_accuracy: 0.6595\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6023 - accuracy: 0.6555 - val_loss: 0.5972 - val_accuracy: 0.6576\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6023 - accuracy: 0.6467 - val_loss: 0.5968 - val_accuracy: 0.6537\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6023 - accuracy: 0.6340 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6023 - accuracy: 0.6326 - val_loss: 0.5968 - val_accuracy: 0.6556\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6022 - accuracy: 0.6350 - val_loss: 0.5973 - val_accuracy: 0.6576\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6022 - accuracy: 0.6481 - val_loss: 0.5982 - val_accuracy: 0.6673\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6024 - accuracy: 0.6559 - val_loss: 0.5989 - val_accuracy: 0.6654\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6026 - accuracy: 0.6530 - val_loss: 0.5991 - val_accuracy: 0.6576\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6027 - accuracy: 0.6545 - val_loss: 0.5998 - val_accuracy: 0.6556\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6031 - accuracy: 0.6472 - val_loss: 0.6003 - val_accuracy: 0.6479\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6033 - accuracy: 0.6452 - val_loss: 0.5991 - val_accuracy: 0.6654\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6028 - accuracy: 0.6525 - val_loss: 0.5975 - val_accuracy: 0.6595\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6023 - accuracy: 0.6467 - val_loss: 0.5966 - val_accuracy: 0.6440\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6023 - accuracy: 0.6345 - val_loss: 0.5963 - val_accuracy: 0.6459\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6024 - accuracy: 0.6365 - val_loss: 0.5962 - val_accuracy: 0.6440\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6025 - accuracy: 0.6389 - val_loss: 0.5962 - val_accuracy: 0.6459\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6026 - accuracy: 0.6365 - val_loss: 0.5961 - val_accuracy: 0.6440\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 199ms/step - loss: 0.6027 - accuracy: 0.6369 - val_loss: 0.5962 - val_accuracy: 0.6459\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 0.6026 - accuracy: 0.6365 - val_loss: 0.5961 - val_accuracy: 0.6420\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6027 - accuracy: 0.6369 - val_loss: 0.5961 - val_accuracy: 0.6440\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6026 - accuracy: 0.6369 - val_loss: 0.5963 - val_accuracy: 0.6459\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6024 - accuracy: 0.6350 - val_loss: 0.5967 - val_accuracy: 0.6459\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6022 - accuracy: 0.6335 - val_loss: 0.5969 - val_accuracy: 0.6556\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6021 - accuracy: 0.6345 - val_loss: 0.5968 - val_accuracy: 0.6459\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6021 - accuracy: 0.6316 - val_loss: 0.5971 - val_accuracy: 0.6615\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6022 - accuracy: 0.6452 - val_loss: 0.5972 - val_accuracy: 0.6595\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6021 - accuracy: 0.6457 - val_loss: 0.5968 - val_accuracy: 0.6556\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6021 - accuracy: 0.6345 - val_loss: 0.5965 - val_accuracy: 0.6459\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6022 - accuracy: 0.6311 - val_loss: 0.5965 - val_accuracy: 0.6459\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6022 - accuracy: 0.6335 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6022 - accuracy: 0.6311 - val_loss: 0.5966 - val_accuracy: 0.6420\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6021 - accuracy: 0.6316 - val_loss: 0.5969 - val_accuracy: 0.6537\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6021 - accuracy: 0.6350 - val_loss: 0.5973 - val_accuracy: 0.6615\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6021 - accuracy: 0.6481 - val_loss: 0.5978 - val_accuracy: 0.6634\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6022 - accuracy: 0.6589 - val_loss: 0.5983 - val_accuracy: 0.6673\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6023 - accuracy: 0.6545 - val_loss: 0.5989 - val_accuracy: 0.6634\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6025 - accuracy: 0.6545 - val_loss: 0.6001 - val_accuracy: 0.6537\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6031 - accuracy: 0.6438 - val_loss: 0.6009 - val_accuracy: 0.6479\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6035 - accuracy: 0.6433 - val_loss: 0.6007 - val_accuracy: 0.6537\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6034 - accuracy: 0.6457 - val_loss: 0.5998 - val_accuracy: 0.6498\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6029 - accuracy: 0.6472 - val_loss: 0.5987 - val_accuracy: 0.6615\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6024 - accuracy: 0.6525 - val_loss: 0.5979 - val_accuracy: 0.6615\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6022 - accuracy: 0.6589 - val_loss: 0.5975 - val_accuracy: 0.6576\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6021 - accuracy: 0.6452 - val_loss: 0.5976 - val_accuracy: 0.6576\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6021 - accuracy: 0.6457 - val_loss: 0.5977 - val_accuracy: 0.6634\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6021 - accuracy: 0.6540 - val_loss: 0.5980 - val_accuracy: 0.6595\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6022 - accuracy: 0.6559 - val_loss: 0.5984 - val_accuracy: 0.6634\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6023 - accuracy: 0.6501 - val_loss: 0.5986 - val_accuracy: 0.6634\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6024 - accuracy: 0.6506 - val_loss: 0.5980 - val_accuracy: 0.6595\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 0.6022 - accuracy: 0.6564 - val_loss: 0.5974 - val_accuracy: 0.6576\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 0.6020 - accuracy: 0.6457 - val_loss: 0.5976 - val_accuracy: 0.6654\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6021 - accuracy: 0.6530 - val_loss: 0.5978 - val_accuracy: 0.6634\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6021 - accuracy: 0.6555 - val_loss: 0.5978 - val_accuracy: 0.6634\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6021 - accuracy: 0.6540 - val_loss: 0.5976 - val_accuracy: 0.6576\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6020 - accuracy: 0.6462 - val_loss: 0.5974 - val_accuracy: 0.6615\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6020 - accuracy: 0.6481 - val_loss: 0.5977 - val_accuracy: 0.6576\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6020 - accuracy: 0.6452 - val_loss: 0.5986 - val_accuracy: 0.6654\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6023 - accuracy: 0.6525 - val_loss: 0.5992 - val_accuracy: 0.6634\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6026 - accuracy: 0.6530 - val_loss: 0.5992 - val_accuracy: 0.6654\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6026 - accuracy: 0.6530 - val_loss: 0.5987 - val_accuracy: 0.6654\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 0.6024 - accuracy: 0.6501 - val_loss: 0.5986 - val_accuracy: 0.6654\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6023 - accuracy: 0.6550 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6021 - accuracy: 0.6569 - val_loss: 0.5978 - val_accuracy: 0.6576\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6020 - accuracy: 0.6462 - val_loss: 0.5975 - val_accuracy: 0.6595\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6019 - accuracy: 0.6447 - val_loss: 0.5972 - val_accuracy: 0.6556\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6019 - accuracy: 0.6350 - val_loss: 0.5968 - val_accuracy: 0.6420\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6018 - accuracy: 0.6306 - val_loss: 0.5963 - val_accuracy: 0.6420\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6020 - accuracy: 0.6369 - val_loss: 0.5960 - val_accuracy: 0.6459\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6023 - accuracy: 0.6384 - val_loss: 0.5959 - val_accuracy: 0.6459\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6025 - accuracy: 0.6413 - val_loss: 0.5960 - val_accuracy: 0.6459\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6023 - accuracy: 0.6384 - val_loss: 0.5963 - val_accuracy: 0.6420\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6020 - accuracy: 0.6360 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6018 - accuracy: 0.6360 - val_loss: 0.5967 - val_accuracy: 0.6420\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6018 - accuracy: 0.6311 - val_loss: 0.5971 - val_accuracy: 0.6518\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6017 - accuracy: 0.6350 - val_loss: 0.5974 - val_accuracy: 0.6595\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6018 - accuracy: 0.6462 - val_loss: 0.5979 - val_accuracy: 0.6634\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6018 - accuracy: 0.6540 - val_loss: 0.5983 - val_accuracy: 0.6615\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6020 - accuracy: 0.6555 - val_loss: 0.5992 - val_accuracy: 0.6634\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6023 - accuracy: 0.6530 - val_loss: 0.6004 - val_accuracy: 0.6537\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6028 - accuracy: 0.6442 - val_loss: 0.6012 - val_accuracy: 0.6479\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 238ms/step - loss: 0.6033 - accuracy: 0.6433 - val_loss: 0.6013 - val_accuracy: 0.6518\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.6033 - accuracy: 0.6472 - val_loss: 0.6008 - val_accuracy: 0.6518\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6030 - accuracy: 0.6442 - val_loss: 0.5998 - val_accuracy: 0.6556\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6024 - accuracy: 0.6516 - val_loss: 0.5981 - val_accuracy: 0.6615\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6018 - accuracy: 0.6589 - val_loss: 0.5970 - val_accuracy: 0.6556\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6016 - accuracy: 0.6345 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6016 - accuracy: 0.6345 - val_loss: 0.5966 - val_accuracy: 0.6440\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 0.6016 - accuracy: 0.6355 - val_loss: 0.5965 - val_accuracy: 0.6459\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6017 - accuracy: 0.6360 - val_loss: 0.5962 - val_accuracy: 0.6440\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6020 - accuracy: 0.6379 - val_loss: 0.5962 - val_accuracy: 0.6440\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6020 - accuracy: 0.6379 - val_loss: 0.5963 - val_accuracy: 0.6459\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6018 - accuracy: 0.6374 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6018 - accuracy: 0.6365 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6018 - accuracy: 0.6369 - val_loss: 0.5962 - val_accuracy: 0.6459\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6020 - accuracy: 0.6389 - val_loss: 0.5960 - val_accuracy: 0.6362\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6026 - accuracy: 0.6311 - val_loss: 0.5960 - val_accuracy: 0.6381\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 0.6027 - accuracy: 0.6301 - val_loss: 0.5961 - val_accuracy: 0.6479\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6022 - accuracy: 0.6404 - val_loss: 0.5964 - val_accuracy: 0.6420\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6018 - accuracy: 0.6355 - val_loss: 0.5967 - val_accuracy: 0.6459\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6015 - accuracy: 0.6311 - val_loss: 0.5969 - val_accuracy: 0.6420\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6015 - accuracy: 0.6301 - val_loss: 0.5972 - val_accuracy: 0.6537\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6015 - accuracy: 0.6345 - val_loss: 0.5974 - val_accuracy: 0.6595\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6015 - accuracy: 0.6452 - val_loss: 0.5978 - val_accuracy: 0.6673\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.6015 - accuracy: 0.6530 - val_loss: 0.5982 - val_accuracy: 0.6595\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6016 - accuracy: 0.6555 - val_loss: 0.5982 - val_accuracy: 0.6615\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 242ms/step - loss: 0.6016 - accuracy: 0.6555 - val_loss: 0.5980 - val_accuracy: 0.6615\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 0.6015 - accuracy: 0.6594 - val_loss: 0.5976 - val_accuracy: 0.6576\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6015 - accuracy: 0.6462 - val_loss: 0.5973 - val_accuracy: 0.6595\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6015 - accuracy: 0.6462 - val_loss: 0.5975 - val_accuracy: 0.6576\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6015 - accuracy: 0.6462 - val_loss: 0.5985 - val_accuracy: 0.6634\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6017 - accuracy: 0.6525 - val_loss: 0.5997 - val_accuracy: 0.6498\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6021 - accuracy: 0.6462 - val_loss: 0.6008 - val_accuracy: 0.6518\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6027 - accuracy: 0.6447 - val_loss: 0.6008 - val_accuracy: 0.6518\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6027 - accuracy: 0.6457 - val_loss: 0.5997 - val_accuracy: 0.6479\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6022 - accuracy: 0.6447 - val_loss: 0.5990 - val_accuracy: 0.6576\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.6019 - accuracy: 0.6535 - val_loss: 0.5987 - val_accuracy: 0.6634\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6018 - accuracy: 0.6540 - val_loss: 0.5980 - val_accuracy: 0.6654\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6016 - accuracy: 0.6569 - val_loss: 0.5972 - val_accuracy: 0.6595\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6016 - accuracy: 0.6442 - val_loss: 0.5966 - val_accuracy: 0.6459\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6021 - accuracy: 0.6369 - val_loss: 0.5964 - val_accuracy: 0.6420\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6025 - accuracy: 0.6360 - val_loss: 0.5964 - val_accuracy: 0.6459\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6029 - accuracy: 0.6389 - val_loss: 0.5964 - val_accuracy: 0.6479\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6030 - accuracy: 0.6394 - val_loss: 0.5963 - val_accuracy: 0.6459\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6028 - accuracy: 0.6389 - val_loss: 0.5963 - val_accuracy: 0.6440\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6025 - accuracy: 0.6384 - val_loss: 0.5963 - val_accuracy: 0.6420\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6024 - accuracy: 0.6365 - val_loss: 0.5963 - val_accuracy: 0.6420\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6020 - accuracy: 0.6365 - val_loss: 0.5967 - val_accuracy: 0.6459\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6015 - accuracy: 0.6326 - val_loss: 0.5977 - val_accuracy: 0.6634\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6015 - accuracy: 0.6530 - val_loss: 0.5990 - val_accuracy: 0.6634\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.6018 - accuracy: 0.6535 - val_loss: 0.5997 - val_accuracy: 0.6479\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 222ms/step - loss: 0.6021 - accuracy: 0.6496 - val_loss: 0.6001 - val_accuracy: 0.6518\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.6023 - accuracy: 0.6438 - val_loss: 0.6003 - val_accuracy: 0.6518\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6024 - accuracy: 0.6438 - val_loss: 0.6009 - val_accuracy: 0.6518\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6027 - accuracy: 0.6457 - val_loss: 0.6014 - val_accuracy: 0.6576\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6029 - accuracy: 0.6472 - val_loss: 0.6006 - val_accuracy: 0.6537\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.6025 - accuracy: 0.6428 - val_loss: 0.5995 - val_accuracy: 0.6537\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6020 - accuracy: 0.6516 - val_loss: 0.5987 - val_accuracy: 0.6634\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6016 - accuracy: 0.6525 - val_loss: 0.5979 - val_accuracy: 0.6615\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6014 - accuracy: 0.6584 - val_loss: 0.5972 - val_accuracy: 0.6615\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.6013 - accuracy: 0.6442 - val_loss: 0.5965 - val_accuracy: 0.6440\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6015 - accuracy: 0.6365 - val_loss: 0.5962 - val_accuracy: 0.6440\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6019 - accuracy: 0.6389 - val_loss: 0.5961 - val_accuracy: 0.6459\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6023 - accuracy: 0.6418 - val_loss: 0.5961 - val_accuracy: 0.6479\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6021 - accuracy: 0.6399 - val_loss: 0.5963 - val_accuracy: 0.6459\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6017 - accuracy: 0.6365 - val_loss: 0.5967 - val_accuracy: 0.6459\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6014 - accuracy: 0.6355 - val_loss: 0.5974 - val_accuracy: 0.6595\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6013 - accuracy: 0.6452 - val_loss: 0.5980 - val_accuracy: 0.6634\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.6014 - accuracy: 0.6589 - val_loss: 0.5987 - val_accuracy: 0.6654\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.6016 - accuracy: 0.6535 - val_loss: 0.5991 - val_accuracy: 0.6615\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6018 - accuracy: 0.6525 - val_loss: 0.5993 - val_accuracy: 0.6634\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6019 - accuracy: 0.6535 - val_loss: 0.5990 - val_accuracy: 0.6634\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.6017 - accuracy: 0.6525 - val_loss: 0.5978 - val_accuracy: 0.6576\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6014 - accuracy: 0.6462 - val_loss: 0.5967 - val_accuracy: 0.6440\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6013 - accuracy: 0.6311 - val_loss: 0.5961 - val_accuracy: 0.6440\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.6017 - accuracy: 0.6389 - val_loss: 0.5961 - val_accuracy: 0.6420\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 207ms/step - loss: 0.6032 - accuracy: 0.6335 - val_loss: 0.5970 - val_accuracy: 0.6556\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.6056 - accuracy: 0.6330 - val_loss: 0.5980 - val_accuracy: 0.6556\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.6075 - accuracy: 0.6330 - val_loss: 0.5984 - val_accuracy: 0.6537\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.6082 - accuracy: 0.6330 - val_loss: 0.5978 - val_accuracy: 0.6556\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6071 - accuracy: 0.6330 - val_loss: 0.5969 - val_accuracy: 0.6556\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6053 - accuracy: 0.6321 - val_loss: 0.5963 - val_accuracy: 0.6342\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6038 - accuracy: 0.6291 - val_loss: 0.5960 - val_accuracy: 0.6362\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.6025 - accuracy: 0.6306 - val_loss: 0.5963 - val_accuracy: 0.6420\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6016 - accuracy: 0.6365 - val_loss: 0.5969 - val_accuracy: 0.6537\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6013 - accuracy: 0.6350 - val_loss: 0.5977 - val_accuracy: 0.6634\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.6013 - accuracy: 0.6579 - val_loss: 0.5985 - val_accuracy: 0.6634\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.6015 - accuracy: 0.6520 - val_loss: 0.5990 - val_accuracy: 0.6615\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6017 - accuracy: 0.6535 - val_loss: 0.5987 - val_accuracy: 0.6654\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.6016 - accuracy: 0.6540 - val_loss: 0.5982 - val_accuracy: 0.6654\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.6014 - accuracy: 0.6530 - val_loss: 0.5979 - val_accuracy: 0.6615\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6014 - accuracy: 0.6564 - val_loss: 0.5979 - val_accuracy: 0.6615\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6013 - accuracy: 0.6559 - val_loss: 0.5987 - val_accuracy: 0.6634\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.6015 - accuracy: 0.6535 - val_loss: 0.5997 - val_accuracy: 0.6479\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.6020 - accuracy: 0.6467 - val_loss: 0.6003 - val_accuracy: 0.6537\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6022 - accuracy: 0.6442 - val_loss: 0.6004 - val_accuracy: 0.6537\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.6023 - accuracy: 0.6442 - val_loss: 0.6003 - val_accuracy: 0.6518\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.6022 - accuracy: 0.6447 - val_loss: 0.6004 - val_accuracy: 0.6498\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6022 - accuracy: 0.6452 - val_loss: 0.5998 - val_accuracy: 0.6479\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6019 - accuracy: 0.6501 - val_loss: 0.5986 - val_accuracy: 0.6634\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 386ms/step - loss: 0.6015 - accuracy: 0.6520 - val_loss: 0.5972 - val_accuracy: 0.6615\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 0.6012 - accuracy: 0.6452 - val_loss: 0.5964 - val_accuracy: 0.6440\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 0.6013 - accuracy: 0.6365 - val_loss: 0.5960 - val_accuracy: 0.6479\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 0.6019 - accuracy: 0.6394 - val_loss: 0.5960 - val_accuracy: 0.6381\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.6028 - accuracy: 0.6340 - val_loss: 0.5961 - val_accuracy: 0.6342\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6036 - accuracy: 0.6282 - val_loss: 0.5961 - val_accuracy: 0.6323\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.6037 - accuracy: 0.6287 - val_loss: 0.5960 - val_accuracy: 0.6304\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.6035 - accuracy: 0.6277 - val_loss: 0.5959 - val_accuracy: 0.6342\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.6032 - accuracy: 0.6287 - val_loss: 0.5958 - val_accuracy: 0.6342\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.6027 - accuracy: 0.6282 - val_loss: 0.5958 - val_accuracy: 0.6401\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.6023 - accuracy: 0.6335 - val_loss: 0.5959 - val_accuracy: 0.6362\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.6018 - accuracy: 0.6301 - val_loss: 0.5963 - val_accuracy: 0.6440\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.6014 - accuracy: 0.6389 - val_loss: 0.5967 - val_accuracy: 0.6440\n",
      "Evaluating the model...\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 4 (Word2Vec):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.643969\n",
      "1  Precision  0.421053\n",
      "2     Recall  0.137931\n",
      "3   F1 Score  0.207792\n",
      "4    ROC AUC  0.660505\n",
      "\n",
      "Confusion Matrix for Method 4 (Word2Vec):\n",
      " [[307  33]\n",
      " [150  24]]\n",
      "Generating classification report...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77       340\n",
      "           1       0.42      0.14      0.21       174\n",
      "\n",
      "    accuracy                           0.64       514\n",
      "   macro avg       0.55      0.52      0.49       514\n",
      "weighted avg       0.59      0.64      0.58       514\n",
      "\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Prediction for sequence 'GLWSKIKEVGKEAAKAAAKAAG': Antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Assuming build_nn_model function is defined elsewhere\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define your data loading and preprocessing steps\n",
    "# Example assumes 'allDataDF', 'posDF', 'negDF', and other variables are defined as in your original code\n",
    "\n",
    "# Ensure indices are set correctly\n",
    "allDataDF = allDataDF.set_index('DADP ID')\n",
    "posDF = posDF.set_index('DADP ID')\n",
    "\n",
    "# Filter out sequences with invalid characters\n",
    "invalid_chars = \"/\"\n",
    "negDF = allDataDF[~allDataDF.index.isin(posDF.index)]\n",
    "negDF = negDF[~negDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "posDF = posDF[~posDF['Bioactive sequence'].str.contains(invalid_chars)]\n",
    "\n",
    "# Process sequences for Word2Vec\n",
    "print(\"Processing sequences for Word2Vec...\")\n",
    "pos_sequences = posDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "neg_sequences = negDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(sentences=all_sequences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate sequence vectors\n",
    "print(\"Generating sequence vectors...\")\n",
    "def sequence_vector(seq, model):\n",
    "    return np.mean([model.wv[char] for char in seq if char in model.wv], axis=0)\n",
    "\n",
    "posDF['vector'] = posDF['Bioactive sequence'].str.upper().apply(lambda x: sequence_vector(list(x), w2v_model))\n",
    "negDF['vector'] = negDF['Bioactive sequence'].str.upper().apply(lambda x: sequence_vector(list(x), w2v_model))\n",
    "\n",
    "# Prepare data for classification\n",
    "print(\"Preparing data for classification...\")\n",
    "allDataDF3 = pd.concat([posDF, negDF])\n",
    "X = np.stack(allDataDF3['vector'].values)\n",
    "y = np.array([1]*len(posDF) + [0]*len(negDF))\n",
    "allDataDF2['vector']=allDataDF3['vector']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Build the neural network model using build_nn_model function\n",
    "print(\"Building the neural network model...\")\n",
    "model = build_nn_model(input_dim=X.shape[1])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=2000, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method4 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 4 (Word2Vec):\\n\", performance_metrics_method4)\n",
    "print(\"\\nConfusion Matrix for Method 4 (Word2Vec):\\n\", conf_matrix)\n",
    "\n",
    "# Generate classification report\n",
    "print(\"Generating classification report...\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function to predict antimicrobial property using the neural network model\n",
    "def predict_nn(sequence, model, w2v_model):\n",
    "    sequence = sequence.upper()\n",
    "    vector = sequence_vector(list(sequence), w2v_model)\n",
    "    prediction = model.predict(np.array([vector]))[0][0]\n",
    "    return 'Antimicrobial' if prediction > 0.5 else 'Non-antimicrobial'\n",
    "\n",
    "# Predictions using the neural network model\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "prediction_nn = predict_nn(sequence, model, w2v_model)\n",
    "print(f\"Prediction for sequence '{sequence}': {prediction_nn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee92c4-fc82-4231-86ad-43731a026b08",
   "metadata": {},
   "source": [
    "**Combined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3893737f-0127-47bf-811c-a68f511b23e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.09701026, 0.023356073, 0.19239551, 0.34847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.10609158, 0.031600513, 0.15865144, 0.31917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.107020475, 0.032668713, 0.16226107, 0.3235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.1107386, 0.036594443, 0.15842809, 0.322235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.10410814, 0.046210337, 0.17820424, 0.33955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11710616, 0.032537665, 0.117993765, 0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.1260696, -0.010238703, 0.0796683, 0.233125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.16503033, -0.014033677, -0.032334927, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.145197, -0.018769035, 0.019576171, 0.18281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.109951705, 0.013725347, 0.11754644, 0.2610...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.09701026, 0.023356073, 0.19239551, 0.34847...  \n",
       "SP_2643    [-0.10609158, 0.031600513, 0.15865144, 0.31917...  \n",
       "SP_2644    [-0.107020475, 0.032668713, 0.16226107, 0.3235...  \n",
       "SP_2645    [-0.1107386, 0.036594443, 0.15842809, 0.322235...  \n",
       "SP_2646    [-0.10410814, 0.046210337, 0.17820424, 0.33955...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11710616, 0.032537665, 0.117993765, 0.2792...  \n",
       "SP_2853    [-0.1260696, -0.010238703, 0.0796683, 0.233125...  \n",
       "SP_2854    [-0.16503033, -0.014033677, -0.032334927, 0.12...  \n",
       "SP_2855    [-0.145197, -0.018769035, 0.019576171, 0.18281...  \n",
       "SP_Q09022  [-0.109951705, 0.013725347, 0.11754644, 0.2610...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \n",
       "DADP ID                                                                   \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319  \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875  \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542  \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154  \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037  \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541  \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362  \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276  \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679  \n",
       "\n",
       "[2566 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1333</th>\n",
       "      <th>1334</th>\n",
       "      <th>1335</th>\n",
       "      <th>1336</th>\n",
       "      <th>1337</th>\n",
       "      <th>1338</th>\n",
       "      <th>1339</th>\n",
       "      <th>1340</th>\n",
       "      <th>1341</th>\n",
       "      <th>1342</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060075</td>\n",
       "      <td>0.072157</td>\n",
       "      <td>-0.057001</td>\n",
       "      <td>-0.199033</td>\n",
       "      <td>0.213588</td>\n",
       "      <td>0.297404</td>\n",
       "      <td>-0.060959</td>\n",
       "      <td>-0.105691</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.212193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.069961</td>\n",
       "      <td>-0.063975</td>\n",
       "      <td>-0.183580</td>\n",
       "      <td>0.222905</td>\n",
       "      <td>0.289790</td>\n",
       "      <td>-0.058714</td>\n",
       "      <td>-0.099017</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.197055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041615</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>-0.062850</td>\n",
       "      <td>-0.183642</td>\n",
       "      <td>0.221408</td>\n",
       "      <td>0.290012</td>\n",
       "      <td>-0.060219</td>\n",
       "      <td>-0.097840</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.200506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>-0.181834</td>\n",
       "      <td>0.219751</td>\n",
       "      <td>0.290568</td>\n",
       "      <td>-0.060691</td>\n",
       "      <td>-0.096536</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.202205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.083404</td>\n",
       "      <td>-0.051983</td>\n",
       "      <td>-0.184543</td>\n",
       "      <td>0.197916</td>\n",
       "      <td>0.295361</td>\n",
       "      <td>-0.080805</td>\n",
       "      <td>-0.106384</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.213447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>0.059670</td>\n",
       "      <td>-0.081960</td>\n",
       "      <td>-0.166358</td>\n",
       "      <td>0.249094</td>\n",
       "      <td>0.268477</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.169574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038159</td>\n",
       "      <td>0.017906</td>\n",
       "      <td>-0.160036</td>\n",
       "      <td>-0.169702</td>\n",
       "      <td>0.345982</td>\n",
       "      <td>0.219215</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>-0.026315</td>\n",
       "      <td>0.075662</td>\n",
       "      <td>0.143840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>-0.030374</td>\n",
       "      <td>-0.224305</td>\n",
       "      <td>-0.136530</td>\n",
       "      <td>0.415614</td>\n",
       "      <td>0.163084</td>\n",
       "      <td>0.064353</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>0.094428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029248</td>\n",
       "      <td>-0.010688</td>\n",
       "      <td>-0.193861</td>\n",
       "      <td>-0.153010</td>\n",
       "      <td>0.369639</td>\n",
       "      <td>0.194822</td>\n",
       "      <td>0.043160</td>\n",
       "      <td>-0.007631</td>\n",
       "      <td>0.083578</td>\n",
       "      <td>0.123238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040760</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>-0.112429</td>\n",
       "      <td>-0.175255</td>\n",
       "      <td>0.286306</td>\n",
       "      <td>0.246535</td>\n",
       "      <td>-0.018752</td>\n",
       "      <td>-0.074633</td>\n",
       "      <td>0.040948</td>\n",
       "      <td>0.159354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 1343 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2          3         4         5         6     \\\n",
       "0     3181.6835   9.701025  0.030303   7.272727  0.196970  2.762539  3181.736   \n",
       "1     2279.6794   9.703153  0.043478 -12.986957  0.630435  1.845400  2279.719   \n",
       "2     2293.7060   9.703153  0.043478  -4.613043  0.630435  1.847089  2293.746   \n",
       "3     2266.6807   9.703153  0.043478  -4.613043  0.747826  1.847089  2266.720   \n",
       "4     1583.9103   8.750052  0.000000  10.800000  1.275000  0.759103  1583.937   \n",
       "...         ...        ...       ...        ...       ...       ...       ...   \n",
       "2561  1405.7470  10.002350  0.076923  16.907692  1.000000  1.731990  1405.774   \n",
       "2562  2124.3526   4.651158  0.166667  37.416667 -0.455556 -2.147981  2124.388   \n",
       "2563  2138.4670   8.045284  0.150000  72.685000 -0.230000  0.739412  2138.506   \n",
       "2564  2076.4408   9.820679  0.117647  50.894118 -0.911765  2.732905  2076.477   \n",
       "2565  7235.6096   8.884082  0.015152  44.725758 -0.174242  4.685244  7235.743   \n",
       "\n",
       "          7        8      9     ...      1333      1334      1335      1336  \\\n",
       "0    -13.16030  1318.79   46.0  ...  0.060075  0.072157 -0.057001 -0.199033   \n",
       "1     -7.85050   910.88   31.0  ...  0.040859  0.069961 -0.063975 -0.183580   \n",
       "2     -7.46040   910.88   31.0  ...  0.041615  0.071203 -0.062850 -0.183642   \n",
       "3     -7.34350   888.02   31.0  ...  0.038818  0.071274 -0.060835 -0.181834   \n",
       "4     -5.33460   629.62   22.0  ...  0.038598  0.083404 -0.051983 -0.184543   \n",
       "...        ...      ...    ...  ...       ...       ...       ...       ...   \n",
       "2561  -2.94950   527.88   18.0  ...  0.027876  0.059670 -0.081960 -0.166358   \n",
       "2562  -2.70150   801.94   26.0  ...  0.038159  0.017906 -0.160036 -0.169702   \n",
       "2563  -6.72983   758.18   26.0  ...  0.011908 -0.030374 -0.224305 -0.136530   \n",
       "2564  -4.87636   848.49   30.0  ...  0.029248 -0.010688 -0.193861 -0.153010   \n",
       "2565 -36.35396  2988.65  111.0  ...  0.040760  0.040252 -0.112429 -0.175255   \n",
       "\n",
       "          1337      1338      1339      1340      1341      1342  \n",
       "0     0.213588  0.297404 -0.060959 -0.105691  0.021453  0.212193  \n",
       "1     0.222905  0.289790 -0.058714 -0.099017  0.023472  0.197055  \n",
       "2     0.221408  0.290012 -0.060219 -0.097840  0.023686  0.200506  \n",
       "3     0.219751  0.290568 -0.060691 -0.096536  0.022934  0.202205  \n",
       "4     0.197916  0.295361 -0.080805 -0.106384  0.008533  0.213447  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2561  0.249094  0.268477 -0.037353 -0.086254  0.035719  0.169574  \n",
       "2562  0.345982  0.219215  0.007181 -0.026315  0.075662  0.143840  \n",
       "2563  0.415614  0.163084  0.064353  0.024089  0.091599  0.094428  \n",
       "2564  0.369639  0.194822  0.043160 -0.007631  0.083578  0.123238  \n",
       "2565  0.286306  0.246535 -0.018752 -0.074633  0.040948  0.159354  \n",
       "\n",
       "[2566 rows x 1343 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2566, 19) (2566, 506) (2566, 50) (2566, 768) (2566, 1343)\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.8794 - accuracy: 0.4711 - val_loss: 0.6043 - val_accuracy: 0.6253\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.7628 - accuracy: 0.5393 - val_loss: 0.5781 - val_accuracy: 0.6861\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6803 - accuracy: 0.6015 - val_loss: 0.5700 - val_accuracy: 0.7032\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6678 - accuracy: 0.6191 - val_loss: 0.5679 - val_accuracy: 0.7178\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6654 - accuracy: 0.6179 - val_loss: 0.5656 - val_accuracy: 0.7105\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6313 - accuracy: 0.6453 - val_loss: 0.5620 - val_accuracy: 0.7275\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.6390 - accuracy: 0.6453 - val_loss: 0.5577 - val_accuracy: 0.7372\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6027 - accuracy: 0.6612 - val_loss: 0.5530 - val_accuracy: 0.7397\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6035 - accuracy: 0.6612 - val_loss: 0.5495 - val_accuracy: 0.7299\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.5740 - accuracy: 0.6606 - val_loss: 0.5469 - val_accuracy: 0.7372\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.5670 - accuracy: 0.6667 - val_loss: 0.5456 - val_accuracy: 0.7445\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.5729 - accuracy: 0.6813 - val_loss: 0.5453 - val_accuracy: 0.7470\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5802 - accuracy: 0.6856 - val_loss: 0.5466 - val_accuracy: 0.7324\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.5582 - accuracy: 0.6843 - val_loss: 0.5480 - val_accuracy: 0.7275\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5491 - accuracy: 0.6910 - val_loss: 0.5490 - val_accuracy: 0.7299\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.5423 - accuracy: 0.6990 - val_loss: 0.5490 - val_accuracy: 0.7324\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5285 - accuracy: 0.7172 - val_loss: 0.5487 - val_accuracy: 0.7397\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.5215 - accuracy: 0.7203 - val_loss: 0.5486 - val_accuracy: 0.7518\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.5274 - accuracy: 0.7112 - val_loss: 0.5486 - val_accuracy: 0.7372\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5080 - accuracy: 0.7185 - val_loss: 0.5484 - val_accuracy: 0.7372\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.5204 - accuracy: 0.7264 - val_loss: 0.5480 - val_accuracy: 0.7348\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5139 - accuracy: 0.7258 - val_loss: 0.5485 - val_accuracy: 0.7372\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4964 - accuracy: 0.7380 - val_loss: 0.5490 - val_accuracy: 0.7397\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4955 - accuracy: 0.7258 - val_loss: 0.5495 - val_accuracy: 0.7372\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4891 - accuracy: 0.7239 - val_loss: 0.5505 - val_accuracy: 0.7397\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4731 - accuracy: 0.7569 - val_loss: 0.5516 - val_accuracy: 0.7494\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4703 - accuracy: 0.7556 - val_loss: 0.5536 - val_accuracy: 0.7494\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4762 - accuracy: 0.7410 - val_loss: 0.5556 - val_accuracy: 0.7470\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4604 - accuracy: 0.7751 - val_loss: 0.5580 - val_accuracy: 0.7299\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4569 - accuracy: 0.7502 - val_loss: 0.5607 - val_accuracy: 0.7324\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4422 - accuracy: 0.7770 - val_loss: 0.5637 - val_accuracy: 0.7275\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.4467 - accuracy: 0.7690 - val_loss: 0.5674 - val_accuracy: 0.7299\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4387 - accuracy: 0.7770 - val_loss: 0.5707 - val_accuracy: 0.7348\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.4393 - accuracy: 0.7690 - val_loss: 0.5738 - val_accuracy: 0.7299\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.4498 - accuracy: 0.7788 - val_loss: 0.5761 - val_accuracy: 0.7348\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4260 - accuracy: 0.7837 - val_loss: 0.5786 - val_accuracy: 0.7397\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4275 - accuracy: 0.7904 - val_loss: 0.5808 - val_accuracy: 0.7397\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4149 - accuracy: 0.7867 - val_loss: 0.5831 - val_accuracy: 0.7543\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.4256 - accuracy: 0.7885 - val_loss: 0.5853 - val_accuracy: 0.7543\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.4116 - accuracy: 0.7946 - val_loss: 0.5868 - val_accuracy: 0.7543\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3994 - accuracy: 0.8050 - val_loss: 0.5885 - val_accuracy: 0.7518\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3926 - accuracy: 0.8087 - val_loss: 0.5896 - val_accuracy: 0.7494\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4074 - accuracy: 0.7952 - val_loss: 0.5900 - val_accuracy: 0.7470\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3793 - accuracy: 0.8160 - val_loss: 0.5915 - val_accuracy: 0.7397\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3828 - accuracy: 0.8160 - val_loss: 0.5928 - val_accuracy: 0.7397\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3919 - accuracy: 0.8166 - val_loss: 0.5947 - val_accuracy: 0.7372\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3742 - accuracy: 0.8184 - val_loss: 0.5965 - val_accuracy: 0.7348\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3654 - accuracy: 0.8300 - val_loss: 0.5995 - val_accuracy: 0.7372\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3608 - accuracy: 0.8294 - val_loss: 0.6036 - val_accuracy: 0.7494\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3624 - accuracy: 0.8251 - val_loss: 0.6080 - val_accuracy: 0.7543\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3551 - accuracy: 0.8245 - val_loss: 0.6132 - val_accuracy: 0.7470\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.3387 - accuracy: 0.8434 - val_loss: 0.6193 - val_accuracy: 0.7494\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3459 - accuracy: 0.8446 - val_loss: 0.6247 - val_accuracy: 0.7494\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3429 - accuracy: 0.8416 - val_loss: 0.6302 - val_accuracy: 0.7518\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3350 - accuracy: 0.8416 - val_loss: 0.6355 - val_accuracy: 0.7494\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.3250 - accuracy: 0.8483 - val_loss: 0.6393 - val_accuracy: 0.7494\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3400 - accuracy: 0.8397 - val_loss: 0.6438 - val_accuracy: 0.7494\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3389 - accuracy: 0.8416 - val_loss: 0.6488 - val_accuracy: 0.7567\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3344 - accuracy: 0.8434 - val_loss: 0.6541 - val_accuracy: 0.7543\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3155 - accuracy: 0.8519 - val_loss: 0.6597 - val_accuracy: 0.7591\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3107 - accuracy: 0.8623 - val_loss: 0.6674 - val_accuracy: 0.7616\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2876 - accuracy: 0.8708 - val_loss: 0.6750 - val_accuracy: 0.7616\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3091 - accuracy: 0.8495 - val_loss: 0.6828 - val_accuracy: 0.7591\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2826 - accuracy: 0.8690 - val_loss: 0.6915 - val_accuracy: 0.7567\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2798 - accuracy: 0.8726 - val_loss: 0.7001 - val_accuracy: 0.7567\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2836 - accuracy: 0.8708 - val_loss: 0.7097 - val_accuracy: 0.7543\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2903 - accuracy: 0.8726 - val_loss: 0.7197 - val_accuracy: 0.7518\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2771 - accuracy: 0.8739 - val_loss: 0.7300 - val_accuracy: 0.7518\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2718 - accuracy: 0.8958 - val_loss: 0.7392 - val_accuracy: 0.7567\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2780 - accuracy: 0.8769 - val_loss: 0.7471 - val_accuracy: 0.7494\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2646 - accuracy: 0.8903 - val_loss: 0.7558 - val_accuracy: 0.7543\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2539 - accuracy: 0.8836 - val_loss: 0.7643 - val_accuracy: 0.7518\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2564 - accuracy: 0.8934 - val_loss: 0.7728 - val_accuracy: 0.7567\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2751 - accuracy: 0.8781 - val_loss: 0.7820 - val_accuracy: 0.7567\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2417 - accuracy: 0.8970 - val_loss: 0.7907 - val_accuracy: 0.7567\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.2431 - accuracy: 0.8952 - val_loss: 0.8007 - val_accuracy: 0.7591\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2344 - accuracy: 0.9062 - val_loss: 0.8097 - val_accuracy: 0.7567\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2276 - accuracy: 0.9055 - val_loss: 0.8192 - val_accuracy: 0.7518\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2223 - accuracy: 0.9086 - val_loss: 0.8283 - val_accuracy: 0.7543\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2252 - accuracy: 0.9159 - val_loss: 0.8372 - val_accuracy: 0.7494\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2340 - accuracy: 0.9037 - val_loss: 0.8443 - val_accuracy: 0.7518\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2124 - accuracy: 0.9110 - val_loss: 0.8549 - val_accuracy: 0.7518\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2211 - accuracy: 0.9110 - val_loss: 0.8664 - val_accuracy: 0.7567\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2160 - accuracy: 0.9080 - val_loss: 0.8787 - val_accuracy: 0.7567\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2113 - accuracy: 0.9122 - val_loss: 0.8905 - val_accuracy: 0.7567\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2086 - accuracy: 0.9116 - val_loss: 0.8994 - val_accuracy: 0.7543\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1920 - accuracy: 0.9275 - val_loss: 0.9091 - val_accuracy: 0.7518\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2010 - accuracy: 0.9190 - val_loss: 0.9188 - val_accuracy: 0.7494\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1896 - accuracy: 0.9269 - val_loss: 0.9279 - val_accuracy: 0.7470\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2034 - accuracy: 0.9141 - val_loss: 0.9384 - val_accuracy: 0.7591\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1887 - accuracy: 0.9190 - val_loss: 0.9509 - val_accuracy: 0.7567\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1875 - accuracy: 0.9226 - val_loss: 0.9634 - val_accuracy: 0.7543\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1784 - accuracy: 0.9299 - val_loss: 0.9783 - val_accuracy: 0.7567\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1767 - accuracy: 0.9275 - val_loss: 0.9938 - val_accuracy: 0.7567\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1749 - accuracy: 0.9311 - val_loss: 1.0106 - val_accuracy: 0.7567\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1961 - accuracy: 0.9275 - val_loss: 1.0251 - val_accuracy: 0.7470\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1755 - accuracy: 0.9244 - val_loss: 1.0379 - val_accuracy: 0.7518\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1713 - accuracy: 0.9269 - val_loss: 1.0467 - val_accuracy: 0.7470\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1738 - accuracy: 0.9281 - val_loss: 1.0580 - val_accuracy: 0.7518\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1505 - accuracy: 0.9385 - val_loss: 1.0704 - val_accuracy: 0.7518\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1574 - accuracy: 0.9372 - val_loss: 1.0819 - val_accuracy: 0.7518\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1551 - accuracy: 0.9372 - val_loss: 1.0973 - val_accuracy: 0.7591\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1440 - accuracy: 0.9439 - val_loss: 1.1166 - val_accuracy: 0.7616\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1488 - accuracy: 0.9415 - val_loss: 1.1340 - val_accuracy: 0.7640\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1480 - accuracy: 0.9409 - val_loss: 1.1462 - val_accuracy: 0.7640\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1451 - accuracy: 0.9409 - val_loss: 1.1599 - val_accuracy: 0.7640\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1333 - accuracy: 0.9512 - val_loss: 1.1755 - val_accuracy: 0.7591\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1499 - accuracy: 0.9360 - val_loss: 1.1873 - val_accuracy: 0.7640\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1392 - accuracy: 0.9397 - val_loss: 1.2023 - val_accuracy: 0.7591\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1308 - accuracy: 0.9506 - val_loss: 1.2235 - val_accuracy: 0.7640\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1435 - accuracy: 0.9360 - val_loss: 1.2448 - val_accuracy: 0.7591\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1360 - accuracy: 0.9427 - val_loss: 1.2663 - val_accuracy: 0.7640\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1337 - accuracy: 0.9433 - val_loss: 1.2845 - val_accuracy: 0.7640\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1280 - accuracy: 0.9519 - val_loss: 1.3043 - val_accuracy: 0.7591\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1216 - accuracy: 0.9537 - val_loss: 1.3186 - val_accuracy: 0.7567\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1194 - accuracy: 0.9531 - val_loss: 1.3338 - val_accuracy: 0.7591\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1263 - accuracy: 0.9488 - val_loss: 1.3490 - val_accuracy: 0.7543\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1139 - accuracy: 0.9549 - val_loss: 1.3658 - val_accuracy: 0.7518\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1190 - accuracy: 0.9494 - val_loss: 1.3870 - val_accuracy: 0.7591\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1156 - accuracy: 0.9525 - val_loss: 1.3995 - val_accuracy: 0.7616\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1145 - accuracy: 0.9531 - val_loss: 1.4094 - val_accuracy: 0.7640\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1173 - accuracy: 0.9525 - val_loss: 1.4204 - val_accuracy: 0.7640\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1151 - accuracy: 0.9580 - val_loss: 1.4325 - val_accuracy: 0.7518\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1050 - accuracy: 0.9616 - val_loss: 1.4484 - val_accuracy: 0.7567\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1102 - accuracy: 0.9537 - val_loss: 1.4657 - val_accuracy: 0.7567\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1062 - accuracy: 0.9567 - val_loss: 1.4814 - val_accuracy: 0.7543\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1045 - accuracy: 0.9628 - val_loss: 1.4931 - val_accuracy: 0.7543\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1045 - accuracy: 0.9598 - val_loss: 1.5054 - val_accuracy: 0.7567\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1041 - accuracy: 0.9592 - val_loss: 1.5224 - val_accuracy: 0.7591\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1035 - accuracy: 0.9604 - val_loss: 1.5393 - val_accuracy: 0.7591\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1006 - accuracy: 0.9616 - val_loss: 1.5644 - val_accuracy: 0.7567\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0964 - accuracy: 0.9628 - val_loss: 1.5898 - val_accuracy: 0.7591\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0925 - accuracy: 0.9695 - val_loss: 1.6048 - val_accuracy: 0.7543\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0948 - accuracy: 0.9622 - val_loss: 1.6124 - val_accuracy: 0.7616\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0967 - accuracy: 0.9567 - val_loss: 1.6202 - val_accuracy: 0.7616\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0838 - accuracy: 0.9640 - val_loss: 1.6310 - val_accuracy: 0.7567\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0941 - accuracy: 0.9610 - val_loss: 1.6517 - val_accuracy: 0.7640\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0850 - accuracy: 0.9677 - val_loss: 1.6742 - val_accuracy: 0.7640\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0822 - accuracy: 0.9701 - val_loss: 1.6967 - val_accuracy: 0.7664\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0818 - accuracy: 0.9707 - val_loss: 1.7157 - val_accuracy: 0.7689\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0839 - accuracy: 0.9659 - val_loss: 1.7283 - val_accuracy: 0.7664\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0809 - accuracy: 0.9707 - val_loss: 1.7308 - val_accuracy: 0.7737\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0827 - accuracy: 0.9677 - val_loss: 1.7322 - val_accuracy: 0.7689\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0806 - accuracy: 0.9720 - val_loss: 1.7428 - val_accuracy: 0.7591\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0761 - accuracy: 0.9738 - val_loss: 1.7681 - val_accuracy: 0.7664\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0691 - accuracy: 0.9793 - val_loss: 1.7984 - val_accuracy: 0.7664\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0761 - accuracy: 0.9695 - val_loss: 1.8161 - val_accuracy: 0.7689\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0785 - accuracy: 0.9707 - val_loss: 1.8318 - val_accuracy: 0.7737\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0839 - accuracy: 0.9683 - val_loss: 1.8515 - val_accuracy: 0.7689\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0679 - accuracy: 0.9720 - val_loss: 1.8756 - val_accuracy: 0.7689\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0837 - accuracy: 0.9671 - val_loss: 1.8953 - val_accuracy: 0.7616\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0724 - accuracy: 0.9738 - val_loss: 1.9213 - val_accuracy: 0.7689\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0771 - accuracy: 0.9732 - val_loss: 1.9361 - val_accuracy: 0.7713\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0830 - accuracy: 0.9634 - val_loss: 1.9522 - val_accuracy: 0.7762\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0718 - accuracy: 0.9726 - val_loss: 1.9619 - val_accuracy: 0.7737\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0767 - accuracy: 0.9744 - val_loss: 1.9725 - val_accuracy: 0.7689\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0769 - accuracy: 0.9720 - val_loss: 1.9841 - val_accuracy: 0.7713\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0627 - accuracy: 0.9811 - val_loss: 2.0058 - val_accuracy: 0.7640\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0661 - accuracy: 0.9781 - val_loss: 2.0293 - val_accuracy: 0.7616\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0676 - accuracy: 0.9732 - val_loss: 2.0558 - val_accuracy: 0.7543\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0681 - accuracy: 0.9732 - val_loss: 2.0663 - val_accuracy: 0.7591\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0559 - accuracy: 0.9829 - val_loss: 2.0816 - val_accuracy: 0.7640\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.0691 - accuracy: 0.9659 - val_loss: 2.0861 - val_accuracy: 0.7713\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0722 - accuracy: 0.9683 - val_loss: 2.0903 - val_accuracy: 0.7713\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0611 - accuracy: 0.9750 - val_loss: 2.0998 - val_accuracy: 0.7737\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0778 - accuracy: 0.9726 - val_loss: 2.1098 - val_accuracy: 0.7737\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0652 - accuracy: 0.9756 - val_loss: 2.1230 - val_accuracy: 0.7689\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0653 - accuracy: 0.9781 - val_loss: 2.1340 - val_accuracy: 0.7664\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0628 - accuracy: 0.9750 - val_loss: 2.1436 - val_accuracy: 0.7640\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0637 - accuracy: 0.9750 - val_loss: 2.1550 - val_accuracy: 0.7664\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0568 - accuracy: 0.9787 - val_loss: 2.1712 - val_accuracy: 0.7664\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0688 - accuracy: 0.9738 - val_loss: 2.1964 - val_accuracy: 0.7640\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0621 - accuracy: 0.9775 - val_loss: 2.2236 - val_accuracy: 0.7640\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 2.2464 - val_accuracy: 0.7616\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0580 - accuracy: 0.9781 - val_loss: 2.2604 - val_accuracy: 0.7640\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0631 - accuracy: 0.9738 - val_loss: 2.2560 - val_accuracy: 0.7713\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0434 - accuracy: 0.9842 - val_loss: 2.2594 - val_accuracy: 0.7737\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0596 - accuracy: 0.9775 - val_loss: 2.2745 - val_accuracy: 0.7762\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0553 - accuracy: 0.9744 - val_loss: 2.2946 - val_accuracy: 0.7713\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0458 - accuracy: 0.9829 - val_loss: 2.3151 - val_accuracy: 0.7616\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0580 - accuracy: 0.9744 - val_loss: 2.3353 - val_accuracy: 0.7616\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0599 - accuracy: 0.9714 - val_loss: 2.3375 - val_accuracy: 0.7664\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0594 - accuracy: 0.9762 - val_loss: 2.3381 - val_accuracy: 0.7737\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0663 - accuracy: 0.9762 - val_loss: 2.3486 - val_accuracy: 0.7762\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0668 - accuracy: 0.9720 - val_loss: 2.3690 - val_accuracy: 0.7737\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0648 - accuracy: 0.9744 - val_loss: 2.3914 - val_accuracy: 0.7762\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0489 - accuracy: 0.9817 - val_loss: 2.4142 - val_accuracy: 0.7737\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0561 - accuracy: 0.9805 - val_loss: 2.4327 - val_accuracy: 0.7689\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0623 - accuracy: 0.9726 - val_loss: 2.4473 - val_accuracy: 0.7664\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0480 - accuracy: 0.9811 - val_loss: 2.4657 - val_accuracy: 0.7664\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 2.4853 - val_accuracy: 0.7737\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0480 - accuracy: 0.9787 - val_loss: 2.4957 - val_accuracy: 0.7689\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0597 - accuracy: 0.9799 - val_loss: 2.4973 - val_accuracy: 0.7689\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0570 - accuracy: 0.9787 - val_loss: 2.5064 - val_accuracy: 0.7664\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0547 - accuracy: 0.9787 - val_loss: 2.5073 - val_accuracy: 0.7689\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0463 - accuracy: 0.9799 - val_loss: 2.5027 - val_accuracy: 0.7664\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0495 - accuracy: 0.9811 - val_loss: 2.5003 - val_accuracy: 0.7664\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0481 - accuracy: 0.9835 - val_loss: 2.5077 - val_accuracy: 0.7689\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0431 - accuracy: 0.9829 - val_loss: 2.5174 - val_accuracy: 0.7689\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0503 - accuracy: 0.9805 - val_loss: 2.5272 - val_accuracy: 0.7640\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0708 - accuracy: 0.9714 - val_loss: 2.5437 - val_accuracy: 0.7664\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0477 - accuracy: 0.9823 - val_loss: 2.5627 - val_accuracy: 0.7689\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0469 - accuracy: 0.9817 - val_loss: 2.5772 - val_accuracy: 0.7664\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0462 - accuracy: 0.9854 - val_loss: 2.5868 - val_accuracy: 0.7591\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0489 - accuracy: 0.9805 - val_loss: 2.5981 - val_accuracy: 0.7616\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0450 - accuracy: 0.9829 - val_loss: 2.6118 - val_accuracy: 0.7616\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0488 - accuracy: 0.9775 - val_loss: 2.6253 - val_accuracy: 0.7591\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0583 - accuracy: 0.9756 - val_loss: 2.6339 - val_accuracy: 0.7591\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0475 - accuracy: 0.9805 - val_loss: 2.6368 - val_accuracy: 0.7591\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0521 - accuracy: 0.9787 - val_loss: 2.6382 - val_accuracy: 0.7616\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0484 - accuracy: 0.9799 - val_loss: 2.6389 - val_accuracy: 0.7640\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0545 - accuracy: 0.9775 - val_loss: 2.6382 - val_accuracy: 0.7664\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0523 - accuracy: 0.9793 - val_loss: 2.6519 - val_accuracy: 0.7713\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 2.6650 - val_accuracy: 0.7664\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0438 - accuracy: 0.9872 - val_loss: 2.6793 - val_accuracy: 0.7689\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0457 - accuracy: 0.9860 - val_loss: 2.6986 - val_accuracy: 0.7664\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0413 - accuracy: 0.9835 - val_loss: 2.7152 - val_accuracy: 0.7664\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0463 - accuracy: 0.9805 - val_loss: 2.7231 - val_accuracy: 0.7664\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0464 - accuracy: 0.9799 - val_loss: 2.7293 - val_accuracy: 0.7689\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 2.7416 - val_accuracy: 0.7640\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0534 - accuracy: 0.9805 - val_loss: 2.7508 - val_accuracy: 0.7616\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0451 - accuracy: 0.9829 - val_loss: 2.7579 - val_accuracy: 0.7591\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0456 - accuracy: 0.9823 - val_loss: 2.7694 - val_accuracy: 0.7640\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0435 - accuracy: 0.9805 - val_loss: 2.7749 - val_accuracy: 0.7591\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0365 - accuracy: 0.9884 - val_loss: 2.7794 - val_accuracy: 0.7616\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0461 - accuracy: 0.9817 - val_loss: 2.7756 - val_accuracy: 0.7664\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0467 - accuracy: 0.9817 - val_loss: 2.7641 - val_accuracy: 0.7640\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0392 - accuracy: 0.9829 - val_loss: 2.7697 - val_accuracy: 0.7689\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0403 - accuracy: 0.9835 - val_loss: 2.7804 - val_accuracy: 0.7689\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0382 - accuracy: 0.9848 - val_loss: 2.8054 - val_accuracy: 0.7737\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0448 - accuracy: 0.9811 - val_loss: 2.8296 - val_accuracy: 0.7713\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0478 - accuracy: 0.9811 - val_loss: 2.8479 - val_accuracy: 0.7640\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0429 - accuracy: 0.9848 - val_loss: 2.8523 - val_accuracy: 0.7664\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0399 - accuracy: 0.9854 - val_loss: 2.8473 - val_accuracy: 0.7664\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0477 - accuracy: 0.9799 - val_loss: 2.8420 - val_accuracy: 0.7616\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0433 - accuracy: 0.9823 - val_loss: 2.8341 - val_accuracy: 0.7591\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0439 - accuracy: 0.9823 - val_loss: 2.8416 - val_accuracy: 0.7518\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0480 - accuracy: 0.9829 - val_loss: 2.8567 - val_accuracy: 0.7543\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0405 - accuracy: 0.9842 - val_loss: 2.8928 - val_accuracy: 0.7494\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0398 - accuracy: 0.9842 - val_loss: 2.9169 - val_accuracy: 0.7567\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0479 - accuracy: 0.9805 - val_loss: 2.9280 - val_accuracy: 0.7664\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0425 - accuracy: 0.9823 - val_loss: 2.9236 - val_accuracy: 0.7591\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0378 - accuracy: 0.9854 - val_loss: 2.9212 - val_accuracy: 0.7616\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0417 - accuracy: 0.9829 - val_loss: 2.9207 - val_accuracy: 0.7664\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0460 - accuracy: 0.9823 - val_loss: 2.9098 - val_accuracy: 0.7664\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0437 - accuracy: 0.9805 - val_loss: 2.9102 - val_accuracy: 0.7664\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0360 - accuracy: 0.9860 - val_loss: 2.9180 - val_accuracy: 0.7689\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.0411 - accuracy: 0.9842 - val_loss: 2.9323 - val_accuracy: 0.7640\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0462 - accuracy: 0.9848 - val_loss: 2.9545 - val_accuracy: 0.7664\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0400 - accuracy: 0.9842 - val_loss: 2.9712 - val_accuracy: 0.7713\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0418 - accuracy: 0.9835 - val_loss: 2.9818 - val_accuracy: 0.7737\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0432 - accuracy: 0.9817 - val_loss: 2.9936 - val_accuracy: 0.7713\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0395 - accuracy: 0.9823 - val_loss: 3.0030 - val_accuracy: 0.7664\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0376 - accuracy: 0.9842 - val_loss: 3.0060 - val_accuracy: 0.7664\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0347 - accuracy: 0.9860 - val_loss: 3.0096 - val_accuracy: 0.7640\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0420 - accuracy: 0.9805 - val_loss: 3.0224 - val_accuracy: 0.7591\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0492 - accuracy: 0.9799 - val_loss: 3.0322 - val_accuracy: 0.7591\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0440 - accuracy: 0.9823 - val_loss: 3.0535 - val_accuracy: 0.7591\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0451 - accuracy: 0.9799 - val_loss: 3.0849 - val_accuracy: 0.7591\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0375 - accuracy: 0.9860 - val_loss: 3.1209 - val_accuracy: 0.7616\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0341 - accuracy: 0.9854 - val_loss: 3.1557 - val_accuracy: 0.7543\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0310 - accuracy: 0.9884 - val_loss: 3.1831 - val_accuracy: 0.7567\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0383 - accuracy: 0.9854 - val_loss: 3.1937 - val_accuracy: 0.7664\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0399 - accuracy: 0.9811 - val_loss: 3.1964 - val_accuracy: 0.7689\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0351 - accuracy: 0.9860 - val_loss: 3.1964 - val_accuracy: 0.7689\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0465 - accuracy: 0.9811 - val_loss: 3.1990 - val_accuracy: 0.7664\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0374 - accuracy: 0.9848 - val_loss: 3.2080 - val_accuracy: 0.7616\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0370 - accuracy: 0.9878 - val_loss: 3.2126 - val_accuracy: 0.7591\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0399 - accuracy: 0.9823 - val_loss: 3.2202 - val_accuracy: 0.7664\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0387 - accuracy: 0.9842 - val_loss: 3.2239 - val_accuracy: 0.7689\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0400 - accuracy: 0.9842 - val_loss: 3.2382 - val_accuracy: 0.7713\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0421 - accuracy: 0.9805 - val_loss: 3.2505 - val_accuracy: 0.7713\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0320 - accuracy: 0.9866 - val_loss: 3.2710 - val_accuracy: 0.7689\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0399 - accuracy: 0.9860 - val_loss: 3.2945 - val_accuracy: 0.7664\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0362 - accuracy: 0.9805 - val_loss: 3.3082 - val_accuracy: 0.7640\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0308 - accuracy: 0.9896 - val_loss: 3.3182 - val_accuracy: 0.7616\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0366 - accuracy: 0.9842 - val_loss: 3.3257 - val_accuracy: 0.7616\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0368 - accuracy: 0.9848 - val_loss: 3.3237 - val_accuracy: 0.7591\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0331 - accuracy: 0.9860 - val_loss: 3.3262 - val_accuracy: 0.7640\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0365 - accuracy: 0.9866 - val_loss: 3.3366 - val_accuracy: 0.7591\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0359 - accuracy: 0.9835 - val_loss: 3.3528 - val_accuracy: 0.7616\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0377 - accuracy: 0.9823 - val_loss: 3.3615 - val_accuracy: 0.7640\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0396 - accuracy: 0.9835 - val_loss: 3.3630 - val_accuracy: 0.7664\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0365 - accuracy: 0.9835 - val_loss: 3.3715 - val_accuracy: 0.7664\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0432 - accuracy: 0.9842 - val_loss: 3.3764 - val_accuracy: 0.7689\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0362 - accuracy: 0.9854 - val_loss: 3.3804 - val_accuracy: 0.7713\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0385 - accuracy: 0.9854 - val_loss: 3.3905 - val_accuracy: 0.7640\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0378 - accuracy: 0.9842 - val_loss: 3.4057 - val_accuracy: 0.7640\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0304 - accuracy: 0.9872 - val_loss: 3.4249 - val_accuracy: 0.7640\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0429 - accuracy: 0.9823 - val_loss: 3.4414 - val_accuracy: 0.7664\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0435 - accuracy: 0.9805 - val_loss: 3.4458 - val_accuracy: 0.7689\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0309 - accuracy: 0.9854 - val_loss: 3.4483 - val_accuracy: 0.7664\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0387 - accuracy: 0.9835 - val_loss: 3.4566 - val_accuracy: 0.7616\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0305 - accuracy: 0.9872 - val_loss: 3.4635 - val_accuracy: 0.7640\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0297 - accuracy: 0.9860 - val_loss: 3.4734 - val_accuracy: 0.7640\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0340 - accuracy: 0.9884 - val_loss: 3.4841 - val_accuracy: 0.7640\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0295 - accuracy: 0.9866 - val_loss: 3.4966 - val_accuracy: 0.7616\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0356 - accuracy: 0.9842 - val_loss: 3.5113 - val_accuracy: 0.7616\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0378 - accuracy: 0.9823 - val_loss: 3.5245 - val_accuracy: 0.7567\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0365 - accuracy: 0.9793 - val_loss: 3.5491 - val_accuracy: 0.7567\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0429 - accuracy: 0.9811 - val_loss: 3.5587 - val_accuracy: 0.7567\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0330 - accuracy: 0.9835 - val_loss: 3.5645 - val_accuracy: 0.7567\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0387 - accuracy: 0.9842 - val_loss: 3.5605 - val_accuracy: 0.7543\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0312 - accuracy: 0.9878 - val_loss: 3.5572 - val_accuracy: 0.7543\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0356 - accuracy: 0.9848 - val_loss: 3.5691 - val_accuracy: 0.7567\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0308 - accuracy: 0.9860 - val_loss: 3.5872 - val_accuracy: 0.7567\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0319 - accuracy: 0.9854 - val_loss: 3.6000 - val_accuracy: 0.7567\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0385 - accuracy: 0.9823 - val_loss: 3.6141 - val_accuracy: 0.7567\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0410 - accuracy: 0.9817 - val_loss: 3.6144 - val_accuracy: 0.7543\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0350 - accuracy: 0.9829 - val_loss: 3.6151 - val_accuracy: 0.7518\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0304 - accuracy: 0.9848 - val_loss: 3.6155 - val_accuracy: 0.7494\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0300 - accuracy: 0.9878 - val_loss: 3.6167 - val_accuracy: 0.7518\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0381 - accuracy: 0.9823 - val_loss: 3.6129 - val_accuracy: 0.7518\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0347 - accuracy: 0.9842 - val_loss: 3.6101 - val_accuracy: 0.7494\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0386 - accuracy: 0.9848 - val_loss: 3.6091 - val_accuracy: 0.7543\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0338 - accuracy: 0.9878 - val_loss: 3.6101 - val_accuracy: 0.7543\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0428 - accuracy: 0.9823 - val_loss: 3.6057 - val_accuracy: 0.7567\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0328 - accuracy: 0.9866 - val_loss: 3.6014 - val_accuracy: 0.7664\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0344 - accuracy: 0.9842 - val_loss: 3.5960 - val_accuracy: 0.7713\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0342 - accuracy: 0.9860 - val_loss: 3.5902 - val_accuracy: 0.7713\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0378 - accuracy: 0.9842 - val_loss: 3.5963 - val_accuracy: 0.7737\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0295 - accuracy: 0.9872 - val_loss: 3.6040 - val_accuracy: 0.7737\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0392 - accuracy: 0.9829 - val_loss: 3.6045 - val_accuracy: 0.7737\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0353 - accuracy: 0.9866 - val_loss: 3.5950 - val_accuracy: 0.7737\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0324 - accuracy: 0.9842 - val_loss: 3.5915 - val_accuracy: 0.7713\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0306 - accuracy: 0.9866 - val_loss: 3.5963 - val_accuracy: 0.7713\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0324 - accuracy: 0.9842 - val_loss: 3.5919 - val_accuracy: 0.7713\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0283 - accuracy: 0.9896 - val_loss: 3.5952 - val_accuracy: 0.7640\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0383 - accuracy: 0.9854 - val_loss: 3.6028 - val_accuracy: 0.7591\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0306 - accuracy: 0.9860 - val_loss: 3.6235 - val_accuracy: 0.7616\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0315 - accuracy: 0.9854 - val_loss: 3.6452 - val_accuracy: 0.7616\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0431 - accuracy: 0.9866 - val_loss: 3.6637 - val_accuracy: 0.7591\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0321 - accuracy: 0.9842 - val_loss: 3.6783 - val_accuracy: 0.7616\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0288 - accuracy: 0.9872 - val_loss: 3.6894 - val_accuracy: 0.7640\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0355 - accuracy: 0.9860 - val_loss: 3.7004 - val_accuracy: 0.7640\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0319 - accuracy: 0.9860 - val_loss: 3.7098 - val_accuracy: 0.7640\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0293 - accuracy: 0.9896 - val_loss: 3.7338 - val_accuracy: 0.7616\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0273 - accuracy: 0.9878 - val_loss: 3.7631 - val_accuracy: 0.7640\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0439 - accuracy: 0.9860 - val_loss: 3.8113 - val_accuracy: 0.7664\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0363 - accuracy: 0.9860 - val_loss: 3.8668 - val_accuracy: 0.7689\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0402 - accuracy: 0.9848 - val_loss: 3.9214 - val_accuracy: 0.7664\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0432 - accuracy: 0.9805 - val_loss: 3.9447 - val_accuracy: 0.7616\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0336 - accuracy: 0.9860 - val_loss: 3.9458 - val_accuracy: 0.7640\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0364 - accuracy: 0.9842 - val_loss: 3.9422 - val_accuracy: 0.7616\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0351 - accuracy: 0.9884 - val_loss: 3.9364 - val_accuracy: 0.7640\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0288 - accuracy: 0.9878 - val_loss: 3.9293 - val_accuracy: 0.7737\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0369 - accuracy: 0.9842 - val_loss: 3.9353 - val_accuracy: 0.7786\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0263 - accuracy: 0.9890 - val_loss: 3.9424 - val_accuracy: 0.7786\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0309 - accuracy: 0.9842 - val_loss: 3.9504 - val_accuracy: 0.7713\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0356 - accuracy: 0.9842 - val_loss: 3.9618 - val_accuracy: 0.7762\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0387 - accuracy: 0.9854 - val_loss: 3.9723 - val_accuracy: 0.7786\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0397 - accuracy: 0.9817 - val_loss: 3.9801 - val_accuracy: 0.7737\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0383 - accuracy: 0.9878 - val_loss: 3.9954 - val_accuracy: 0.7689\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0330 - accuracy: 0.9860 - val_loss: 3.9995 - val_accuracy: 0.7713\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0285 - accuracy: 0.9884 - val_loss: 4.0071 - val_accuracy: 0.7713\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0320 - accuracy: 0.9854 - val_loss: 4.0180 - val_accuracy: 0.7664\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0361 - accuracy: 0.9829 - val_loss: 4.0218 - val_accuracy: 0.7640\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0361 - accuracy: 0.9835 - val_loss: 4.0208 - val_accuracy: 0.7689\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0331 - accuracy: 0.9848 - val_loss: 4.0224 - val_accuracy: 0.7664\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0372 - accuracy: 0.9811 - val_loss: 4.0135 - val_accuracy: 0.7640\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0265 - accuracy: 0.9896 - val_loss: 4.0102 - val_accuracy: 0.7664\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0291 - accuracy: 0.9896 - val_loss: 4.0073 - val_accuracy: 0.7713\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0389 - accuracy: 0.9854 - val_loss: 4.0086 - val_accuracy: 0.7640\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0331 - accuracy: 0.9835 - val_loss: 4.0135 - val_accuracy: 0.7616\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0288 - accuracy: 0.9866 - val_loss: 4.0179 - val_accuracy: 0.7616\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0385 - accuracy: 0.9811 - val_loss: 4.0171 - val_accuracy: 0.7640\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0296 - accuracy: 0.9860 - val_loss: 4.0203 - val_accuracy: 0.7591\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0340 - accuracy: 0.9866 - val_loss: 4.0262 - val_accuracy: 0.7591\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0276 - accuracy: 0.9866 - val_loss: 4.0355 - val_accuracy: 0.7591\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0311 - accuracy: 0.9866 - val_loss: 4.0502 - val_accuracy: 0.7640\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0328 - accuracy: 0.9848 - val_loss: 4.0649 - val_accuracy: 0.7616\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0307 - accuracy: 0.9872 - val_loss: 4.0827 - val_accuracy: 0.7640\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0278 - accuracy: 0.9921 - val_loss: 4.1023 - val_accuracy: 0.7591\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0293 - accuracy: 0.9860 - val_loss: 4.1227 - val_accuracy: 0.7567\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0351 - accuracy: 0.9866 - val_loss: 4.1515 - val_accuracy: 0.7567\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0280 - accuracy: 0.9872 - val_loss: 4.1760 - val_accuracy: 0.7567\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0248 - accuracy: 0.9890 - val_loss: 4.2063 - val_accuracy: 0.7543\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0319 - accuracy: 0.9848 - val_loss: 4.2206 - val_accuracy: 0.7543\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0268 - accuracy: 0.9860 - val_loss: 4.2314 - val_accuracy: 0.7543\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0268 - accuracy: 0.9878 - val_loss: 4.2291 - val_accuracy: 0.7640\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0314 - accuracy: 0.9854 - val_loss: 4.2305 - val_accuracy: 0.7616\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 4.2309 - val_accuracy: 0.7591\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0357 - accuracy: 0.9854 - val_loss: 4.2445 - val_accuracy: 0.7591\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0312 - accuracy: 0.9866 - val_loss: 4.2620 - val_accuracy: 0.7616\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0324 - accuracy: 0.9817 - val_loss: 4.2918 - val_accuracy: 0.7591\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0369 - accuracy: 0.9848 - val_loss: 4.3305 - val_accuracy: 0.7616\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0311 - accuracy: 0.9866 - val_loss: 4.3467 - val_accuracy: 0.7591\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0266 - accuracy: 0.9878 - val_loss: 4.3534 - val_accuracy: 0.7591\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0243 - accuracy: 0.9902 - val_loss: 4.3640 - val_accuracy: 0.7543\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0345 - accuracy: 0.9842 - val_loss: 4.3743 - val_accuracy: 0.7543\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0283 - accuracy: 0.9878 - val_loss: 4.3853 - val_accuracy: 0.7567\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0350 - accuracy: 0.9854 - val_loss: 4.3905 - val_accuracy: 0.7494\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0318 - accuracy: 0.9842 - val_loss: 4.3867 - val_accuracy: 0.7494\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0292 - accuracy: 0.9872 - val_loss: 4.3855 - val_accuracy: 0.7494\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0291 - accuracy: 0.9866 - val_loss: 4.3980 - val_accuracy: 0.7494\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0293 - accuracy: 0.9872 - val_loss: 4.4117 - val_accuracy: 0.7445\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0334 - accuracy: 0.9854 - val_loss: 4.4262 - val_accuracy: 0.7445\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0306 - accuracy: 0.9866 - val_loss: 4.4354 - val_accuracy: 0.7518\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0306 - accuracy: 0.9848 - val_loss: 4.4387 - val_accuracy: 0.7494\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0306 - accuracy: 0.9878 - val_loss: 4.4423 - val_accuracy: 0.7494\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0304 - accuracy: 0.9854 - val_loss: 4.4481 - val_accuracy: 0.7470\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0249 - accuracy: 0.9896 - val_loss: 4.4567 - val_accuracy: 0.7518\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0381 - accuracy: 0.9805 - val_loss: 4.4741 - val_accuracy: 0.7494\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0319 - accuracy: 0.9890 - val_loss: 4.4840 - val_accuracy: 0.7591\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0349 - accuracy: 0.9848 - val_loss: 4.4851 - val_accuracy: 0.7640\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0402 - accuracy: 0.9884 - val_loss: 4.4887 - val_accuracy: 0.7616\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0323 - accuracy: 0.9842 - val_loss: 4.4941 - val_accuracy: 0.7567\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0264 - accuracy: 0.9884 - val_loss: 4.5055 - val_accuracy: 0.7616\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0270 - accuracy: 0.9866 - val_loss: 4.5109 - val_accuracy: 0.7616\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0245 - accuracy: 0.9884 - val_loss: 4.5149 - val_accuracy: 0.7640\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0275 - accuracy: 0.9884 - val_loss: 4.5200 - val_accuracy: 0.7591\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0289 - accuracy: 0.9890 - val_loss: 4.5177 - val_accuracy: 0.7567\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0254 - accuracy: 0.9866 - val_loss: 4.5100 - val_accuracy: 0.7543\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0306 - accuracy: 0.9835 - val_loss: 4.5046 - val_accuracy: 0.7543\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0320 - accuracy: 0.9866 - val_loss: 4.4974 - val_accuracy: 0.7567\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0294 - accuracy: 0.9878 - val_loss: 4.4849 - val_accuracy: 0.7616\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0351 - accuracy: 0.9823 - val_loss: 4.4796 - val_accuracy: 0.7616\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0278 - accuracy: 0.9890 - val_loss: 4.4782 - val_accuracy: 0.7616\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0346 - accuracy: 0.9854 - val_loss: 4.4744 - val_accuracy: 0.7640\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0258 - accuracy: 0.9866 - val_loss: 4.4732 - val_accuracy: 0.7616\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0280 - accuracy: 0.9866 - val_loss: 4.4730 - val_accuracy: 0.7640\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0317 - accuracy: 0.9860 - val_loss: 4.4728 - val_accuracy: 0.7713\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0287 - accuracy: 0.9854 - val_loss: 4.4873 - val_accuracy: 0.7664\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0250 - accuracy: 0.9878 - val_loss: 4.4990 - val_accuracy: 0.7640\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0261 - accuracy: 0.9872 - val_loss: 4.5107 - val_accuracy: 0.7616\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0367 - accuracy: 0.9860 - val_loss: 4.5027 - val_accuracy: 0.7640\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0301 - accuracy: 0.9835 - val_loss: 4.5045 - val_accuracy: 0.7591\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0284 - accuracy: 0.9848 - val_loss: 4.5174 - val_accuracy: 0.7518\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0285 - accuracy: 0.9866 - val_loss: 4.5365 - val_accuracy: 0.7518\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0290 - accuracy: 0.9872 - val_loss: 4.5584 - val_accuracy: 0.7494\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0319 - accuracy: 0.9842 - val_loss: 4.5830 - val_accuracy: 0.7494\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0282 - accuracy: 0.9848 - val_loss: 4.6022 - val_accuracy: 0.7494\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0275 - accuracy: 0.9872 - val_loss: 4.6192 - val_accuracy: 0.7494\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0361 - accuracy: 0.9811 - val_loss: 4.6229 - val_accuracy: 0.7494\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0343 - accuracy: 0.9842 - val_loss: 4.6105 - val_accuracy: 0.7543\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0357 - accuracy: 0.9811 - val_loss: 4.5995 - val_accuracy: 0.7591\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0295 - accuracy: 0.9890 - val_loss: 4.6038 - val_accuracy: 0.7591\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0428 - accuracy: 0.9805 - val_loss: 4.6278 - val_accuracy: 0.7616\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0305 - accuracy: 0.9878 - val_loss: 4.6615 - val_accuracy: 0.7591\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0244 - accuracy: 0.9872 - val_loss: 4.6952 - val_accuracy: 0.7567\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0288 - accuracy: 0.9884 - val_loss: 4.7211 - val_accuracy: 0.7543\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 4.7461 - val_accuracy: 0.7518\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 0.0305 - accuracy: 0.9878 - val_loss: 4.7558 - val_accuracy: 0.7470\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 4.7537 - val_accuracy: 0.7567\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0457 - accuracy: 0.9829 - val_loss: 4.7744 - val_accuracy: 0.7518\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0309 - accuracy: 0.9884 - val_loss: 4.8119 - val_accuracy: 0.7518\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0360 - accuracy: 0.9848 - val_loss: 4.8561 - val_accuracy: 0.7470\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0296 - accuracy: 0.9866 - val_loss: 4.8824 - val_accuracy: 0.7470\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0375 - accuracy: 0.9842 - val_loss: 4.8807 - val_accuracy: 0.7470\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0289 - accuracy: 0.9878 - val_loss: 4.8645 - val_accuracy: 0.7567\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0234 - accuracy: 0.9890 - val_loss: 4.8468 - val_accuracy: 0.7543\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0355 - accuracy: 0.9872 - val_loss: 4.8312 - val_accuracy: 0.7470\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0280 - accuracy: 0.9848 - val_loss: 4.8294 - val_accuracy: 0.7445\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0532 - accuracy: 0.9835 - val_loss: 4.8462 - val_accuracy: 0.7445\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0303 - accuracy: 0.9878 - val_loss: 4.8712 - val_accuracy: 0.7494\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0279 - accuracy: 0.9866 - val_loss: 4.8930 - val_accuracy: 0.7470\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0417 - accuracy: 0.9823 - val_loss: 4.9074 - val_accuracy: 0.7421\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0339 - accuracy: 0.9860 - val_loss: 4.9267 - val_accuracy: 0.7518\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0300 - accuracy: 0.9848 - val_loss: 4.9445 - val_accuracy: 0.7518\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0398 - accuracy: 0.9799 - val_loss: 4.9293 - val_accuracy: 0.7494\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0307 - accuracy: 0.9860 - val_loss: 4.9234 - val_accuracy: 0.7518\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0363 - accuracy: 0.9854 - val_loss: 4.8949 - val_accuracy: 0.7518\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0309 - accuracy: 0.9866 - val_loss: 4.8626 - val_accuracy: 0.7494\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0307 - accuracy: 0.9854 - val_loss: 4.8406 - val_accuracy: 0.7470\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0322 - accuracy: 0.9854 - val_loss: 4.8319 - val_accuracy: 0.7518\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0344 - accuracy: 0.9872 - val_loss: 4.8468 - val_accuracy: 0.7494\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 4.8592 - val_accuracy: 0.7518\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0366 - accuracy: 0.9805 - val_loss: 4.8301 - val_accuracy: 0.7567\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0301 - accuracy: 0.9884 - val_loss: 4.8104 - val_accuracy: 0.7567\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0345 - accuracy: 0.9854 - val_loss: 4.7797 - val_accuracy: 0.7591\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0362 - accuracy: 0.9829 - val_loss: 4.7696 - val_accuracy: 0.7640\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0415 - accuracy: 0.9781 - val_loss: 4.7773 - val_accuracy: 0.7640\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0391 - accuracy: 0.9823 - val_loss: 4.8203 - val_accuracy: 0.7616\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0310 - accuracy: 0.9860 - val_loss: 4.8517 - val_accuracy: 0.7567\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0371 - accuracy: 0.9842 - val_loss: 4.8690 - val_accuracy: 0.7591\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0357 - accuracy: 0.9817 - val_loss: 4.8661 - val_accuracy: 0.7591\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0323 - accuracy: 0.9866 - val_loss: 4.8611 - val_accuracy: 0.7616\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0312 - accuracy: 0.9878 - val_loss: 4.8598 - val_accuracy: 0.7591\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0315 - accuracy: 0.9829 - val_loss: 4.8684 - val_accuracy: 0.7640\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0386 - accuracy: 0.9787 - val_loss: 4.8964 - val_accuracy: 0.7616\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0337 - accuracy: 0.9823 - val_loss: 4.9301 - val_accuracy: 0.7567\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0352 - accuracy: 0.9866 - val_loss: 4.9554 - val_accuracy: 0.7567\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0352 - accuracy: 0.9811 - val_loss: 4.9571 - val_accuracy: 0.7591\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0334 - accuracy: 0.9866 - val_loss: 4.9438 - val_accuracy: 0.7640\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0311 - accuracy: 0.9860 - val_loss: 4.9207 - val_accuracy: 0.7689\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0334 - accuracy: 0.9842 - val_loss: 4.9149 - val_accuracy: 0.7689\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0320 - accuracy: 0.9848 - val_loss: 4.9366 - val_accuracy: 0.7640\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0325 - accuracy: 0.9872 - val_loss: 4.9600 - val_accuracy: 0.7591\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0292 - accuracy: 0.9878 - val_loss: 4.9957 - val_accuracy: 0.7591\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0340 - accuracy: 0.9854 - val_loss: 5.0205 - val_accuracy: 0.7567\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0298 - accuracy: 0.9866 - val_loss: 5.0379 - val_accuracy: 0.7591\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0333 - accuracy: 0.9860 - val_loss: 5.0296 - val_accuracy: 0.7591\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0351 - accuracy: 0.9860 - val_loss: 5.0059 - val_accuracy: 0.7616\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0381 - accuracy: 0.9817 - val_loss: 4.9963 - val_accuracy: 0.7616\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0341 - accuracy: 0.9829 - val_loss: 4.9912 - val_accuracy: 0.7616\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0378 - accuracy: 0.9811 - val_loss: 4.9669 - val_accuracy: 0.7616\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0445 - accuracy: 0.9799 - val_loss: 4.9559 - val_accuracy: 0.7591\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0282 - accuracy: 0.9860 - val_loss: 4.9547 - val_accuracy: 0.7470\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0271 - accuracy: 0.9878 - val_loss: 4.9603 - val_accuracy: 0.7445\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0311 - accuracy: 0.9872 - val_loss: 4.9730 - val_accuracy: 0.7567\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 5 (Combined):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.826848\n",
      "1  Precision  0.754491\n",
      "2     Recall  0.724138\n",
      "3   F1 Score  0.739003\n",
      "4    ROC AUC  0.860979\n",
      "\n",
      "Confusion Matrix for Method 5 (Combined):\n",
      " [[299  41]\n",
      " [ 48 126]]\n",
      "Loading BioBERT model and tokenizer...\n",
      "Predicting for sequence: GLWSKIKEVGKEAAKAAAKAAG\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "\n",
      "Predictions using Combined technique:\n",
      "Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load DataFrame\n",
    "display(allDataDF2)\n",
    "\n",
    "# Process data\n",
    "X1 = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1).values\n",
    "t = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1)\n",
    "\n",
    "display(t)\n",
    "y1 = allDataDF2['Value'].values\n",
    "\n",
    "# Reshape encoded columns\n",
    "X2 = np.array(list(allDataDF2['EncodedX'])).reshape((len(allDataDF2), -1))\n",
    "X3 = np.array(list(allDataDF2['vector'])).reshape((len(allDataDF2), -1))\n",
    "X4 = np.array(list(allDataDF2['Bert'])).reshape((len(allDataDF2), -1))\n",
    "\n",
    "# Concatenate arrays\n",
    "X = np.concatenate((X1, X2, X4, X3), axis=1)\n",
    "x11 = pd.DataFrame(X)\n",
    "display(x11)\n",
    "\n",
    "# Split data\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# Display shapes to verify\n",
    "print(X1.shape, X2.shape, X3.shape, X4.shape, X.shape)\n",
    "\n",
    "# Scale data\n",
    "scaler2 = StandardScaler()\n",
    "X4_train = scaler2.fit_transform(X4_train)\n",
    "X4_test = scaler2.transform(X4_test)\n",
    "\n",
    "# Define neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train neural network model\n",
    "input_dim = X4_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "nn_model.fit(X4_train, y4_train, epochs=500, batch_size=2000, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = nn_model.predict(X4_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method5 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 5 (Combined):\\n\", performance_metrics_method5)\n",
    "print(\"\\nConfusion Matrix for Method 5 (Combined):\\n\", conf_matrix)\n",
    "\n",
    "# Train Word2Vec model\n",
    "pos_sequences = posDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "neg_sequences = negDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "w2v_model = Word2Vec(sentences=all_sequences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate sequence vectors\n",
    "def sequence_vector(seq, model):\n",
    "    return np.mean([model.wv[char] for char in seq if char in model.wv], axis=0)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embeddings1(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_method3(sequence, nn_model, scaler, model, tokenizer, w2v_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    properties = calculate_all_properties(sequence)\n",
    "   \n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    \n",
    "    \n",
    "    sequence1 = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence1]\n",
    "    onehot_encoded = np.ravel([[0 if i != val else 1 for i in range(len(char_to_int))] for val in integer_encoded])\n",
    "    \n",
    "    \n",
    "    embedding = get_bert_embeddings1(sequence, tokenizer, model)\n",
    "    \n",
    "    \n",
    "    sequence = sequence.upper()\n",
    "    vector = sequence_vector(list(sequence), w2v_model)\n",
    "    \n",
    "    \n",
    "    total = np.concatenate((vector, embedding.flatten(), prop_values.flatten(), onehot_encoded))\n",
    "    total = total.reshape(1, -1)\n",
    "    \n",
    "    prediction = nn_model.predict(total)\n",
    "    result = 'Antimicrobial' if prediction[0] >= 0.5 else 'Non-antimicrobial'\n",
    "    \n",
    "    return result, embedding\n",
    "\n",
    "# Prediction\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "predictions, embed = predict_method3(sequence, nn_model, scaler, bert_model, tokenizer, w2v_model)\n",
    "print(\"\\nPredictions using Combined technique:\")\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de690b6-ff7e-4dbd-a517-c12a9ce4130a",
   "metadata": {},
   "source": [
    "**Physico&oneHOT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f77fce-a450-4681-8c34-62ef955863c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.09701026, 0.023356073, 0.19239551, 0.34847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.10609158, 0.031600513, 0.15865144, 0.31917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.107020475, 0.032668713, 0.16226107, 0.3235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.1107386, 0.036594443, 0.15842809, 0.322235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.10410814, 0.046210337, 0.17820424, 0.33955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11710616, 0.032537665, 0.117993765, 0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.1260696, -0.010238703, 0.0796683, 0.233125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.16503033, -0.014033677, -0.032334927, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.145197, -0.018769035, 0.019576171, 0.18281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.109951705, 0.013725347, 0.11754644, 0.2610...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.09701026, 0.023356073, 0.19239551, 0.34847...  \n",
       "SP_2643    [-0.10609158, 0.031600513, 0.15865144, 0.31917...  \n",
       "SP_2644    [-0.107020475, 0.032668713, 0.16226107, 0.3235...  \n",
       "SP_2645    [-0.1107386, 0.036594443, 0.15842809, 0.322235...  \n",
       "SP_2646    [-0.10410814, 0.046210337, 0.17820424, 0.33955...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11710616, 0.032537665, 0.117993765, 0.2792...  \n",
       "SP_2853    [-0.1260696, -0.010238703, 0.0796683, 0.233125...  \n",
       "SP_2854    [-0.16503033, -0.014033677, -0.032334927, 0.12...  \n",
       "SP_2855    [-0.145197, -0.018769035, 0.019576171, 0.18281...  \n",
       "SP_Q09022  [-0.109951705, 0.013725347, 0.11754644, 0.2610...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \n",
       "DADP ID                                                                   \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319  \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875  \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542  \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154  \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037  \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541  \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362  \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276  \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679  \n",
       "\n",
       "[2566 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1         2          3         4         5         6    \\\n",
       "0     3181.6835   9.701025  0.030303   7.272727  0.196970  2.762539  3181.736   \n",
       "1     2279.6794   9.703153  0.043478 -12.986957  0.630435  1.845400  2279.719   \n",
       "2     2293.7060   9.703153  0.043478  -4.613043  0.630435  1.847089  2293.746   \n",
       "3     2266.6807   9.703153  0.043478  -4.613043  0.747826  1.847089  2266.720   \n",
       "4     1583.9103   8.750052  0.000000  10.800000  1.275000  0.759103  1583.937   \n",
       "...         ...        ...       ...        ...       ...       ...       ...   \n",
       "2561  1405.7470  10.002350  0.076923  16.907692  1.000000  1.731990  1405.774   \n",
       "2562  2124.3526   4.651158  0.166667  37.416667 -0.455556 -2.147981  2124.388   \n",
       "2563  2138.4670   8.045284  0.150000  72.685000 -0.230000  0.739412  2138.506   \n",
       "2564  2076.4408   9.820679  0.117647  50.894118 -0.911765  2.732905  2076.477   \n",
       "2565  7235.6096   8.884082  0.015152  44.725758 -0.174242  4.685244  7235.743   \n",
       "\n",
       "           7        8      9    ...  515  516  517  518  519  520  521  522  \\\n",
       "0    -13.16030  1318.79   46.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1     -7.85050   910.88   31.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2     -7.46040   910.88   31.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3     -7.34350   888.02   31.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4     -5.33460   629.62   22.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "...        ...      ...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2561  -2.94950   527.88   18.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2562  -2.70150   801.94   26.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2563  -6.72983   758.18   26.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2564  -4.87636   848.49   30.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "2565 -36.35396  2988.65  111.0  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      523  524  \n",
       "0     0.0  0.0  \n",
       "1     0.0  0.0  \n",
       "2     0.0  0.0  \n",
       "3     0.0  0.0  \n",
       "4     0.0  0.0  \n",
       "...   ...  ...  \n",
       "2561  0.0  0.0  \n",
       "2562  0.0  0.0  \n",
       "2563  0.0  0.0  \n",
       "2564  0.0  0.0  \n",
       "2565  0.0  0.0  \n",
       "\n",
       "[2566 rows x 525 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8530 - accuracy: 0.5990 - val_loss: 0.6427 - val_accuracy: 0.6448\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7734 - accuracy: 0.5990 - val_loss: 0.6388 - val_accuracy: 0.6448\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7408 - accuracy: 0.5868 - val_loss: 0.6425 - val_accuracy: 0.6350\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6867 - accuracy: 0.5929 - val_loss: 0.6472 - val_accuracy: 0.6180\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6780 - accuracy: 0.6009 - val_loss: 0.6504 - val_accuracy: 0.5937\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6763 - accuracy: 0.5753 - val_loss: 0.6498 - val_accuracy: 0.5815\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6411 - accuracy: 0.6009 - val_loss: 0.6462 - val_accuracy: 0.5815\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6460 - accuracy: 0.6173 - val_loss: 0.6404 - val_accuracy: 0.6010\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6312 - accuracy: 0.6301 - val_loss: 0.6332 - val_accuracy: 0.6131\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6359 - accuracy: 0.6124 - val_loss: 0.6250 - val_accuracy: 0.6156\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6278 - accuracy: 0.6173 - val_loss: 0.6166 - val_accuracy: 0.6156\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6140 - accuracy: 0.6411 - val_loss: 0.6091 - val_accuracy: 0.6253\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6076 - accuracy: 0.6478 - val_loss: 0.6021 - val_accuracy: 0.6204\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6056 - accuracy: 0.6380 - val_loss: 0.5957 - val_accuracy: 0.6277\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5884 - accuracy: 0.6600 - val_loss: 0.5899 - val_accuracy: 0.6156\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.5870 - accuracy: 0.6386 - val_loss: 0.5844 - val_accuracy: 0.6180\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5842 - accuracy: 0.6685 - val_loss: 0.5795 - val_accuracy: 0.6277\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5771 - accuracy: 0.6691 - val_loss: 0.5752 - val_accuracy: 0.6448\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5715 - accuracy: 0.6624 - val_loss: 0.5714 - val_accuracy: 0.6715\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5609 - accuracy: 0.6789 - val_loss: 0.5678 - val_accuracy: 0.6764\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5674 - accuracy: 0.6630 - val_loss: 0.5646 - val_accuracy: 0.6764\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.5525 - accuracy: 0.6734 - val_loss: 0.5618 - val_accuracy: 0.6886\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5533 - accuracy: 0.6917 - val_loss: 0.5592 - val_accuracy: 0.6837\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5594 - accuracy: 0.6801 - val_loss: 0.5569 - val_accuracy: 0.6788\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5455 - accuracy: 0.6874 - val_loss: 0.5548 - val_accuracy: 0.6764\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5305 - accuracy: 0.7038 - val_loss: 0.5528 - val_accuracy: 0.6788\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5263 - accuracy: 0.6971 - val_loss: 0.5508 - val_accuracy: 0.6813\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5262 - accuracy: 0.7136 - val_loss: 0.5489 - val_accuracy: 0.6813\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5156 - accuracy: 0.7118 - val_loss: 0.5471 - val_accuracy: 0.6910\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5273 - accuracy: 0.7032 - val_loss: 0.5454 - val_accuracy: 0.6983\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5056 - accuracy: 0.7319 - val_loss: 0.5439 - val_accuracy: 0.7007\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5105 - accuracy: 0.7197 - val_loss: 0.5426 - val_accuracy: 0.6983\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5151 - accuracy: 0.7172 - val_loss: 0.5415 - val_accuracy: 0.6959\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5112 - accuracy: 0.7136 - val_loss: 0.5403 - val_accuracy: 0.6886\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.4975 - accuracy: 0.7282 - val_loss: 0.5392 - val_accuracy: 0.6837\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4939 - accuracy: 0.7367 - val_loss: 0.5383 - val_accuracy: 0.6861\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4889 - accuracy: 0.7404 - val_loss: 0.5373 - val_accuracy: 0.6886\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4889 - accuracy: 0.7300 - val_loss: 0.5365 - val_accuracy: 0.6983\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4777 - accuracy: 0.7434 - val_loss: 0.5358 - val_accuracy: 0.7032\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4748 - accuracy: 0.7483 - val_loss: 0.5349 - val_accuracy: 0.7032\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4782 - accuracy: 0.7465 - val_loss: 0.5341 - val_accuracy: 0.7251\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.4665 - accuracy: 0.7660 - val_loss: 0.5334 - val_accuracy: 0.7275\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.4642 - accuracy: 0.7459 - val_loss: 0.5327 - val_accuracy: 0.7299\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4664 - accuracy: 0.7465 - val_loss: 0.5321 - val_accuracy: 0.7275\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4563 - accuracy: 0.7569 - val_loss: 0.5317 - val_accuracy: 0.7299\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4624 - accuracy: 0.7636 - val_loss: 0.5314 - val_accuracy: 0.7275\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4575 - accuracy: 0.7684 - val_loss: 0.5315 - val_accuracy: 0.7348\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4482 - accuracy: 0.7648 - val_loss: 0.5316 - val_accuracy: 0.7372\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4462 - accuracy: 0.7757 - val_loss: 0.5317 - val_accuracy: 0.7372\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.4337 - accuracy: 0.7818 - val_loss: 0.5321 - val_accuracy: 0.7348\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.4426 - accuracy: 0.7715 - val_loss: 0.5325 - val_accuracy: 0.7348\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.4288 - accuracy: 0.7873 - val_loss: 0.5329 - val_accuracy: 0.7299\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.4365 - accuracy: 0.7824 - val_loss: 0.5335 - val_accuracy: 0.7299\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4240 - accuracy: 0.7922 - val_loss: 0.5341 - val_accuracy: 0.7372\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4213 - accuracy: 0.7983 - val_loss: 0.5349 - val_accuracy: 0.7348\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4154 - accuracy: 0.7867 - val_loss: 0.5358 - val_accuracy: 0.7324\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4190 - accuracy: 0.7910 - val_loss: 0.5369 - val_accuracy: 0.7299\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.4007 - accuracy: 0.8038 - val_loss: 0.5382 - val_accuracy: 0.7299\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4103 - accuracy: 0.7965 - val_loss: 0.5395 - val_accuracy: 0.7299\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3923 - accuracy: 0.7989 - val_loss: 0.5410 - val_accuracy: 0.7251\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.4036 - accuracy: 0.8123 - val_loss: 0.5425 - val_accuracy: 0.7226\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.4041 - accuracy: 0.8038 - val_loss: 0.5440 - val_accuracy: 0.7226\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3930 - accuracy: 0.8062 - val_loss: 0.5453 - val_accuracy: 0.7226\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.3978 - accuracy: 0.8087 - val_loss: 0.5468 - val_accuracy: 0.7251\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.3820 - accuracy: 0.8160 - val_loss: 0.5482 - val_accuracy: 0.7299\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3845 - accuracy: 0.8117 - val_loss: 0.5497 - val_accuracy: 0.7299\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3749 - accuracy: 0.8172 - val_loss: 0.5514 - val_accuracy: 0.7299\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3734 - accuracy: 0.8123 - val_loss: 0.5532 - val_accuracy: 0.7299\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3709 - accuracy: 0.8269 - val_loss: 0.5553 - val_accuracy: 0.7299\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3681 - accuracy: 0.8208 - val_loss: 0.5575 - val_accuracy: 0.7299\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3644 - accuracy: 0.8257 - val_loss: 0.5598 - val_accuracy: 0.7324\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3701 - accuracy: 0.8166 - val_loss: 0.5621 - val_accuracy: 0.7348\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3616 - accuracy: 0.8342 - val_loss: 0.5644 - val_accuracy: 0.7348\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.3620 - accuracy: 0.8300 - val_loss: 0.5667 - val_accuracy: 0.7348\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3499 - accuracy: 0.8367 - val_loss: 0.5690 - val_accuracy: 0.7372\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3478 - accuracy: 0.8379 - val_loss: 0.5716 - val_accuracy: 0.7372\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3388 - accuracy: 0.8428 - val_loss: 0.5745 - val_accuracy: 0.7397\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.3411 - accuracy: 0.8489 - val_loss: 0.5775 - val_accuracy: 0.7397\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3500 - accuracy: 0.8306 - val_loss: 0.5807 - val_accuracy: 0.7397\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3314 - accuracy: 0.8434 - val_loss: 0.5844 - val_accuracy: 0.7421\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3373 - accuracy: 0.8367 - val_loss: 0.5882 - val_accuracy: 0.7445\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3227 - accuracy: 0.8464 - val_loss: 0.5925 - val_accuracy: 0.7445\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3195 - accuracy: 0.8483 - val_loss: 0.5969 - val_accuracy: 0.7470\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.3381 - accuracy: 0.8373 - val_loss: 0.6010 - val_accuracy: 0.7470\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.3027 - accuracy: 0.8665 - val_loss: 0.6055 - val_accuracy: 0.7494\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3286 - accuracy: 0.8519 - val_loss: 0.6096 - val_accuracy: 0.7494\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.3052 - accuracy: 0.8641 - val_loss: 0.6139 - val_accuracy: 0.7494\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3065 - accuracy: 0.8592 - val_loss: 0.6183 - val_accuracy: 0.7494\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2971 - accuracy: 0.8672 - val_loss: 0.6227 - val_accuracy: 0.7470\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2947 - accuracy: 0.8708 - val_loss: 0.6275 - val_accuracy: 0.7494\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2975 - accuracy: 0.8665 - val_loss: 0.6323 - val_accuracy: 0.7494\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2945 - accuracy: 0.8665 - val_loss: 0.6372 - val_accuracy: 0.7543\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2985 - accuracy: 0.8678 - val_loss: 0.6421 - val_accuracy: 0.7591\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2946 - accuracy: 0.8672 - val_loss: 0.6476 - val_accuracy: 0.7591\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.3025 - accuracy: 0.8580 - val_loss: 0.6529 - val_accuracy: 0.7591\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2849 - accuracy: 0.8732 - val_loss: 0.6586 - val_accuracy: 0.7616\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2789 - accuracy: 0.8800 - val_loss: 0.6644 - val_accuracy: 0.7591\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2725 - accuracy: 0.8787 - val_loss: 0.6703 - val_accuracy: 0.7567\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2878 - accuracy: 0.8678 - val_loss: 0.6766 - val_accuracy: 0.7567\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2807 - accuracy: 0.8714 - val_loss: 0.6831 - val_accuracy: 0.7591\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.2655 - accuracy: 0.8842 - val_loss: 0.6896 - val_accuracy: 0.7616\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2725 - accuracy: 0.8860 - val_loss: 0.6964 - val_accuracy: 0.7640\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2511 - accuracy: 0.8885 - val_loss: 0.7036 - val_accuracy: 0.7567\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2670 - accuracy: 0.8830 - val_loss: 0.7109 - val_accuracy: 0.7543\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2685 - accuracy: 0.8830 - val_loss: 0.7181 - val_accuracy: 0.7543\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2543 - accuracy: 0.8879 - val_loss: 0.7250 - val_accuracy: 0.7543\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2524 - accuracy: 0.8946 - val_loss: 0.7320 - val_accuracy: 0.7543\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2630 - accuracy: 0.8854 - val_loss: 0.7388 - val_accuracy: 0.7518\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2452 - accuracy: 0.8909 - val_loss: 0.7457 - val_accuracy: 0.7494\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2518 - accuracy: 0.8934 - val_loss: 0.7524 - val_accuracy: 0.7494\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2364 - accuracy: 0.8964 - val_loss: 0.7595 - val_accuracy: 0.7470\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2514 - accuracy: 0.8854 - val_loss: 0.7660 - val_accuracy: 0.7470\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2232 - accuracy: 0.9055 - val_loss: 0.7725 - val_accuracy: 0.7470\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2335 - accuracy: 0.9013 - val_loss: 0.7797 - val_accuracy: 0.7445\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2273 - accuracy: 0.9062 - val_loss: 0.7861 - val_accuracy: 0.7470\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2358 - accuracy: 0.9001 - val_loss: 0.7930 - val_accuracy: 0.7470\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2268 - accuracy: 0.9037 - val_loss: 0.7996 - val_accuracy: 0.7518\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2264 - accuracy: 0.8970 - val_loss: 0.8073 - val_accuracy: 0.7494\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2173 - accuracy: 0.9074 - val_loss: 0.8159 - val_accuracy: 0.7470\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2158 - accuracy: 0.9080 - val_loss: 0.8246 - val_accuracy: 0.7494\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2276 - accuracy: 0.9037 - val_loss: 0.8323 - val_accuracy: 0.7543\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2251 - accuracy: 0.9086 - val_loss: 0.8404 - val_accuracy: 0.7567\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2196 - accuracy: 0.9074 - val_loss: 0.8477 - val_accuracy: 0.7543\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2090 - accuracy: 0.9135 - val_loss: 0.8559 - val_accuracy: 0.7591\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.2006 - accuracy: 0.9122 - val_loss: 0.8654 - val_accuracy: 0.7591\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2132 - accuracy: 0.9068 - val_loss: 0.8756 - val_accuracy: 0.7591\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2081 - accuracy: 0.9122 - val_loss: 0.8870 - val_accuracy: 0.7567\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1958 - accuracy: 0.9196 - val_loss: 0.8979 - val_accuracy: 0.7591\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2109 - accuracy: 0.9122 - val_loss: 0.9084 - val_accuracy: 0.7616\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1957 - accuracy: 0.9153 - val_loss: 0.9176 - val_accuracy: 0.7640\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.2004 - accuracy: 0.9092 - val_loss: 0.9266 - val_accuracy: 0.7616\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1997 - accuracy: 0.9135 - val_loss: 0.9345 - val_accuracy: 0.7591\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2051 - accuracy: 0.9086 - val_loss: 0.9426 - val_accuracy: 0.7616\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.2028 - accuracy: 0.9177 - val_loss: 0.9510 - val_accuracy: 0.7640\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1964 - accuracy: 0.9147 - val_loss: 0.9588 - val_accuracy: 0.7640\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1944 - accuracy: 0.9208 - val_loss: 0.9669 - val_accuracy: 0.7689\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1943 - accuracy: 0.9165 - val_loss: 0.9751 - val_accuracy: 0.7689\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1903 - accuracy: 0.9171 - val_loss: 0.9836 - val_accuracy: 0.7689\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1862 - accuracy: 0.9129 - val_loss: 0.9923 - val_accuracy: 0.7713\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1775 - accuracy: 0.9250 - val_loss: 1.0014 - val_accuracy: 0.7713\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1720 - accuracy: 0.9269 - val_loss: 1.0104 - val_accuracy: 0.7713\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1769 - accuracy: 0.9269 - val_loss: 1.0200 - val_accuracy: 0.7713\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1723 - accuracy: 0.9317 - val_loss: 1.0305 - val_accuracy: 0.7762\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1712 - accuracy: 0.9269 - val_loss: 1.0409 - val_accuracy: 0.7762\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1700 - accuracy: 0.9336 - val_loss: 1.0512 - val_accuracy: 0.7762\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1794 - accuracy: 0.9238 - val_loss: 1.0621 - val_accuracy: 0.7762\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1675 - accuracy: 0.9299 - val_loss: 1.0720 - val_accuracy: 0.7762\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1762 - accuracy: 0.9330 - val_loss: 1.0821 - val_accuracy: 0.7762\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1802 - accuracy: 0.9263 - val_loss: 1.0917 - val_accuracy: 0.7762\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1603 - accuracy: 0.9311 - val_loss: 1.1022 - val_accuracy: 0.7762\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1637 - accuracy: 0.9378 - val_loss: 1.1128 - val_accuracy: 0.7762\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1620 - accuracy: 0.9372 - val_loss: 1.1237 - val_accuracy: 0.7762\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1733 - accuracy: 0.9299 - val_loss: 1.1335 - val_accuracy: 0.7762\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1533 - accuracy: 0.9433 - val_loss: 1.1439 - val_accuracy: 0.7737\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1578 - accuracy: 0.9342 - val_loss: 1.1552 - val_accuracy: 0.7737\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1531 - accuracy: 0.9427 - val_loss: 1.1664 - val_accuracy: 0.7713\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1488 - accuracy: 0.9397 - val_loss: 1.1769 - val_accuracy: 0.7664\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1451 - accuracy: 0.9427 - val_loss: 1.1882 - val_accuracy: 0.7640\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.1421 - accuracy: 0.9458 - val_loss: 1.1996 - val_accuracy: 0.7640\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1451 - accuracy: 0.9421 - val_loss: 1.2113 - val_accuracy: 0.7689\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1417 - accuracy: 0.9458 - val_loss: 1.2234 - val_accuracy: 0.7689\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1481 - accuracy: 0.9403 - val_loss: 1.2364 - val_accuracy: 0.7689\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1518 - accuracy: 0.9311 - val_loss: 1.2489 - val_accuracy: 0.7713\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1386 - accuracy: 0.9470 - val_loss: 1.2609 - val_accuracy: 0.7664\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1468 - accuracy: 0.9470 - val_loss: 1.2733 - val_accuracy: 0.7640\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1378 - accuracy: 0.9409 - val_loss: 1.2856 - val_accuracy: 0.7640\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1312 - accuracy: 0.9415 - val_loss: 1.2977 - val_accuracy: 0.7640\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1314 - accuracy: 0.9464 - val_loss: 1.3106 - val_accuracy: 0.7664\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1321 - accuracy: 0.9519 - val_loss: 1.3232 - val_accuracy: 0.7640\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1297 - accuracy: 0.9531 - val_loss: 1.3354 - val_accuracy: 0.7664\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1302 - accuracy: 0.9500 - val_loss: 1.3484 - val_accuracy: 0.7664\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.1277 - accuracy: 0.9482 - val_loss: 1.3604 - val_accuracy: 0.7640\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1318 - accuracy: 0.9421 - val_loss: 1.3731 - val_accuracy: 0.7640\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1318 - accuracy: 0.9433 - val_loss: 1.3843 - val_accuracy: 0.7640\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1357 - accuracy: 0.9500 - val_loss: 1.3967 - val_accuracy: 0.7616\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1265 - accuracy: 0.9476 - val_loss: 1.4110 - val_accuracy: 0.7640\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1416 - accuracy: 0.9421 - val_loss: 1.4248 - val_accuracy: 0.7664\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1197 - accuracy: 0.9482 - val_loss: 1.4393 - val_accuracy: 0.7664\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.1339 - accuracy: 0.9476 - val_loss: 1.4541 - val_accuracy: 0.7689\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1179 - accuracy: 0.9464 - val_loss: 1.4695 - val_accuracy: 0.7689\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1142 - accuracy: 0.9519 - val_loss: 1.4867 - val_accuracy: 0.7689\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1176 - accuracy: 0.9525 - val_loss: 1.5046 - val_accuracy: 0.7664\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1247 - accuracy: 0.9543 - val_loss: 1.5209 - val_accuracy: 0.7664\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1195 - accuracy: 0.9561 - val_loss: 1.5373 - val_accuracy: 0.7664\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1203 - accuracy: 0.9470 - val_loss: 1.5533 - val_accuracy: 0.7664\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1201 - accuracy: 0.9543 - val_loss: 1.5682 - val_accuracy: 0.7640\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1146 - accuracy: 0.9580 - val_loss: 1.5827 - val_accuracy: 0.7616\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1166 - accuracy: 0.9543 - val_loss: 1.5985 - val_accuracy: 0.7616\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1216 - accuracy: 0.9488 - val_loss: 1.6124 - val_accuracy: 0.7640\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1134 - accuracy: 0.9573 - val_loss: 1.6259 - val_accuracy: 0.7616\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1097 - accuracy: 0.9567 - val_loss: 1.6386 - val_accuracy: 0.7616\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1059 - accuracy: 0.9604 - val_loss: 1.6506 - val_accuracy: 0.7616\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1166 - accuracy: 0.9482 - val_loss: 1.6618 - val_accuracy: 0.7640\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1093 - accuracy: 0.9537 - val_loss: 1.6713 - val_accuracy: 0.7640\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1103 - accuracy: 0.9586 - val_loss: 1.6819 - val_accuracy: 0.7640\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1118 - accuracy: 0.9580 - val_loss: 1.6934 - val_accuracy: 0.7664\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.1215 - accuracy: 0.9561 - val_loss: 1.7063 - val_accuracy: 0.7689\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1077 - accuracy: 0.9567 - val_loss: 1.7191 - val_accuracy: 0.7689\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1058 - accuracy: 0.9586 - val_loss: 1.7303 - val_accuracy: 0.7713\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.1024 - accuracy: 0.9616 - val_loss: 1.7424 - val_accuracy: 0.7737\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1124 - accuracy: 0.9561 - val_loss: 1.7548 - val_accuracy: 0.7737\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1096 - accuracy: 0.9592 - val_loss: 1.7707 - val_accuracy: 0.7713\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.1113 - accuracy: 0.9561 - val_loss: 1.7856 - val_accuracy: 0.7664\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0995 - accuracy: 0.9586 - val_loss: 1.7993 - val_accuracy: 0.7664\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0995 - accuracy: 0.9567 - val_loss: 1.8129 - val_accuracy: 0.7689\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.1119 - accuracy: 0.9525 - val_loss: 1.8251 - val_accuracy: 0.7689\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1117 - accuracy: 0.9580 - val_loss: 1.8395 - val_accuracy: 0.7713\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0959 - accuracy: 0.9598 - val_loss: 1.8538 - val_accuracy: 0.7689\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1037 - accuracy: 0.9580 - val_loss: 1.8683 - val_accuracy: 0.7737\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0947 - accuracy: 0.9586 - val_loss: 1.8827 - val_accuracy: 0.7737\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1050 - accuracy: 0.9573 - val_loss: 1.8961 - val_accuracy: 0.7762\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0969 - accuracy: 0.9634 - val_loss: 1.9097 - val_accuracy: 0.7737\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0931 - accuracy: 0.9634 - val_loss: 1.9262 - val_accuracy: 0.7762\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1038 - accuracy: 0.9567 - val_loss: 1.9413 - val_accuracy: 0.7762\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1037 - accuracy: 0.9561 - val_loss: 1.9590 - val_accuracy: 0.7786\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0921 - accuracy: 0.9677 - val_loss: 1.9766 - val_accuracy: 0.7762\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0859 - accuracy: 0.9665 - val_loss: 1.9940 - val_accuracy: 0.7762\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1024 - accuracy: 0.9640 - val_loss: 2.0090 - val_accuracy: 0.7737\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0963 - accuracy: 0.9616 - val_loss: 2.0227 - val_accuracy: 0.7737\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.1000 - accuracy: 0.9634 - val_loss: 2.0351 - val_accuracy: 0.7689\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0880 - accuracy: 0.9647 - val_loss: 2.0504 - val_accuracy: 0.7689\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0958 - accuracy: 0.9580 - val_loss: 2.0635 - val_accuracy: 0.7713\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0857 - accuracy: 0.9677 - val_loss: 2.0790 - val_accuracy: 0.7713\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0986 - accuracy: 0.9628 - val_loss: 2.0954 - val_accuracy: 0.7737\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0906 - accuracy: 0.9634 - val_loss: 2.1132 - val_accuracy: 0.7689\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0963 - accuracy: 0.9592 - val_loss: 2.1286 - val_accuracy: 0.7737\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0917 - accuracy: 0.9586 - val_loss: 2.1421 - val_accuracy: 0.7737\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0821 - accuracy: 0.9671 - val_loss: 2.1537 - val_accuracy: 0.7713\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0963 - accuracy: 0.9610 - val_loss: 2.1655 - val_accuracy: 0.7689\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0909 - accuracy: 0.9659 - val_loss: 2.1762 - val_accuracy: 0.7664\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0839 - accuracy: 0.9659 - val_loss: 2.1877 - val_accuracy: 0.7664\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0858 - accuracy: 0.9671 - val_loss: 2.2014 - val_accuracy: 0.7664\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0766 - accuracy: 0.9701 - val_loss: 2.2145 - val_accuracy: 0.7713\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0850 - accuracy: 0.9634 - val_loss: 2.2286 - val_accuracy: 0.7713\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0871 - accuracy: 0.9634 - val_loss: 2.2422 - val_accuracy: 0.7713\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0883 - accuracy: 0.9647 - val_loss: 2.2575 - val_accuracy: 0.7786\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0792 - accuracy: 0.9689 - val_loss: 2.2738 - val_accuracy: 0.7786\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0804 - accuracy: 0.9683 - val_loss: 2.2903 - val_accuracy: 0.7762\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0925 - accuracy: 0.9647 - val_loss: 2.3050 - val_accuracy: 0.7786\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0819 - accuracy: 0.9689 - val_loss: 2.3201 - val_accuracy: 0.7786\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0842 - accuracy: 0.9677 - val_loss: 2.3338 - val_accuracy: 0.7786\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0814 - accuracy: 0.9683 - val_loss: 2.3471 - val_accuracy: 0.7762\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0844 - accuracy: 0.9647 - val_loss: 2.3634 - val_accuracy: 0.7762\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0861 - accuracy: 0.9659 - val_loss: 2.3780 - val_accuracy: 0.7762\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0754 - accuracy: 0.9738 - val_loss: 2.3921 - val_accuracy: 0.7762\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0773 - accuracy: 0.9677 - val_loss: 2.4078 - val_accuracy: 0.7762\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0812 - accuracy: 0.9695 - val_loss: 2.4239 - val_accuracy: 0.7713\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0744 - accuracy: 0.9677 - val_loss: 2.4413 - val_accuracy: 0.7737\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0787 - accuracy: 0.9683 - val_loss: 2.4583 - val_accuracy: 0.7737\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0843 - accuracy: 0.9640 - val_loss: 2.4726 - val_accuracy: 0.7689\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0802 - accuracy: 0.9671 - val_loss: 2.4855 - val_accuracy: 0.7689\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0728 - accuracy: 0.9677 - val_loss: 2.5007 - val_accuracy: 0.7689\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0814 - accuracy: 0.9695 - val_loss: 2.5147 - val_accuracy: 0.7689\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0789 - accuracy: 0.9653 - val_loss: 2.5309 - val_accuracy: 0.7689\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0801 - accuracy: 0.9677 - val_loss: 2.5414 - val_accuracy: 0.7689\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0832 - accuracy: 0.9665 - val_loss: 2.5509 - val_accuracy: 0.7689\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 2.5588 - val_accuracy: 0.7689\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0771 - accuracy: 0.9695 - val_loss: 2.5666 - val_accuracy: 0.7689\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0647 - accuracy: 0.9738 - val_loss: 2.5742 - val_accuracy: 0.7713\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0731 - accuracy: 0.9744 - val_loss: 2.5830 - val_accuracy: 0.7737\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0740 - accuracy: 0.9744 - val_loss: 2.5912 - val_accuracy: 0.7737\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0747 - accuracy: 0.9695 - val_loss: 2.6021 - val_accuracy: 0.7713\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0688 - accuracy: 0.9726 - val_loss: 2.6153 - val_accuracy: 0.7737\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0681 - accuracy: 0.9720 - val_loss: 2.6281 - val_accuracy: 0.7737\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0733 - accuracy: 0.9677 - val_loss: 2.6425 - val_accuracy: 0.7737\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0735 - accuracy: 0.9689 - val_loss: 2.6560 - val_accuracy: 0.7737\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0802 - accuracy: 0.9659 - val_loss: 2.6709 - val_accuracy: 0.7737\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0728 - accuracy: 0.9720 - val_loss: 2.6860 - val_accuracy: 0.7737\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0720 - accuracy: 0.9714 - val_loss: 2.7020 - val_accuracy: 0.7689\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0719 - accuracy: 0.9695 - val_loss: 2.7164 - val_accuracy: 0.7713\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0747 - accuracy: 0.9677 - val_loss: 2.7267 - val_accuracy: 0.7713\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0722 - accuracy: 0.9714 - val_loss: 2.7382 - val_accuracy: 0.7713\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0762 - accuracy: 0.9689 - val_loss: 2.7502 - val_accuracy: 0.7689\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0696 - accuracy: 0.9726 - val_loss: 2.7653 - val_accuracy: 0.7664\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0678 - accuracy: 0.9768 - val_loss: 2.7808 - val_accuracy: 0.7664\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0668 - accuracy: 0.9720 - val_loss: 2.7967 - val_accuracy: 0.7664\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0650 - accuracy: 0.9768 - val_loss: 2.8117 - val_accuracy: 0.7713\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.0634 - accuracy: 0.9720 - val_loss: 2.8256 - val_accuracy: 0.7713\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0653 - accuracy: 0.9744 - val_loss: 2.8391 - val_accuracy: 0.7713\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0736 - accuracy: 0.9701 - val_loss: 2.8514 - val_accuracy: 0.7713\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0748 - accuracy: 0.9707 - val_loss: 2.8614 - val_accuracy: 0.7737\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0724 - accuracy: 0.9707 - val_loss: 2.8714 - val_accuracy: 0.7713\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0637 - accuracy: 0.9714 - val_loss: 2.8814 - val_accuracy: 0.7713\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0688 - accuracy: 0.9738 - val_loss: 2.8918 - val_accuracy: 0.7737\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0720 - accuracy: 0.9677 - val_loss: 2.9005 - val_accuracy: 0.7762\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0731 - accuracy: 0.9689 - val_loss: 2.9091 - val_accuracy: 0.7786\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0778 - accuracy: 0.9665 - val_loss: 2.9187 - val_accuracy: 0.7762\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0637 - accuracy: 0.9720 - val_loss: 2.9270 - val_accuracy: 0.7786\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0687 - accuracy: 0.9695 - val_loss: 2.9362 - val_accuracy: 0.7786\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0781 - accuracy: 0.9665 - val_loss: 2.9466 - val_accuracy: 0.7786\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0650 - accuracy: 0.9744 - val_loss: 2.9551 - val_accuracy: 0.7786\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0647 - accuracy: 0.9714 - val_loss: 2.9635 - val_accuracy: 0.7762\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0664 - accuracy: 0.9732 - val_loss: 2.9720 - val_accuracy: 0.7810\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0647 - accuracy: 0.9750 - val_loss: 2.9816 - val_accuracy: 0.7810\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0664 - accuracy: 0.9781 - val_loss: 2.9905 - val_accuracy: 0.7786\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0628 - accuracy: 0.9701 - val_loss: 2.9995 - val_accuracy: 0.7786\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0627 - accuracy: 0.9756 - val_loss: 3.0099 - val_accuracy: 0.7762\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0639 - accuracy: 0.9726 - val_loss: 3.0220 - val_accuracy: 0.7762\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0707 - accuracy: 0.9701 - val_loss: 3.0351 - val_accuracy: 0.7786\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0670 - accuracy: 0.9714 - val_loss: 3.0502 - val_accuracy: 0.7786\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0659 - accuracy: 0.9714 - val_loss: 3.0628 - val_accuracy: 0.7810\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0634 - accuracy: 0.9744 - val_loss: 3.0736 - val_accuracy: 0.7786\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0638 - accuracy: 0.9738 - val_loss: 3.0837 - val_accuracy: 0.7786\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0602 - accuracy: 0.9744 - val_loss: 3.0944 - val_accuracy: 0.7786\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0685 - accuracy: 0.9689 - val_loss: 3.1053 - val_accuracy: 0.7786\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0620 - accuracy: 0.9781 - val_loss: 3.1156 - val_accuracy: 0.7786\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0649 - accuracy: 0.9726 - val_loss: 3.1246 - val_accuracy: 0.7786\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0669 - accuracy: 0.9787 - val_loss: 3.1322 - val_accuracy: 0.7786\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0632 - accuracy: 0.9750 - val_loss: 3.1393 - val_accuracy: 0.7835\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0662 - accuracy: 0.9762 - val_loss: 3.1458 - val_accuracy: 0.7810\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0570 - accuracy: 0.9744 - val_loss: 3.1538 - val_accuracy: 0.7810\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0626 - accuracy: 0.9714 - val_loss: 3.1631 - val_accuracy: 0.7810\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0597 - accuracy: 0.9707 - val_loss: 3.1725 - val_accuracy: 0.7859\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0637 - accuracy: 0.9738 - val_loss: 3.1832 - val_accuracy: 0.7810\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0610 - accuracy: 0.9726 - val_loss: 3.1945 - val_accuracy: 0.7810\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0614 - accuracy: 0.9793 - val_loss: 3.2056 - val_accuracy: 0.7786\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0616 - accuracy: 0.9720 - val_loss: 3.2171 - val_accuracy: 0.7810\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0626 - accuracy: 0.9738 - val_loss: 3.2280 - val_accuracy: 0.7810\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0578 - accuracy: 0.9756 - val_loss: 3.2384 - val_accuracy: 0.7835\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0532 - accuracy: 0.9775 - val_loss: 3.2494 - val_accuracy: 0.7835\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0595 - accuracy: 0.9738 - val_loss: 3.2605 - val_accuracy: 0.7883\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0634 - accuracy: 0.9738 - val_loss: 3.2717 - val_accuracy: 0.7859\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0639 - accuracy: 0.9707 - val_loss: 3.2841 - val_accuracy: 0.7859\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0662 - accuracy: 0.9744 - val_loss: 3.3003 - val_accuracy: 0.7883\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0542 - accuracy: 0.9817 - val_loss: 3.3160 - val_accuracy: 0.7908\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0617 - accuracy: 0.9750 - val_loss: 3.3312 - val_accuracy: 0.7786\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0651 - accuracy: 0.9695 - val_loss: 3.3471 - val_accuracy: 0.7713\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0669 - accuracy: 0.9768 - val_loss: 3.3606 - val_accuracy: 0.7713\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0644 - accuracy: 0.9714 - val_loss: 3.3721 - val_accuracy: 0.7713\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0629 - accuracy: 0.9726 - val_loss: 3.3804 - val_accuracy: 0.7713\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0631 - accuracy: 0.9695 - val_loss: 3.3903 - val_accuracy: 0.7737\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0669 - accuracy: 0.9720 - val_loss: 3.3946 - val_accuracy: 0.7737\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0592 - accuracy: 0.9768 - val_loss: 3.3996 - val_accuracy: 0.7762\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0569 - accuracy: 0.9775 - val_loss: 3.4047 - val_accuracy: 0.7810\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0571 - accuracy: 0.9762 - val_loss: 3.4094 - val_accuracy: 0.7786\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0576 - accuracy: 0.9738 - val_loss: 3.4155 - val_accuracy: 0.7835\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0681 - accuracy: 0.9726 - val_loss: 3.4260 - val_accuracy: 0.7810\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0635 - accuracy: 0.9732 - val_loss: 3.4364 - val_accuracy: 0.7810\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0668 - accuracy: 0.9732 - val_loss: 3.4481 - val_accuracy: 0.7835\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0572 - accuracy: 0.9787 - val_loss: 3.4618 - val_accuracy: 0.7786\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0578 - accuracy: 0.9762 - val_loss: 3.4775 - val_accuracy: 0.7835\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0612 - accuracy: 0.9787 - val_loss: 3.4871 - val_accuracy: 0.7810\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0565 - accuracy: 0.9744 - val_loss: 3.4971 - val_accuracy: 0.7786\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0552 - accuracy: 0.9781 - val_loss: 3.5086 - val_accuracy: 0.7786\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0609 - accuracy: 0.9732 - val_loss: 3.5188 - val_accuracy: 0.7762\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0616 - accuracy: 0.9762 - val_loss: 3.5279 - val_accuracy: 0.7786\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0586 - accuracy: 0.9768 - val_loss: 3.5375 - val_accuracy: 0.7810\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0540 - accuracy: 0.9768 - val_loss: 3.5483 - val_accuracy: 0.7810\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0559 - accuracy: 0.9799 - val_loss: 3.5604 - val_accuracy: 0.7810\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0613 - accuracy: 0.9756 - val_loss: 3.5704 - val_accuracy: 0.7762\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0581 - accuracy: 0.9775 - val_loss: 3.5779 - val_accuracy: 0.7762\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0591 - accuracy: 0.9756 - val_loss: 3.5896 - val_accuracy: 0.7762\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0595 - accuracy: 0.9726 - val_loss: 3.6009 - val_accuracy: 0.7762\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0624 - accuracy: 0.9720 - val_loss: 3.6046 - val_accuracy: 0.7762\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0626 - accuracy: 0.9726 - val_loss: 3.6075 - val_accuracy: 0.7762\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0624 - accuracy: 0.9762 - val_loss: 3.6121 - val_accuracy: 0.7786\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0601 - accuracy: 0.9744 - val_loss: 3.6154 - val_accuracy: 0.7786\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0649 - accuracy: 0.9683 - val_loss: 3.6209 - val_accuracy: 0.7737\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0640 - accuracy: 0.9720 - val_loss: 3.6266 - val_accuracy: 0.7786\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0585 - accuracy: 0.9762 - val_loss: 3.6341 - val_accuracy: 0.7786\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0543 - accuracy: 0.9768 - val_loss: 3.6458 - val_accuracy: 0.7786\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0561 - accuracy: 0.9787 - val_loss: 3.6586 - val_accuracy: 0.7762\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0565 - accuracy: 0.9738 - val_loss: 3.6726 - val_accuracy: 0.7786\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0534 - accuracy: 0.9756 - val_loss: 3.6837 - val_accuracy: 0.7786\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0523 - accuracy: 0.9787 - val_loss: 3.6962 - val_accuracy: 0.7737\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0523 - accuracy: 0.9805 - val_loss: 3.7049 - val_accuracy: 0.7762\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0572 - accuracy: 0.9762 - val_loss: 3.7153 - val_accuracy: 0.7762\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0584 - accuracy: 0.9775 - val_loss: 3.7262 - val_accuracy: 0.7713\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0583 - accuracy: 0.9738 - val_loss: 3.7359 - val_accuracy: 0.7713\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0528 - accuracy: 0.9775 - val_loss: 3.7454 - val_accuracy: 0.7762\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0554 - accuracy: 0.9768 - val_loss: 3.7559 - val_accuracy: 0.7786\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0559 - accuracy: 0.9781 - val_loss: 3.7671 - val_accuracy: 0.7786\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0611 - accuracy: 0.9732 - val_loss: 3.7799 - val_accuracy: 0.7786\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0612 - accuracy: 0.9732 - val_loss: 3.7922 - val_accuracy: 0.7737\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0610 - accuracy: 0.9677 - val_loss: 3.7992 - val_accuracy: 0.7737\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0572 - accuracy: 0.9762 - val_loss: 3.8056 - val_accuracy: 0.7737\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0529 - accuracy: 0.9787 - val_loss: 3.8125 - val_accuracy: 0.7737\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0588 - accuracy: 0.9750 - val_loss: 3.8163 - val_accuracy: 0.7737\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0533 - accuracy: 0.9750 - val_loss: 3.8205 - val_accuracy: 0.7762\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0539 - accuracy: 0.9768 - val_loss: 3.8246 - val_accuracy: 0.7762\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0624 - accuracy: 0.9707 - val_loss: 3.8288 - val_accuracy: 0.7762\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0513 - accuracy: 0.9799 - val_loss: 3.8345 - val_accuracy: 0.7762\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0557 - accuracy: 0.9756 - val_loss: 3.8402 - val_accuracy: 0.7762\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0571 - accuracy: 0.9750 - val_loss: 3.8474 - val_accuracy: 0.7786\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0623 - accuracy: 0.9750 - val_loss: 3.8535 - val_accuracy: 0.7786\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0559 - accuracy: 0.9720 - val_loss: 3.8602 - val_accuracy: 0.7835\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0533 - accuracy: 0.9781 - val_loss: 3.8693 - val_accuracy: 0.7835\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0559 - accuracy: 0.9744 - val_loss: 3.8811 - val_accuracy: 0.7810\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0507 - accuracy: 0.9805 - val_loss: 3.8968 - val_accuracy: 0.7786\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0531 - accuracy: 0.9787 - val_loss: 3.9126 - val_accuracy: 0.7786\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0538 - accuracy: 0.9775 - val_loss: 3.9287 - val_accuracy: 0.7762\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0565 - accuracy: 0.9732 - val_loss: 3.9446 - val_accuracy: 0.7762\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0646 - accuracy: 0.9695 - val_loss: 3.9527 - val_accuracy: 0.7786\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0568 - accuracy: 0.9756 - val_loss: 3.9566 - val_accuracy: 0.7786\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0527 - accuracy: 0.9732 - val_loss: 3.9657 - val_accuracy: 0.7786\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0567 - accuracy: 0.9762 - val_loss: 3.9765 - val_accuracy: 0.7786\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0527 - accuracy: 0.9805 - val_loss: 3.9886 - val_accuracy: 0.7762\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0579 - accuracy: 0.9732 - val_loss: 3.9985 - val_accuracy: 0.7762\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0606 - accuracy: 0.9726 - val_loss: 4.0086 - val_accuracy: 0.7810\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0570 - accuracy: 0.9781 - val_loss: 4.0175 - val_accuracy: 0.7810\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0579 - accuracy: 0.9732 - val_loss: 4.0265 - val_accuracy: 0.7810\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0536 - accuracy: 0.9744 - val_loss: 4.0367 - val_accuracy: 0.7786\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0561 - accuracy: 0.9775 - val_loss: 4.0478 - val_accuracy: 0.7762\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0569 - accuracy: 0.9768 - val_loss: 4.0621 - val_accuracy: 0.7786\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0491 - accuracy: 0.9799 - val_loss: 4.0745 - val_accuracy: 0.7737\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0574 - accuracy: 0.9756 - val_loss: 4.0837 - val_accuracy: 0.7762\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0572 - accuracy: 0.9732 - val_loss: 4.0919 - val_accuracy: 0.7737\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0586 - accuracy: 0.9750 - val_loss: 4.1006 - val_accuracy: 0.7737\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0520 - accuracy: 0.9805 - val_loss: 4.1095 - val_accuracy: 0.7762\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0514 - accuracy: 0.9781 - val_loss: 4.1189 - val_accuracy: 0.7737\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0481 - accuracy: 0.9811 - val_loss: 4.1267 - val_accuracy: 0.7737\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0594 - accuracy: 0.9762 - val_loss: 4.1327 - val_accuracy: 0.7737\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0516 - accuracy: 0.9775 - val_loss: 4.1396 - val_accuracy: 0.7737\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0582 - accuracy: 0.9732 - val_loss: 4.1483 - val_accuracy: 0.7737\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0529 - accuracy: 0.9762 - val_loss: 4.1583 - val_accuracy: 0.7737\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0646 - accuracy: 0.9762 - val_loss: 4.1681 - val_accuracy: 0.7737\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0528 - accuracy: 0.9787 - val_loss: 4.1760 - val_accuracy: 0.7737\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0536 - accuracy: 0.9756 - val_loss: 4.1826 - val_accuracy: 0.7713\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0512 - accuracy: 0.9762 - val_loss: 4.1886 - val_accuracy: 0.7737\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0534 - accuracy: 0.9793 - val_loss: 4.1937 - val_accuracy: 0.7737\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0587 - accuracy: 0.9714 - val_loss: 4.2005 - val_accuracy: 0.7737\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0511 - accuracy: 0.9756 - val_loss: 4.2083 - val_accuracy: 0.7762\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0540 - accuracy: 0.9775 - val_loss: 4.2144 - val_accuracy: 0.7810\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0495 - accuracy: 0.9793 - val_loss: 4.2209 - val_accuracy: 0.7810\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0573 - accuracy: 0.9750 - val_loss: 4.2296 - val_accuracy: 0.7835\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0514 - accuracy: 0.9744 - val_loss: 4.2400 - val_accuracy: 0.7835\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0548 - accuracy: 0.9756 - val_loss: 4.2488 - val_accuracy: 0.7835\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0473 - accuracy: 0.9829 - val_loss: 4.2612 - val_accuracy: 0.7762\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0556 - accuracy: 0.9768 - val_loss: 4.2718 - val_accuracy: 0.7762\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0515 - accuracy: 0.9744 - val_loss: 4.2849 - val_accuracy: 0.7786\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0557 - accuracy: 0.9768 - val_loss: 4.2950 - val_accuracy: 0.7786\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0554 - accuracy: 0.9762 - val_loss: 4.3067 - val_accuracy: 0.7810\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0498 - accuracy: 0.9787 - val_loss: 4.3197 - val_accuracy: 0.7810\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0543 - accuracy: 0.9756 - val_loss: 4.3333 - val_accuracy: 0.7810\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0541 - accuracy: 0.9787 - val_loss: 4.3480 - val_accuracy: 0.7786\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0558 - accuracy: 0.9744 - val_loss: 4.3601 - val_accuracy: 0.7786\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0529 - accuracy: 0.9781 - val_loss: 4.3716 - val_accuracy: 0.7786\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0570 - accuracy: 0.9750 - val_loss: 4.3816 - val_accuracy: 0.7835\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0547 - accuracy: 0.9775 - val_loss: 4.3904 - val_accuracy: 0.7786\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0531 - accuracy: 0.9744 - val_loss: 4.4004 - val_accuracy: 0.7786\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 4.4094 - val_accuracy: 0.7786\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0574 - accuracy: 0.9750 - val_loss: 4.4186 - val_accuracy: 0.7762\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0485 - accuracy: 0.9787 - val_loss: 4.4263 - val_accuracy: 0.7810\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0540 - accuracy: 0.9750 - val_loss: 4.4341 - val_accuracy: 0.7786\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0487 - accuracy: 0.9762 - val_loss: 4.4451 - val_accuracy: 0.7786\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0510 - accuracy: 0.9793 - val_loss: 4.4552 - val_accuracy: 0.7737\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0537 - accuracy: 0.9775 - val_loss: 4.4703 - val_accuracy: 0.7762\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0554 - accuracy: 0.9750 - val_loss: 4.4862 - val_accuracy: 0.7737\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0539 - accuracy: 0.9781 - val_loss: 4.5005 - val_accuracy: 0.7737\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0500 - accuracy: 0.9756 - val_loss: 4.5142 - val_accuracy: 0.7737\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0472 - accuracy: 0.9787 - val_loss: 4.5284 - val_accuracy: 0.7737\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0486 - accuracy: 0.9781 - val_loss: 4.5405 - val_accuracy: 0.7713\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0545 - accuracy: 0.9793 - val_loss: 4.5488 - val_accuracy: 0.7713\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0504 - accuracy: 0.9750 - val_loss: 4.5551 - val_accuracy: 0.7737\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0483 - accuracy: 0.9750 - val_loss: 4.5629 - val_accuracy: 0.7762\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0523 - accuracy: 0.9750 - val_loss: 4.5644 - val_accuracy: 0.7762\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0491 - accuracy: 0.9793 - val_loss: 4.5671 - val_accuracy: 0.7786\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0526 - accuracy: 0.9781 - val_loss: 4.5702 - val_accuracy: 0.7835\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0473 - accuracy: 0.9799 - val_loss: 4.5746 - val_accuracy: 0.7810\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0515 - accuracy: 0.9768 - val_loss: 4.5810 - val_accuracy: 0.7835\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 4.5873 - val_accuracy: 0.7810\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0515 - accuracy: 0.9811 - val_loss: 4.5958 - val_accuracy: 0.7810\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0501 - accuracy: 0.9799 - val_loss: 4.6067 - val_accuracy: 0.7737\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0480 - accuracy: 0.9842 - val_loss: 4.6188 - val_accuracy: 0.7713\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0482 - accuracy: 0.9805 - val_loss: 4.6300 - val_accuracy: 0.7713\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0519 - accuracy: 0.9756 - val_loss: 4.6415 - val_accuracy: 0.7713\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0508 - accuracy: 0.9768 - val_loss: 4.6522 - val_accuracy: 0.7737\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0471 - accuracy: 0.9787 - val_loss: 4.6613 - val_accuracy: 0.7737\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0521 - accuracy: 0.9750 - val_loss: 4.6691 - val_accuracy: 0.7737\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0512 - accuracy: 0.9811 - val_loss: 4.6775 - val_accuracy: 0.7737\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0507 - accuracy: 0.9781 - val_loss: 4.6869 - val_accuracy: 0.7737\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0514 - accuracy: 0.9762 - val_loss: 4.6975 - val_accuracy: 0.7737\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0475 - accuracy: 0.9793 - val_loss: 4.7083 - val_accuracy: 0.7786\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0472 - accuracy: 0.9781 - val_loss: 4.7193 - val_accuracy: 0.7786\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0465 - accuracy: 0.9805 - val_loss: 4.7336 - val_accuracy: 0.7810\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0479 - accuracy: 0.9793 - val_loss: 4.7499 - val_accuracy: 0.7810\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0529 - accuracy: 0.9787 - val_loss: 4.7634 - val_accuracy: 0.7810\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0491 - accuracy: 0.9805 - val_loss: 4.7768 - val_accuracy: 0.7810\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0513 - accuracy: 0.9768 - val_loss: 4.7925 - val_accuracy: 0.7810\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0556 - accuracy: 0.9768 - val_loss: 4.8071 - val_accuracy: 0.7810\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0464 - accuracy: 0.9793 - val_loss: 4.8213 - val_accuracy: 0.7810\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0477 - accuracy: 0.9787 - val_loss: 4.8347 - val_accuracy: 0.7786\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0526 - accuracy: 0.9756 - val_loss: 4.8459 - val_accuracy: 0.7786\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0490 - accuracy: 0.9775 - val_loss: 4.8563 - val_accuracy: 0.7786\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0450 - accuracy: 0.9793 - val_loss: 4.8675 - val_accuracy: 0.7786\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0511 - accuracy: 0.9756 - val_loss: 4.8753 - val_accuracy: 0.7786\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0503 - accuracy: 0.9762 - val_loss: 4.8902 - val_accuracy: 0.7762\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0468 - accuracy: 0.9793 - val_loss: 4.9020 - val_accuracy: 0.7737\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0506 - accuracy: 0.9768 - val_loss: 4.9131 - val_accuracy: 0.7737\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0654 - accuracy: 0.9738 - val_loss: 4.9221 - val_accuracy: 0.7737\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0479 - accuracy: 0.9787 - val_loss: 4.9315 - val_accuracy: 0.7737\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0435 - accuracy: 0.9848 - val_loss: 4.9425 - val_accuracy: 0.7786\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0494 - accuracy: 0.9762 - val_loss: 4.9550 - val_accuracy: 0.7762\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0439 - accuracy: 0.9781 - val_loss: 4.9687 - val_accuracy: 0.7737\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0504 - accuracy: 0.9787 - val_loss: 4.9852 - val_accuracy: 0.7762\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0422 - accuracy: 0.9829 - val_loss: 5.0026 - val_accuracy: 0.7762\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0483 - accuracy: 0.9775 - val_loss: 5.0189 - val_accuracy: 0.7810\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0489 - accuracy: 0.9811 - val_loss: 5.0337 - val_accuracy: 0.7810\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0492 - accuracy: 0.9768 - val_loss: 5.0464 - val_accuracy: 0.7810\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0502 - accuracy: 0.9768 - val_loss: 5.0572 - val_accuracy: 0.7810\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 6 (Prop&1hot):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.824903\n",
      "1  Precision  0.753012\n",
      "2     Recall  0.718391\n",
      "3   F1 Score  0.735294\n",
      "4    ROC AUC  0.836351\n",
      "\n",
      "Confusion Matrix for Method 6 (Prop&1hot):\n",
      " [[299  41]\n",
      " [ 49 125]]\n",
      "Predicting for sequence: GLWSKIKEVGKEAAKAAAKAAG\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "\n",
      "Predictions using Combined technique:\n",
      "Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "# Load DataFrame\n",
    "display(allDataDF2)\n",
    "\n",
    "# Process data\n",
    "X1 = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1).values\n",
    "t = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1)\n",
    "display(t)\n",
    "y1 = allDataDF2['Value'].values\n",
    "\n",
    "# Reshape encoded columns\n",
    "X2 = np.array(list(allDataDF2['EncodedX'])).reshape((len(allDataDF2), -1))\n",
    "\n",
    "# Concatenate arrays\n",
    "X = np.concatenate((X1, X2), axis=1)\n",
    "x11 = pd.DataFrame(X)\n",
    "display(x11)\n",
    "\n",
    "# Split data\n",
    "X6_train, X6_test, y6_train, y6_test = train_test_split(X, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Scale data\n",
    "scaler2 = StandardScaler()\n",
    "X6_train = scaler2.fit_transform(X6_train)\n",
    "X6_test = scaler2.transform(X6_test)\n",
    "\n",
    "# Define neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train neural network model\n",
    "input_dim = X6_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "nn_model.fit(X6_train, y6_train, epochs=500, batch_size=2000, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = nn_model.predict(X6_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method6 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 6 (Prop&1hot):\\n\", performance_metrics_method6)\n",
    "print(\"\\nConfusion Matrix for Method 6 (Prop&1hot):\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_method3(sequence, nn_model, scaler):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    properties = calculate_all_properties(sequence)\n",
    "   \n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    \n",
    "    \n",
    "    sequence1 = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence1]\n",
    "    onehot_encoded = np.ravel([[0 if i != val else 1 for i in range(len(char_to_int))] for val in integer_encoded])  \n",
    "    \n",
    "    \n",
    "    total = np.concatenate((prop_values.flatten(), onehot_encoded))\n",
    "    total = total.reshape(1, -1)\n",
    "    \n",
    "    prediction = nn_model.predict(total)\n",
    "    result = 'Antimicrobial' if prediction[0] >= 0.5 else 'Non-antimicrobial'\n",
    "    \n",
    "    return result, embedding\n",
    "\n",
    "# Prediction\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "predictions, embed = predict_method3(sequence, nn_model, scaler)\n",
    "print(\"\\nPredictions using Combined technique:\")\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f4f062-4e78-4924-84f7-04c40eb4682f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.09701026, 0.023356073, 0.19239551, 0.34847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.10609158, 0.031600513, 0.15865144, 0.31917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.107020475, 0.032668713, 0.16226107, 0.3235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.1107386, 0.036594443, 0.15842809, 0.322235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.10410814, 0.046210337, 0.17820424, 0.33955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11710616, 0.032537665, 0.117993765, 0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.1260696, -0.010238703, 0.0796683, 0.233125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.16503033, -0.014033677, -0.032334927, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.145197, -0.018769035, 0.019576171, 0.18281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.109951705, 0.013725347, 0.11754644, 0.2610...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.09701026, 0.023356073, 0.19239551, 0.34847...  \n",
       "SP_2643    [-0.10609158, 0.031600513, 0.15865144, 0.31917...  \n",
       "SP_2644    [-0.107020475, 0.032668713, 0.16226107, 0.3235...  \n",
       "SP_2645    [-0.1107386, 0.036594443, 0.15842809, 0.322235...  \n",
       "SP_2646    [-0.10410814, 0.046210337, 0.17820424, 0.33955...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11710616, 0.032537665, 0.117993765, 0.2792...  \n",
       "SP_2853    [-0.1260696, -0.010238703, 0.0796683, 0.233125...  \n",
       "SP_2854    [-0.16503033, -0.014033677, -0.032334927, 0.12...  \n",
       "SP_2855    [-0.145197, -0.018769035, 0.019576171, 0.18281...  \n",
       "SP_Q09022  [-0.109951705, 0.013725347, 0.11754644, 0.2610...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>NumHAcceptors</th>\n",
       "      <th>BalabanJ</th>\n",
       "      <th>BertzCT</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>45</td>\n",
       "      <td>2.195969</td>\n",
       "      <td>7612.627585</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.246811</td>\n",
       "      <td>5191.774736</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.254177</td>\n",
       "      <td>5211.853178</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>2.253657</td>\n",
       "      <td>5096.168912</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.186024</td>\n",
       "      <td>3100.024844</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3.276060</td>\n",
       "      <td>2766.535116</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.992706</td>\n",
       "      <td>5867.905389</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>0.884764</td>\n",
       "      <td>5233.121472</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>1.659505</td>\n",
       "      <td>4729.413175</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>2.570045</td>\n",
       "      <td>18204.737165</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  NumHAcceptors  BalabanJ       BertzCT       Chi0v  \\\n",
       "DADP ID                                                                    \n",
       "SP_P31107          46             45  2.195969   7612.627585  131.799954   \n",
       "SP_2643            31             31  2.246811   5191.774736   95.319148   \n",
       "SP_2644            31             31  2.254177   5211.853178   96.026255   \n",
       "SP_2645            31             31  2.253657   5096.168912   94.987870   \n",
       "SP_2646            22             22  6.186024   3100.024844   67.476541   \n",
       "...               ...            ...       ...           ...         ...   \n",
       "SP_2852            18             19  3.276060   2766.535116   59.496327   \n",
       "SP_2853            26             26  0.992706   5867.905389   86.416638   \n",
       "SP_2854            26             29  0.884764   5233.121472   86.425115   \n",
       "SP_2855            30             28  1.659505   4729.413175   85.376209   \n",
       "SP_Q09022         111            112  2.570045  18204.737165  294.537783   \n",
       "\n",
       "                Chi1v  HallKierAlpha      Kappa1      Kappa2      Kappa3  \n",
       "DADP ID                                                                   \n",
       "SP_P31107   75.452403         -20.68  199.359058  105.101422   75.403319  \n",
       "SP_2643     54.422594         -15.02  142.034244   74.405793   54.137875  \n",
       "SP_2644     54.922594         -15.02  143.033879   75.212983   54.431542  \n",
       "SP_2645     54.392469         -14.53  141.524432   74.738555   53.425154  \n",
       "SP_2646     37.978223          -9.21  101.790000   52.104489   40.528848  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "SP_2852     34.969862          -8.01   88.001112   47.066173   33.026037  \n",
       "SP_2853     50.512045         -15.99  121.218778   58.255568   37.036541  \n",
       "SP_2854     51.704602         -13.86  121.347700   60.104035   35.961362  \n",
       "SP_2855     50.244738         -13.69  125.600865   65.022901   43.867276  \n",
       "SP_Q09022  172.867436         -40.18  448.837609  240.573549  174.977679  \n",
       "\n",
       "[2566 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1283</th>\n",
       "      <th>1284</th>\n",
       "      <th>1285</th>\n",
       "      <th>1286</th>\n",
       "      <th>1287</th>\n",
       "      <th>1288</th>\n",
       "      <th>1289</th>\n",
       "      <th>1290</th>\n",
       "      <th>1291</th>\n",
       "      <th>1292</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085722</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>-0.445218</td>\n",
       "      <td>-0.699969</td>\n",
       "      <td>-0.287690</td>\n",
       "      <td>0.483651</td>\n",
       "      <td>-0.154992</td>\n",
       "      <td>0.125977</td>\n",
       "      <td>-0.359323</td>\n",
       "      <td>-0.595465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.160320</td>\n",
       "      <td>-0.102206</td>\n",
       "      <td>-0.571968</td>\n",
       "      <td>-0.834574</td>\n",
       "      <td>-0.226938</td>\n",
       "      <td>0.269230</td>\n",
       "      <td>-0.105080</td>\n",
       "      <td>0.213770</td>\n",
       "      <td>-0.190787</td>\n",
       "      <td>-0.429286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207036</td>\n",
       "      <td>-0.127410</td>\n",
       "      <td>-0.584202</td>\n",
       "      <td>-0.859884</td>\n",
       "      <td>-0.232554</td>\n",
       "      <td>0.348927</td>\n",
       "      <td>-0.120462</td>\n",
       "      <td>0.156949</td>\n",
       "      <td>-0.223212</td>\n",
       "      <td>-0.437118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207918</td>\n",
       "      <td>-0.079614</td>\n",
       "      <td>-0.620285</td>\n",
       "      <td>-0.842502</td>\n",
       "      <td>-0.245935</td>\n",
       "      <td>0.361073</td>\n",
       "      <td>-0.104621</td>\n",
       "      <td>0.177529</td>\n",
       "      <td>-0.227643</td>\n",
       "      <td>-0.500198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123724</td>\n",
       "      <td>-0.004068</td>\n",
       "      <td>-0.421231</td>\n",
       "      <td>-0.905763</td>\n",
       "      <td>-0.590226</td>\n",
       "      <td>0.281780</td>\n",
       "      <td>-0.018657</td>\n",
       "      <td>0.265048</td>\n",
       "      <td>-0.290352</td>\n",
       "      <td>-0.314621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004705</td>\n",
       "      <td>-0.098891</td>\n",
       "      <td>-0.298940</td>\n",
       "      <td>-0.354114</td>\n",
       "      <td>-0.155395</td>\n",
       "      <td>0.128088</td>\n",
       "      <td>0.073450</td>\n",
       "      <td>0.155887</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.105002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066372</td>\n",
       "      <td>0.137381</td>\n",
       "      <td>-0.323248</td>\n",
       "      <td>-0.378942</td>\n",
       "      <td>0.036451</td>\n",
       "      <td>0.281903</td>\n",
       "      <td>0.206052</td>\n",
       "      <td>0.233403</td>\n",
       "      <td>-0.043506</td>\n",
       "      <td>-0.444345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168312</td>\n",
       "      <td>-0.047159</td>\n",
       "      <td>-0.257725</td>\n",
       "      <td>-0.333802</td>\n",
       "      <td>-0.282807</td>\n",
       "      <td>0.091171</td>\n",
       "      <td>-0.050740</td>\n",
       "      <td>0.128265</td>\n",
       "      <td>0.042231</td>\n",
       "      <td>-0.126539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026083</td>\n",
       "      <td>0.054826</td>\n",
       "      <td>-0.212293</td>\n",
       "      <td>-0.043077</td>\n",
       "      <td>-0.235688</td>\n",
       "      <td>0.248765</td>\n",
       "      <td>0.039511</td>\n",
       "      <td>0.237762</td>\n",
       "      <td>-0.383695</td>\n",
       "      <td>-0.413327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154212</td>\n",
       "      <td>0.128410</td>\n",
       "      <td>-0.357160</td>\n",
       "      <td>-0.803064</td>\n",
       "      <td>-0.121338</td>\n",
       "      <td>0.357357</td>\n",
       "      <td>-0.036146</td>\n",
       "      <td>0.099633</td>\n",
       "      <td>-0.561975</td>\n",
       "      <td>-0.330682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 1293 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2          3         4         5         6     \\\n",
       "0     3181.6835   9.701025  0.030303   7.272727  0.196970  2.762539  3181.736   \n",
       "1     2279.6794   9.703153  0.043478 -12.986957  0.630435  1.845400  2279.719   \n",
       "2     2293.7060   9.703153  0.043478  -4.613043  0.630435  1.847089  2293.746   \n",
       "3     2266.6807   9.703153  0.043478  -4.613043  0.747826  1.847089  2266.720   \n",
       "4     1583.9103   8.750052  0.000000  10.800000  1.275000  0.759103  1583.937   \n",
       "...         ...        ...       ...        ...       ...       ...       ...   \n",
       "2561  1405.7470  10.002350  0.076923  16.907692  1.000000  1.731990  1405.774   \n",
       "2562  2124.3526   4.651158  0.166667  37.416667 -0.455556 -2.147981  2124.388   \n",
       "2563  2138.4670   8.045284  0.150000  72.685000 -0.230000  0.739412  2138.506   \n",
       "2564  2076.4408   9.820679  0.117647  50.894118 -0.911765  2.732905  2076.477   \n",
       "2565  7235.6096   8.884082  0.015152  44.725758 -0.174242  4.685244  7235.743   \n",
       "\n",
       "          7        8      9     ...      1283      1284      1285      1286  \\\n",
       "0    -13.16030  1318.79   46.0  ... -0.085722  0.090498 -0.445218 -0.699969   \n",
       "1     -7.85050   910.88   31.0  ... -0.160320 -0.102206 -0.571968 -0.834574   \n",
       "2     -7.46040   910.88   31.0  ... -0.207036 -0.127410 -0.584202 -0.859884   \n",
       "3     -7.34350   888.02   31.0  ... -0.207918 -0.079614 -0.620285 -0.842502   \n",
       "4     -5.33460   629.62   22.0  ... -0.123724 -0.004068 -0.421231 -0.905763   \n",
       "...        ...      ...    ...  ...       ...       ...       ...       ...   \n",
       "2561  -2.94950   527.88   18.0  ... -0.004705 -0.098891 -0.298940 -0.354114   \n",
       "2562  -2.70150   801.94   26.0  ...  0.066372  0.137381 -0.323248 -0.378942   \n",
       "2563  -6.72983   758.18   26.0  ...  0.168312 -0.047159 -0.257725 -0.333802   \n",
       "2564  -4.87636   848.49   30.0  ...  0.026083  0.054826 -0.212293 -0.043077   \n",
       "2565 -36.35396  2988.65  111.0  ...  0.154212  0.128410 -0.357160 -0.803064   \n",
       "\n",
       "          1287      1288      1289      1290      1291      1292  \n",
       "0    -0.287690  0.483651 -0.154992  0.125977 -0.359323 -0.595465  \n",
       "1    -0.226938  0.269230 -0.105080  0.213770 -0.190787 -0.429286  \n",
       "2    -0.232554  0.348927 -0.120462  0.156949 -0.223212 -0.437118  \n",
       "3    -0.245935  0.361073 -0.104621  0.177529 -0.227643 -0.500198  \n",
       "4    -0.590226  0.281780 -0.018657  0.265048 -0.290352 -0.314621  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2561 -0.155395  0.128088  0.073450  0.155887 -0.015761 -0.105002  \n",
       "2562  0.036451  0.281903  0.206052  0.233403 -0.043506 -0.444345  \n",
       "2563 -0.282807  0.091171 -0.050740  0.128265  0.042231 -0.126539  \n",
       "2564 -0.235688  0.248765  0.039511  0.237762 -0.383695 -0.413327  \n",
       "2565 -0.121338  0.357357 -0.036146  0.099633 -0.561975 -0.330682  \n",
       "\n",
       "[2566 rows x 1293 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2566, 19) (2566, 506) (2566, 50) (2566, 768) (2566, 1293)\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7745 - accuracy: 0.6045 - val_loss: 0.6155 - val_accuracy: 0.6667\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6866 - accuracy: 0.6283 - val_loss: 0.6173 - val_accuracy: 0.6156\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6660 - accuracy: 0.6167 - val_loss: 0.6216 - val_accuracy: 0.6156\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.6487 - accuracy: 0.6350 - val_loss: 0.6195 - val_accuracy: 0.6253\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6399 - accuracy: 0.6252 - val_loss: 0.6118 - val_accuracy: 0.6156\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6325 - accuracy: 0.6356 - val_loss: 0.6016 - val_accuracy: 0.6399\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.5955 - accuracy: 0.6661 - val_loss: 0.5921 - val_accuracy: 0.6667\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6022 - accuracy: 0.6539 - val_loss: 0.5849 - val_accuracy: 0.6813\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5880 - accuracy: 0.6636 - val_loss: 0.5792 - val_accuracy: 0.6886\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5623 - accuracy: 0.6819 - val_loss: 0.5748 - val_accuracy: 0.6837\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.5604 - accuracy: 0.7002 - val_loss: 0.5703 - val_accuracy: 0.6983\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.5562 - accuracy: 0.7008 - val_loss: 0.5656 - val_accuracy: 0.6959\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5492 - accuracy: 0.6953 - val_loss: 0.5606 - val_accuracy: 0.6983\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5446 - accuracy: 0.6996 - val_loss: 0.5565 - val_accuracy: 0.7032\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.5320 - accuracy: 0.7044 - val_loss: 0.5529 - val_accuracy: 0.6983\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5184 - accuracy: 0.7343 - val_loss: 0.5499 - val_accuracy: 0.7080\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5293 - accuracy: 0.7142 - val_loss: 0.5479 - val_accuracy: 0.7007\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.5181 - accuracy: 0.7264 - val_loss: 0.5464 - val_accuracy: 0.7105\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5197 - accuracy: 0.7307 - val_loss: 0.5462 - val_accuracy: 0.7129\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.5031 - accuracy: 0.7209 - val_loss: 0.5461 - val_accuracy: 0.7129\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4882 - accuracy: 0.7465 - val_loss: 0.5466 - val_accuracy: 0.7032\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4784 - accuracy: 0.7508 - val_loss: 0.5468 - val_accuracy: 0.7105\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4797 - accuracy: 0.7477 - val_loss: 0.5460 - val_accuracy: 0.7080\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4741 - accuracy: 0.7502 - val_loss: 0.5454 - val_accuracy: 0.7032\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4684 - accuracy: 0.7581 - val_loss: 0.5451 - val_accuracy: 0.7007\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4550 - accuracy: 0.7727 - val_loss: 0.5448 - val_accuracy: 0.6934\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4661 - accuracy: 0.7538 - val_loss: 0.5450 - val_accuracy: 0.6959\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.4495 - accuracy: 0.7715 - val_loss: 0.5459 - val_accuracy: 0.6934\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.4441 - accuracy: 0.7788 - val_loss: 0.5472 - val_accuracy: 0.6934\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.4377 - accuracy: 0.7837 - val_loss: 0.5487 - val_accuracy: 0.6959\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.4358 - accuracy: 0.7879 - val_loss: 0.5504 - val_accuracy: 0.6886\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4317 - accuracy: 0.7916 - val_loss: 0.5521 - val_accuracy: 0.6934\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.4210 - accuracy: 0.7971 - val_loss: 0.5532 - val_accuracy: 0.6934\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3988 - accuracy: 0.8087 - val_loss: 0.5553 - val_accuracy: 0.6983\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3962 - accuracy: 0.8001 - val_loss: 0.5582 - val_accuracy: 0.7032\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3884 - accuracy: 0.8172 - val_loss: 0.5612 - val_accuracy: 0.7105\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.4062 - accuracy: 0.8080 - val_loss: 0.5639 - val_accuracy: 0.7178\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3888 - accuracy: 0.8147 - val_loss: 0.5671 - val_accuracy: 0.7178\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.3735 - accuracy: 0.8245 - val_loss: 0.5703 - val_accuracy: 0.7202\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.3763 - accuracy: 0.8312 - val_loss: 0.5740 - val_accuracy: 0.7153\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.3633 - accuracy: 0.8233 - val_loss: 0.5785 - val_accuracy: 0.7153\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.3596 - accuracy: 0.8379 - val_loss: 0.5831 - val_accuracy: 0.7056\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3442 - accuracy: 0.8434 - val_loss: 0.5876 - val_accuracy: 0.7153\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3382 - accuracy: 0.8519 - val_loss: 0.5924 - val_accuracy: 0.7202\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.3466 - accuracy: 0.8379 - val_loss: 0.5978 - val_accuracy: 0.7129\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.3430 - accuracy: 0.8245 - val_loss: 0.6034 - val_accuracy: 0.7129\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.3319 - accuracy: 0.8446 - val_loss: 0.6082 - val_accuracy: 0.7129\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.3248 - accuracy: 0.8550 - val_loss: 0.6118 - val_accuracy: 0.7129\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3166 - accuracy: 0.8611 - val_loss: 0.6154 - val_accuracy: 0.7129\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.3253 - accuracy: 0.8531 - val_loss: 0.6193 - val_accuracy: 0.7153\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.3180 - accuracy: 0.8568 - val_loss: 0.6242 - val_accuracy: 0.7202\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2970 - accuracy: 0.8605 - val_loss: 0.6296 - val_accuracy: 0.7226\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2849 - accuracy: 0.8781 - val_loss: 0.6357 - val_accuracy: 0.7226\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.2987 - accuracy: 0.8544 - val_loss: 0.6422 - val_accuracy: 0.7226\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2863 - accuracy: 0.8787 - val_loss: 0.6488 - val_accuracy: 0.7251\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2877 - accuracy: 0.8726 - val_loss: 0.6564 - val_accuracy: 0.7275\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2787 - accuracy: 0.8860 - val_loss: 0.6638 - val_accuracy: 0.7299\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2742 - accuracy: 0.8891 - val_loss: 0.6706 - val_accuracy: 0.7348\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2593 - accuracy: 0.8897 - val_loss: 0.6780 - val_accuracy: 0.7324\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.2647 - accuracy: 0.8903 - val_loss: 0.6869 - val_accuracy: 0.7324\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2586 - accuracy: 0.8867 - val_loss: 0.6965 - val_accuracy: 0.7324\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2443 - accuracy: 0.9019 - val_loss: 0.7059 - val_accuracy: 0.7299\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.2528 - accuracy: 0.8873 - val_loss: 0.7159 - val_accuracy: 0.7275\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2348 - accuracy: 0.8976 - val_loss: 0.7250 - val_accuracy: 0.7275\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2555 - accuracy: 0.9007 - val_loss: 0.7328 - val_accuracy: 0.7251\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2262 - accuracy: 0.9098 - val_loss: 0.7403 - val_accuracy: 0.7251\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.2281 - accuracy: 0.9062 - val_loss: 0.7493 - val_accuracy: 0.7275\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.2292 - accuracy: 0.9037 - val_loss: 0.7594 - val_accuracy: 0.7299\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2197 - accuracy: 0.9080 - val_loss: 0.7705 - val_accuracy: 0.7178\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2101 - accuracy: 0.9104 - val_loss: 0.7819 - val_accuracy: 0.7275\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.2040 - accuracy: 0.9190 - val_loss: 0.7930 - val_accuracy: 0.7299\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2049 - accuracy: 0.9183 - val_loss: 0.8036 - val_accuracy: 0.7299\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2014 - accuracy: 0.9226 - val_loss: 0.8128 - val_accuracy: 0.7324\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.2058 - accuracy: 0.9183 - val_loss: 0.8225 - val_accuracy: 0.7348\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1974 - accuracy: 0.9171 - val_loss: 0.8344 - val_accuracy: 0.7275\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2007 - accuracy: 0.9153 - val_loss: 0.8495 - val_accuracy: 0.7324\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2026 - accuracy: 0.9196 - val_loss: 0.8628 - val_accuracy: 0.7324\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1865 - accuracy: 0.9238 - val_loss: 0.8736 - val_accuracy: 0.7348\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1856 - accuracy: 0.9257 - val_loss: 0.8801 - val_accuracy: 0.7445\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1726 - accuracy: 0.9330 - val_loss: 0.8867 - val_accuracy: 0.7470\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1770 - accuracy: 0.9311 - val_loss: 0.8936 - val_accuracy: 0.7470\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1793 - accuracy: 0.9232 - val_loss: 0.9051 - val_accuracy: 0.7421\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.1659 - accuracy: 0.9360 - val_loss: 0.9194 - val_accuracy: 0.7470\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1658 - accuracy: 0.9354 - val_loss: 0.9351 - val_accuracy: 0.7445\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1663 - accuracy: 0.9372 - val_loss: 0.9508 - val_accuracy: 0.7518\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1561 - accuracy: 0.9324 - val_loss: 0.9645 - val_accuracy: 0.7421\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1594 - accuracy: 0.9311 - val_loss: 0.9811 - val_accuracy: 0.7470\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1450 - accuracy: 0.9415 - val_loss: 0.9960 - val_accuracy: 0.7470\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1402 - accuracy: 0.9488 - val_loss: 1.0109 - val_accuracy: 0.7445\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1407 - accuracy: 0.9500 - val_loss: 1.0240 - val_accuracy: 0.7445\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.1508 - accuracy: 0.9458 - val_loss: 1.0366 - val_accuracy: 0.7494\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1367 - accuracy: 0.9476 - val_loss: 1.0497 - val_accuracy: 0.7494\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1385 - accuracy: 0.9439 - val_loss: 1.0682 - val_accuracy: 0.7445\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1339 - accuracy: 0.9537 - val_loss: 1.0884 - val_accuracy: 0.7470\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1445 - accuracy: 0.9415 - val_loss: 1.1055 - val_accuracy: 0.7470\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1284 - accuracy: 0.9537 - val_loss: 1.1213 - val_accuracy: 0.7445\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1202 - accuracy: 0.9500 - val_loss: 1.1389 - val_accuracy: 0.7494\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1255 - accuracy: 0.9519 - val_loss: 1.1583 - val_accuracy: 0.7494\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1356 - accuracy: 0.9488 - val_loss: 1.1744 - val_accuracy: 0.7494\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.1184 - accuracy: 0.9525 - val_loss: 1.1898 - val_accuracy: 0.7518\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1078 - accuracy: 0.9561 - val_loss: 1.2069 - val_accuracy: 0.7518\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1179 - accuracy: 0.9537 - val_loss: 1.2250 - val_accuracy: 0.7543\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1051 - accuracy: 0.9616 - val_loss: 1.2432 - val_accuracy: 0.7567\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.1054 - accuracy: 0.9604 - val_loss: 1.2626 - val_accuracy: 0.7591\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0939 - accuracy: 0.9622 - val_loss: 1.2835 - val_accuracy: 0.7591\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1240 - accuracy: 0.9567 - val_loss: 1.3058 - val_accuracy: 0.7616\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.1086 - accuracy: 0.9586 - val_loss: 1.3270 - val_accuracy: 0.7494\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0982 - accuracy: 0.9628 - val_loss: 1.3488 - val_accuracy: 0.7518\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0974 - accuracy: 0.9604 - val_loss: 1.3683 - val_accuracy: 0.7518\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1011 - accuracy: 0.9634 - val_loss: 1.3866 - val_accuracy: 0.7470\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1019 - accuracy: 0.9622 - val_loss: 1.4055 - val_accuracy: 0.7470\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0948 - accuracy: 0.9610 - val_loss: 1.4265 - val_accuracy: 0.7445\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0996 - accuracy: 0.9604 - val_loss: 1.4483 - val_accuracy: 0.7470\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0908 - accuracy: 0.9628 - val_loss: 1.4728 - val_accuracy: 0.7445\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0886 - accuracy: 0.9707 - val_loss: 1.5025 - val_accuracy: 0.7421\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.1048 - accuracy: 0.9604 - val_loss: 1.5252 - val_accuracy: 0.7397\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0818 - accuracy: 0.9720 - val_loss: 1.5456 - val_accuracy: 0.7397\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.1046 - accuracy: 0.9671 - val_loss: 1.5622 - val_accuracy: 0.7421\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0942 - accuracy: 0.9537 - val_loss: 1.5784 - val_accuracy: 0.7397\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0760 - accuracy: 0.9720 - val_loss: 1.5921 - val_accuracy: 0.7470\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0859 - accuracy: 0.9665 - val_loss: 1.6107 - val_accuracy: 0.7445\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0733 - accuracy: 0.9732 - val_loss: 1.6235 - val_accuracy: 0.7445\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0760 - accuracy: 0.9720 - val_loss: 1.6450 - val_accuracy: 0.7518\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0802 - accuracy: 0.9683 - val_loss: 1.6708 - val_accuracy: 0.7445\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0825 - accuracy: 0.9671 - val_loss: 1.6985 - val_accuracy: 0.7445\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0732 - accuracy: 0.9756 - val_loss: 1.7272 - val_accuracy: 0.7445\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0846 - accuracy: 0.9689 - val_loss: 1.7529 - val_accuracy: 0.7470\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0907 - accuracy: 0.9677 - val_loss: 1.7729 - val_accuracy: 0.7397\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0950 - accuracy: 0.9604 - val_loss: 1.7870 - val_accuracy: 0.7372\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.0717 - accuracy: 0.9762 - val_loss: 1.8035 - val_accuracy: 0.7445\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0725 - accuracy: 0.9689 - val_loss: 1.8227 - val_accuracy: 0.7494\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0719 - accuracy: 0.9744 - val_loss: 1.8440 - val_accuracy: 0.7470\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0812 - accuracy: 0.9689 - val_loss: 1.8625 - val_accuracy: 0.7470\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0789 - accuracy: 0.9701 - val_loss: 1.8794 - val_accuracy: 0.7421\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0740 - accuracy: 0.9714 - val_loss: 1.8942 - val_accuracy: 0.7421\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0854 - accuracy: 0.9665 - val_loss: 1.9049 - val_accuracy: 0.7494\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0746 - accuracy: 0.9744 - val_loss: 1.9130 - val_accuracy: 0.7494\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0759 - accuracy: 0.9714 - val_loss: 1.9242 - val_accuracy: 0.7543\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0668 - accuracy: 0.9775 - val_loss: 1.9356 - val_accuracy: 0.7518\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0778 - accuracy: 0.9738 - val_loss: 1.9484 - val_accuracy: 0.7567\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0698 - accuracy: 0.9762 - val_loss: 1.9613 - val_accuracy: 0.7591\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0698 - accuracy: 0.9701 - val_loss: 1.9689 - val_accuracy: 0.7591\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0590 - accuracy: 0.9781 - val_loss: 1.9780 - val_accuracy: 0.7567\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0722 - accuracy: 0.9726 - val_loss: 1.9867 - val_accuracy: 0.7616\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0556 - accuracy: 0.9793 - val_loss: 2.0005 - val_accuracy: 0.7591\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0712 - accuracy: 0.9720 - val_loss: 2.0169 - val_accuracy: 0.7567\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0612 - accuracy: 0.9775 - val_loss: 2.0319 - val_accuracy: 0.7567\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0638 - accuracy: 0.9695 - val_loss: 2.0467 - val_accuracy: 0.7543\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0636 - accuracy: 0.9775 - val_loss: 2.0661 - val_accuracy: 0.7591\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0626 - accuracy: 0.9787 - val_loss: 2.0887 - val_accuracy: 0.7567\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0629 - accuracy: 0.9732 - val_loss: 2.1041 - val_accuracy: 0.7567\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0523 - accuracy: 0.9793 - val_loss: 2.1202 - val_accuracy: 0.7518\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0666 - accuracy: 0.9781 - val_loss: 2.1408 - val_accuracy: 0.7518\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0575 - accuracy: 0.9768 - val_loss: 2.1639 - val_accuracy: 0.7494\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0576 - accuracy: 0.9756 - val_loss: 2.1821 - val_accuracy: 0.7543\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0556 - accuracy: 0.9793 - val_loss: 2.1982 - val_accuracy: 0.7567\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0583 - accuracy: 0.9744 - val_loss: 2.2119 - val_accuracy: 0.7567\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0601 - accuracy: 0.9793 - val_loss: 2.2311 - val_accuracy: 0.7591\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 2.2559 - val_accuracy: 0.7518\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0519 - accuracy: 0.9775 - val_loss: 2.2784 - val_accuracy: 0.7494\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0674 - accuracy: 0.9707 - val_loss: 2.2901 - val_accuracy: 0.7518\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0545 - accuracy: 0.9781 - val_loss: 2.3016 - val_accuracy: 0.7445\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0517 - accuracy: 0.9799 - val_loss: 2.3187 - val_accuracy: 0.7494\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0519 - accuracy: 0.9805 - val_loss: 2.3376 - val_accuracy: 0.7518\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0523 - accuracy: 0.9775 - val_loss: 2.3570 - val_accuracy: 0.7567\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0468 - accuracy: 0.9848 - val_loss: 2.3749 - val_accuracy: 0.7591\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0583 - accuracy: 0.9799 - val_loss: 2.3864 - val_accuracy: 0.7543\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0520 - accuracy: 0.9811 - val_loss: 2.3990 - val_accuracy: 0.7543\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0517 - accuracy: 0.9811 - val_loss: 2.4070 - val_accuracy: 0.7567\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0416 - accuracy: 0.9854 - val_loss: 2.4132 - val_accuracy: 0.7591\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0407 - accuracy: 0.9866 - val_loss: 2.4226 - val_accuracy: 0.7591\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0508 - accuracy: 0.9805 - val_loss: 2.4368 - val_accuracy: 0.7616\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0472 - accuracy: 0.9811 - val_loss: 2.4564 - val_accuracy: 0.7591\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0578 - accuracy: 0.9738 - val_loss: 2.4744 - val_accuracy: 0.7591\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0503 - accuracy: 0.9835 - val_loss: 2.4925 - val_accuracy: 0.7543\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0574 - accuracy: 0.9793 - val_loss: 2.5158 - val_accuracy: 0.7543\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0543 - accuracy: 0.9762 - val_loss: 2.5391 - val_accuracy: 0.7518\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0444 - accuracy: 0.9829 - val_loss: 2.5633 - val_accuracy: 0.7494\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0666 - accuracy: 0.9744 - val_loss: 2.5876 - val_accuracy: 0.7518\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0441 - accuracy: 0.9866 - val_loss: 2.6126 - val_accuracy: 0.7445\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0497 - accuracy: 0.9811 - val_loss: 2.6351 - val_accuracy: 0.7470\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0424 - accuracy: 0.9860 - val_loss: 2.6531 - val_accuracy: 0.7470\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0590 - accuracy: 0.9775 - val_loss: 2.6649 - val_accuracy: 0.7445\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0530 - accuracy: 0.9775 - val_loss: 2.6774 - val_accuracy: 0.7445\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0482 - accuracy: 0.9829 - val_loss: 2.6865 - val_accuracy: 0.7470\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0540 - accuracy: 0.9805 - val_loss: 2.6910 - val_accuracy: 0.7421\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0507 - accuracy: 0.9781 - val_loss: 2.6953 - val_accuracy: 0.7421\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0420 - accuracy: 0.9823 - val_loss: 2.7023 - val_accuracy: 0.7445\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0463 - accuracy: 0.9805 - val_loss: 2.7134 - val_accuracy: 0.7470\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0446 - accuracy: 0.9811 - val_loss: 2.7227 - val_accuracy: 0.7470\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0385 - accuracy: 0.9878 - val_loss: 2.7336 - val_accuracy: 0.7470\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0420 - accuracy: 0.9848 - val_loss: 2.7459 - val_accuracy: 0.7470\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0382 - accuracy: 0.9866 - val_loss: 2.7599 - val_accuracy: 0.7470\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0488 - accuracy: 0.9781 - val_loss: 2.7772 - val_accuracy: 0.7494\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0544 - accuracy: 0.9805 - val_loss: 2.7955 - val_accuracy: 0.7543\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0532 - accuracy: 0.9805 - val_loss: 2.8132 - val_accuracy: 0.7470\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0471 - accuracy: 0.9781 - val_loss: 2.8331 - val_accuracy: 0.7421\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0414 - accuracy: 0.9842 - val_loss: 2.8508 - val_accuracy: 0.7421\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0425 - accuracy: 0.9842 - val_loss: 2.8644 - val_accuracy: 0.7445\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0444 - accuracy: 0.9823 - val_loss: 2.8859 - val_accuracy: 0.7445\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "Performance Metrics for Method 7 (Prop&1hot&bert):\n",
      "       Metric     Value\n",
      "0   Accuracy  0.811284\n",
      "1  Precision  0.725146\n",
      "2     Recall  0.712644\n",
      "3   F1 Score  0.718841\n",
      "4    ROC AUC  0.853491\n",
      "\n",
      "Confusion Matrix for Method 7 (Prop&1hot&bert):\n",
      " [[293  47]\n",
      " [ 50 124]]\n",
      "Loading BioBERT model and tokenizer...\n",
      "Predicting for sequence: GLWSKIKEVGKEAAKAAAKAAG\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "\n",
      "Predictions using Combined technique:\n",
      "Non-antimicrobial\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load DataFrame\n",
    "display(allDataDF2)\n",
    "\n",
    "# Process data\n",
    "X1 = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1).values\n",
    "t = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1)\n",
    "display(t)\n",
    "y1 = allDataDF2['Value'].values\n",
    "\n",
    "# Reshape encoded columns\n",
    "X2 = np.array(list(allDataDF2['EncodedX'])).reshape((len(allDataDF2), -1))\n",
    "X4 = np.array(list(allDataDF2['Bert'])).reshape((len(allDataDF2), -1))\n",
    "\n",
    "# Concatenate arrays\n",
    "X = np.concatenate((X1, X2, X4), axis=1)\n",
    "x11 = pd.DataFrame(X)\n",
    "display(x11)\n",
    "\n",
    "# Split data\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, y1, test_size=0.2, random_state=0)\n",
    "\n",
    "# Display shapes to verify\n",
    "print(X1.shape, X2.shape, X3.shape, X4.shape, X.shape)\n",
    "\n",
    "# Scale data\n",
    "scaler2 = StandardScaler()\n",
    "X4_train = scaler2.fit_transform(X4_train)\n",
    "X4_test = scaler2.transform(X4_test)\n",
    "\n",
    "# Define neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train neural network model\n",
    "input_dim = X4_train.shape[1]\n",
    "nn_model = build_nn_model(input_dim)\n",
    "nn_model.fit(X4_train, y4_train, epochs=200, batch_size=2000, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = nn_model.predict(X4_test)\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Performance metrics dataframe\n",
    "performance_metrics_method7 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'],\n",
    "    'Value': [accuracy, precision, recall, f1, roc_auc]\n",
    "})\n",
    "\n",
    "# Print performance metrics and confusion matrix\n",
    "print(\"Performance Metrics for Method 7 (Prop&1hot&bert):\\n\", performance_metrics_method7)\n",
    "print(\"\\nConfusion Matrix for Method 7 (Prop&1hot&bert):\\n\", conf_matrix)\n",
    "\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embeddings1(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_method3(sequence, nn_model, scaler, model, tokenizer, w2v_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    properties = calculate_all_properties(sequence)\n",
    "   \n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    \n",
    "    \n",
    "    sequence1 = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence1]\n",
    "    onehot_encoded = np.ravel([[0 if i != val else 1 for i in range(len(char_to_int))] for val in integer_encoded])\n",
    "    \n",
    "    \n",
    "    embedding = get_bert_embeddings1(sequence, tokenizer, model)\n",
    "    \n",
    "    total = np.concatenate((  prop_values.flatten(), onehot_encoded,embedding.flatten()))\n",
    "    total = total.reshape(1, -1)\n",
    "    \n",
    "    prediction = nn_model.predict(total)\n",
    "    result = 'Antimicrobial' if prediction[0] >= 0.5 else 'Non-antimicrobial'\n",
    "    \n",
    "    return result, embedding\n",
    "\n",
    "# Prediction\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "predictions, embed = predict_method3(sequence, nn_model, scaler, bert_model, tokenizer, w2v_model)\n",
    "print(\"\\nPredictions using Combined technique:\")\n",
    "print(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2378716-e8b7-4aba-8ac1-4a48e9beca28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Add Method column\n",
    "performance_metrics_method1['Method'] = 'PhysicoChemical Properties'\n",
    "performance_metrics_method2['Method'] = 'One Hot Encoding'\n",
    "performance_metrics_method3['Method'] = 'Bert'\n",
    "performance_metrics_method4['Method'] = 'Word2Vec'\n",
    "performance_metrics_method5['Method'] = 'Combined'\n",
    "performance_metrics_method6['Method'] = 'Prop&1hot'\n",
    "performance_metrics_method7['Method'] = 'Prop&1hot&Bert'\n",
    "# Concatenate the metrics for graphing\n",
    "combined_metrics = pd.concat([performance_metrics_method1, performance_metrics_method2,performance_metrics_method2,performance_metrics_method3,performance_metrics_method4,performance_metrics_method5,performance_metrics_method6,performance_metrics_method7])\n",
    "\n",
    "# Plot the performance metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Value', hue='Method', data=combined_metrics)\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4863211-82ec-4881-af81-4b00815603d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MolecularWeight</th>\n",
       "      <th>IsoelectricPoint</th>\n",
       "      <th>Aromaticity</th>\n",
       "      <th>InstabilityIndex</th>\n",
       "      <th>Hydrophobicity</th>\n",
       "      <th>ChargeAtPH7</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>NumHDonors</th>\n",
       "      <th>...</th>\n",
       "      <th>Chi0v</th>\n",
       "      <th>Chi1v</th>\n",
       "      <th>HallKierAlpha</th>\n",
       "      <th>Kappa1</th>\n",
       "      <th>Kappa2</th>\n",
       "      <th>Kappa3</th>\n",
       "      <th>Value</th>\n",
       "      <th>EncodedX</th>\n",
       "      <th>Bert</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DADP ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SP_P31107</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>131.799954</td>\n",
       "      <td>75.452403</td>\n",
       "      <td>-20.68</td>\n",
       "      <td>199.359058</td>\n",
       "      <td>105.101422</td>\n",
       "      <td>75.403319</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.016400341, -0.4827105, 0.24925455, -0.010...</td>\n",
       "      <td>[-0.09701026, 0.023356073, 0.19239551, 0.34847...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2643</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>95.319148</td>\n",
       "      <td>54.422594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>142.034244</td>\n",
       "      <td>74.405793</td>\n",
       "      <td>54.137875</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[-0.023901049, -0.23354033, 0.19251004, 0.083...</td>\n",
       "      <td>[-0.10609158, 0.031600513, 0.15865144, 0.31917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2644</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>96.026255</td>\n",
       "      <td>54.922594</td>\n",
       "      <td>-15.02</td>\n",
       "      <td>143.033879</td>\n",
       "      <td>75.212983</td>\n",
       "      <td>54.431542</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.008616366, -0.19944721, 0.14948377, 0.1644...</td>\n",
       "      <td>[-0.107020475, 0.032668713, 0.16226107, 0.3235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2645</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>94.987870</td>\n",
       "      <td>54.392469</td>\n",
       "      <td>-14.53</td>\n",
       "      <td>141.524432</td>\n",
       "      <td>74.738555</td>\n",
       "      <td>53.425154</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.029937537, -0.24908826, 0.15552007, 0.1741...</td>\n",
       "      <td>[-0.1107386, 0.036594443, 0.15842809, 0.322235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2646</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>67.476541</td>\n",
       "      <td>37.978223</td>\n",
       "      <td>-9.21</td>\n",
       "      <td>101.790000</td>\n",
       "      <td>52.104489</td>\n",
       "      <td>40.528848</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...</td>\n",
       "      <td>[[-0.07822762, -0.19859661, 0.60224146, 0.1299...</td>\n",
       "      <td>[-0.10410814, 0.046210337, 0.17820424, 0.33955...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2852</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>59.496327</td>\n",
       "      <td>34.969862</td>\n",
       "      <td>-8.01</td>\n",
       "      <td>88.001112</td>\n",
       "      <td>47.066173</td>\n",
       "      <td>33.026037</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.19574007, -0.122425586, 0.08037657, -0.205...</td>\n",
       "      <td>[-0.11710616, 0.032537665, 0.117993765, 0.2792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2853</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.416638</td>\n",
       "      <td>50.512045</td>\n",
       "      <td>-15.99</td>\n",
       "      <td>121.218778</td>\n",
       "      <td>58.255568</td>\n",
       "      <td>37.036541</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.10914601, -0.52614176, 0.2915113, 0.102736...</td>\n",
       "      <td>[-0.1260696, -0.010238703, 0.0796683, 0.233125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2854</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>86.425115</td>\n",
       "      <td>51.704602</td>\n",
       "      <td>-13.86</td>\n",
       "      <td>121.347700</td>\n",
       "      <td>60.104035</td>\n",
       "      <td>35.961362</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.1672608, -0.2234647, 0.14960316, -0.197446...</td>\n",
       "      <td>[-0.16503033, -0.014033677, -0.032334927, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_2855</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>85.376209</td>\n",
       "      <td>50.244738</td>\n",
       "      <td>-13.69</td>\n",
       "      <td>125.600865</td>\n",
       "      <td>65.022901</td>\n",
       "      <td>43.867276</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.26691902, -0.6658726, 0.25356385, -0.15150...</td>\n",
       "      <td>[-0.145197, -0.018769035, 0.019576171, 0.18281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP_Q09022</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>294.537783</td>\n",
       "      <td>172.867436</td>\n",
       "      <td>-40.18</td>\n",
       "      <td>448.837609</td>\n",
       "      <td>240.573549</td>\n",
       "      <td>174.977679</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.046682104, -0.38446936, 0.120936714, 0.060...</td>\n",
       "      <td>[-0.109951705, 0.013725347, 0.11754644, 0.2610...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MolecularWeight  IsoelectricPoint  Aromaticity  InstabilityIndex  \\\n",
       "DADP ID                                                                       \n",
       "SP_P31107        3181.6835          9.701025     0.030303          7.272727   \n",
       "SP_2643          2279.6794          9.703153     0.043478        -12.986957   \n",
       "SP_2644          2293.7060          9.703153     0.043478         -4.613043   \n",
       "SP_2645          2266.6807          9.703153     0.043478         -4.613043   \n",
       "SP_2646          1583.9103          8.750052     0.000000         10.800000   \n",
       "...                    ...               ...          ...               ...   \n",
       "SP_2852          1405.7470         10.002350     0.076923         16.907692   \n",
       "SP_2853          2124.3526          4.651158     0.166667         37.416667   \n",
       "SP_2854          2138.4670          8.045284     0.150000         72.685000   \n",
       "SP_2855          2076.4408          9.820679     0.117647         50.894118   \n",
       "SP_Q09022        7235.6096          8.884082     0.015152         44.725758   \n",
       "\n",
       "           Hydrophobicity  ChargeAtPH7     MolWt      LogP     TPSA  \\\n",
       "DADP ID                                                               \n",
       "SP_P31107        0.196970     2.762539  3181.736 -13.16030  1318.79   \n",
       "SP_2643          0.630435     1.845400  2279.719  -7.85050   910.88   \n",
       "SP_2644          0.630435     1.847089  2293.746  -7.46040   910.88   \n",
       "SP_2645          0.747826     1.847089  2266.720  -7.34350   888.02   \n",
       "SP_2646          1.275000     0.759103  1583.937  -5.33460   629.62   \n",
       "...                   ...          ...       ...       ...      ...   \n",
       "SP_2852          1.000000     1.731990  1405.774  -2.94950   527.88   \n",
       "SP_2853         -0.455556    -2.147981  2124.388  -2.70150   801.94   \n",
       "SP_2854         -0.230000     0.739412  2138.506  -6.72983   758.18   \n",
       "SP_2855         -0.911765     2.732905  2076.477  -4.87636   848.49   \n",
       "SP_Q09022       -0.174242     4.685244  7235.743 -36.35396  2988.65   \n",
       "\n",
       "           NumHDonors  ...       Chi0v       Chi1v  HallKierAlpha      Kappa1  \\\n",
       "DADP ID                ...                                                      \n",
       "SP_P31107          46  ...  131.799954   75.452403         -20.68  199.359058   \n",
       "SP_2643            31  ...   95.319148   54.422594         -15.02  142.034244   \n",
       "SP_2644            31  ...   96.026255   54.922594         -15.02  143.033879   \n",
       "SP_2645            31  ...   94.987870   54.392469         -14.53  141.524432   \n",
       "SP_2646            22  ...   67.476541   37.978223          -9.21  101.790000   \n",
       "...               ...  ...         ...         ...            ...         ...   \n",
       "SP_2852            18  ...   59.496327   34.969862          -8.01   88.001112   \n",
       "SP_2853            26  ...   86.416638   50.512045         -15.99  121.218778   \n",
       "SP_2854            26  ...   86.425115   51.704602         -13.86  121.347700   \n",
       "SP_2855            30  ...   85.376209   50.244738         -13.69  125.600865   \n",
       "SP_Q09022         111  ...  294.537783  172.867436         -40.18  448.837609   \n",
       "\n",
       "               Kappa2      Kappa3  Value  \\\n",
       "DADP ID                                    \n",
       "SP_P31107  105.101422   75.403319      1   \n",
       "SP_2643     74.405793   54.137875      1   \n",
       "SP_2644     75.212983   54.431542      1   \n",
       "SP_2645     74.738555   53.425154      1   \n",
       "SP_2646     52.104489   40.528848      1   \n",
       "...               ...         ...    ...   \n",
       "SP_2852     47.066173   33.026037      0   \n",
       "SP_2853     58.255568   37.036541      0   \n",
       "SP_2854     60.104035   35.961362      0   \n",
       "SP_2855     65.022901   43.867276      0   \n",
       "SP_Q09022  240.573549  174.977679      0   \n",
       "\n",
       "                                                    EncodedX  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2643    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2644    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2645    [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2646    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0,...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2853    [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2854    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_2855    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "SP_Q09022  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                                        Bert  \\\n",
       "DADP ID                                                        \n",
       "SP_P31107  [[-0.016400341, -0.4827105, 0.24925455, -0.010...   \n",
       "SP_2643    [[-0.023901049, -0.23354033, 0.19251004, 0.083...   \n",
       "SP_2644    [[0.008616366, -0.19944721, 0.14948377, 0.1644...   \n",
       "SP_2645    [[0.029937537, -0.24908826, 0.15552007, 0.1741...   \n",
       "SP_2646    [[-0.07822762, -0.19859661, 0.60224146, 0.1299...   \n",
       "...                                                      ...   \n",
       "SP_2852    [[0.19574007, -0.122425586, 0.08037657, -0.205...   \n",
       "SP_2853    [[0.10914601, -0.52614176, 0.2915113, 0.102736...   \n",
       "SP_2854    [[0.1672608, -0.2234647, 0.14960316, -0.197446...   \n",
       "SP_2855    [[0.26691902, -0.6658726, 0.25356385, -0.15150...   \n",
       "SP_Q09022  [[0.046682104, -0.38446936, 0.120936714, 0.060...   \n",
       "\n",
       "                                                      vector  \n",
       "DADP ID                                                       \n",
       "SP_P31107  [-0.09701026, 0.023356073, 0.19239551, 0.34847...  \n",
       "SP_2643    [-0.10609158, 0.031600513, 0.15865144, 0.31917...  \n",
       "SP_2644    [-0.107020475, 0.032668713, 0.16226107, 0.3235...  \n",
       "SP_2645    [-0.1107386, 0.036594443, 0.15842809, 0.322235...  \n",
       "SP_2646    [-0.10410814, 0.046210337, 0.17820424, 0.33955...  \n",
       "...                                                      ...  \n",
       "SP_2852    [-0.11710616, 0.032537665, 0.117993765, 0.2792...  \n",
       "SP_2853    [-0.1260696, -0.010238703, 0.0796683, 0.233125...  \n",
       "SP_2854    [-0.16503033, -0.014033677, -0.032334927, 0.12...  \n",
       "SP_2855    [-0.145197, -0.018769035, 0.019576171, 0.18281...  \n",
       "SP_Q09022  [-0.109951705, 0.013725347, 0.11754644, 0.2610...  \n",
       "\n",
       "[2566 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1333</th>\n",
       "      <th>1334</th>\n",
       "      <th>1335</th>\n",
       "      <th>1336</th>\n",
       "      <th>1337</th>\n",
       "      <th>1338</th>\n",
       "      <th>1339</th>\n",
       "      <th>1340</th>\n",
       "      <th>1341</th>\n",
       "      <th>1342</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3181.6835</td>\n",
       "      <td>9.701025</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>2.762539</td>\n",
       "      <td>3181.736</td>\n",
       "      <td>-13.16030</td>\n",
       "      <td>1318.79</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060075</td>\n",
       "      <td>0.072157</td>\n",
       "      <td>-0.057001</td>\n",
       "      <td>-0.199033</td>\n",
       "      <td>0.213588</td>\n",
       "      <td>0.297404</td>\n",
       "      <td>-0.060959</td>\n",
       "      <td>-0.105691</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.212193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2279.6794</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-12.986957</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.845400</td>\n",
       "      <td>2279.719</td>\n",
       "      <td>-7.85050</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.069961</td>\n",
       "      <td>-0.063975</td>\n",
       "      <td>-0.183580</td>\n",
       "      <td>0.222905</td>\n",
       "      <td>0.289790</td>\n",
       "      <td>-0.058714</td>\n",
       "      <td>-0.099017</td>\n",
       "      <td>0.023472</td>\n",
       "      <td>0.197055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2293.7060</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2293.746</td>\n",
       "      <td>-7.46040</td>\n",
       "      <td>910.88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041615</td>\n",
       "      <td>0.071203</td>\n",
       "      <td>-0.062850</td>\n",
       "      <td>-0.183642</td>\n",
       "      <td>0.221408</td>\n",
       "      <td>0.290012</td>\n",
       "      <td>-0.060219</td>\n",
       "      <td>-0.097840</td>\n",
       "      <td>0.023686</td>\n",
       "      <td>0.200506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2266.6807</td>\n",
       "      <td>9.703153</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>-4.613043</td>\n",
       "      <td>0.747826</td>\n",
       "      <td>1.847089</td>\n",
       "      <td>2266.720</td>\n",
       "      <td>-7.34350</td>\n",
       "      <td>888.02</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.071274</td>\n",
       "      <td>-0.060835</td>\n",
       "      <td>-0.181834</td>\n",
       "      <td>0.219751</td>\n",
       "      <td>0.290568</td>\n",
       "      <td>-0.060691</td>\n",
       "      <td>-0.096536</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.202205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1583.9103</td>\n",
       "      <td>8.750052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>1.275000</td>\n",
       "      <td>0.759103</td>\n",
       "      <td>1583.937</td>\n",
       "      <td>-5.33460</td>\n",
       "      <td>629.62</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038598</td>\n",
       "      <td>0.083404</td>\n",
       "      <td>-0.051983</td>\n",
       "      <td>-0.184543</td>\n",
       "      <td>0.197916</td>\n",
       "      <td>0.295361</td>\n",
       "      <td>-0.080805</td>\n",
       "      <td>-0.106384</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>0.213447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>1405.7470</td>\n",
       "      <td>10.002350</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>16.907692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.731990</td>\n",
       "      <td>1405.774</td>\n",
       "      <td>-2.94950</td>\n",
       "      <td>527.88</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027876</td>\n",
       "      <td>0.059670</td>\n",
       "      <td>-0.081960</td>\n",
       "      <td>-0.166358</td>\n",
       "      <td>0.249094</td>\n",
       "      <td>0.268477</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.086254</td>\n",
       "      <td>0.035719</td>\n",
       "      <td>0.169574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>2124.3526</td>\n",
       "      <td>4.651158</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>37.416667</td>\n",
       "      <td>-0.455556</td>\n",
       "      <td>-2.147981</td>\n",
       "      <td>2124.388</td>\n",
       "      <td>-2.70150</td>\n",
       "      <td>801.94</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038159</td>\n",
       "      <td>0.017906</td>\n",
       "      <td>-0.160036</td>\n",
       "      <td>-0.169702</td>\n",
       "      <td>0.345982</td>\n",
       "      <td>0.219215</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>-0.026315</td>\n",
       "      <td>0.075662</td>\n",
       "      <td>0.143840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>2138.4670</td>\n",
       "      <td>8.045284</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>72.685000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>0.739412</td>\n",
       "      <td>2138.506</td>\n",
       "      <td>-6.72983</td>\n",
       "      <td>758.18</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011908</td>\n",
       "      <td>-0.030374</td>\n",
       "      <td>-0.224305</td>\n",
       "      <td>-0.136530</td>\n",
       "      <td>0.415614</td>\n",
       "      <td>0.163084</td>\n",
       "      <td>0.064353</td>\n",
       "      <td>0.024089</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>0.094428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>2076.4408</td>\n",
       "      <td>9.820679</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>50.894118</td>\n",
       "      <td>-0.911765</td>\n",
       "      <td>2.732905</td>\n",
       "      <td>2076.477</td>\n",
       "      <td>-4.87636</td>\n",
       "      <td>848.49</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029248</td>\n",
       "      <td>-0.010688</td>\n",
       "      <td>-0.193861</td>\n",
       "      <td>-0.153010</td>\n",
       "      <td>0.369639</td>\n",
       "      <td>0.194822</td>\n",
       "      <td>0.043160</td>\n",
       "      <td>-0.007631</td>\n",
       "      <td>0.083578</td>\n",
       "      <td>0.123238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>7235.6096</td>\n",
       "      <td>8.884082</td>\n",
       "      <td>0.015152</td>\n",
       "      <td>44.725758</td>\n",
       "      <td>-0.174242</td>\n",
       "      <td>4.685244</td>\n",
       "      <td>7235.743</td>\n",
       "      <td>-36.35396</td>\n",
       "      <td>2988.65</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040760</td>\n",
       "      <td>0.040252</td>\n",
       "      <td>-0.112429</td>\n",
       "      <td>-0.175255</td>\n",
       "      <td>0.286306</td>\n",
       "      <td>0.246535</td>\n",
       "      <td>-0.018752</td>\n",
       "      <td>-0.074633</td>\n",
       "      <td>0.040948</td>\n",
       "      <td>0.159354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2566 rows × 1343 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1         2          3         4         5         6     \\\n",
       "0     3181.6835   9.701025  0.030303   7.272727  0.196970  2.762539  3181.736   \n",
       "1     2279.6794   9.703153  0.043478 -12.986957  0.630435  1.845400  2279.719   \n",
       "2     2293.7060   9.703153  0.043478  -4.613043  0.630435  1.847089  2293.746   \n",
       "3     2266.6807   9.703153  0.043478  -4.613043  0.747826  1.847089  2266.720   \n",
       "4     1583.9103   8.750052  0.000000  10.800000  1.275000  0.759103  1583.937   \n",
       "...         ...        ...       ...        ...       ...       ...       ...   \n",
       "2561  1405.7470  10.002350  0.076923  16.907692  1.000000  1.731990  1405.774   \n",
       "2562  2124.3526   4.651158  0.166667  37.416667 -0.455556 -2.147981  2124.388   \n",
       "2563  2138.4670   8.045284  0.150000  72.685000 -0.230000  0.739412  2138.506   \n",
       "2564  2076.4408   9.820679  0.117647  50.894118 -0.911765  2.732905  2076.477   \n",
       "2565  7235.6096   8.884082  0.015152  44.725758 -0.174242  4.685244  7235.743   \n",
       "\n",
       "          7        8      9     ...      1333      1334      1335      1336  \\\n",
       "0    -13.16030  1318.79   46.0  ...  0.060075  0.072157 -0.057001 -0.199033   \n",
       "1     -7.85050   910.88   31.0  ...  0.040859  0.069961 -0.063975 -0.183580   \n",
       "2     -7.46040   910.88   31.0  ...  0.041615  0.071203 -0.062850 -0.183642   \n",
       "3     -7.34350   888.02   31.0  ...  0.038818  0.071274 -0.060835 -0.181834   \n",
       "4     -5.33460   629.62   22.0  ...  0.038598  0.083404 -0.051983 -0.184543   \n",
       "...        ...      ...    ...  ...       ...       ...       ...       ...   \n",
       "2561  -2.94950   527.88   18.0  ...  0.027876  0.059670 -0.081960 -0.166358   \n",
       "2562  -2.70150   801.94   26.0  ...  0.038159  0.017906 -0.160036 -0.169702   \n",
       "2563  -6.72983   758.18   26.0  ...  0.011908 -0.030374 -0.224305 -0.136530   \n",
       "2564  -4.87636   848.49   30.0  ...  0.029248 -0.010688 -0.193861 -0.153010   \n",
       "2565 -36.35396  2988.65  111.0  ...  0.040760  0.040252 -0.112429 -0.175255   \n",
       "\n",
       "          1337      1338      1339      1340      1341      1342  \n",
       "0     0.213588  0.297404 -0.060959 -0.105691  0.021453  0.212193  \n",
       "1     0.222905  0.289790 -0.058714 -0.099017  0.023472  0.197055  \n",
       "2     0.221408  0.290012 -0.060219 -0.097840  0.023686  0.200506  \n",
       "3     0.219751  0.290568 -0.060691 -0.096536  0.022934  0.202205  \n",
       "4     0.197916  0.295361 -0.080805 -0.106384  0.008533  0.213447  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "2561  0.249094  0.268477 -0.037353 -0.086254  0.035719  0.169574  \n",
       "2562  0.345982  0.219215  0.007181 -0.026315  0.075662  0.143840  \n",
       "2563  0.415614  0.163084  0.064353  0.024089  0.091599  0.094428  \n",
       "2564  0.369639  0.194822  0.043160 -0.007631  0.083578  0.123238  \n",
       "2565  0.286306  0.246535 -0.018752 -0.074633  0.040948  0.159354  \n",
       "\n",
       "[2566 rows x 1343 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step\n",
      "17/17 [==============================] - 0s 1ms/step\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "17/17 [==============================] - 0s 2ms/step\n",
      "17/17 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "# Load DataFrame\n",
    "display(allDataDF2)\n",
    "\n",
    "# Process data\n",
    "X1 = allDataDF2.drop(columns=['Value', 'EncodedX', 'vector', 'Bert'], axis=1).values\n",
    "y1 = allDataDF2['Value'].values\n",
    "\n",
    "# Reshape encoded columns\n",
    "X2 = np.array(list(allDataDF2['EncodedX'])).reshape((len(allDataDF2), -1))\n",
    "X3 = np.array(list(allDataDF2['vector'])).reshape((len(allDataDF2),-1))\n",
    "X4 = np.array(list(allDataDF2['Bert'])).reshape((len(allDataDF2), -1))\n",
    "\n",
    "\n",
    "\n",
    "X = np.concatenate((X1, X2, X4, X3), axis=1)\n",
    "display(pd.DataFrame(X))\n",
    "# Define neural network model\n",
    "def build_nn_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# K-fold Cross-Validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y1[train_index], y1[test_index]\n",
    "\n",
    "    # Scale data\n",
    "    scaler1 = StandardScaler()\n",
    "    X_train = scaler1.fit_transform(X_train)\n",
    "    X_test = scaler1.transform(X_test)\n",
    "\n",
    "    # Build and train the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    nn_model = build_nn_model(input_dim)\n",
    "    nn_model.fit(X_train, y_train, epochs=500, batch_size=2000, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Evaluate the model on validation set\n",
    "    y_pred_prob = nn_model.predict(X_test)\n",
    "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "    roc_auc_list.append(roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057a3426-57ad-45d3-981b-26dd96d25a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.7661789579872725\n",
      "Average Precision: 0.6413766135043149\n",
      "Average Recall: 0.7931050965840882\n",
      "Average F1 Score: 0.708372118439289\n",
      "Average ROC AUC: 0.8100786391764251\n",
      "Loading BioBERT model and tokenizer...\n",
      "Predicting for sequence: GLWSKIKEVGKEAAKAAAKAAG\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "\n",
      "Predictions using Combined technique:\n",
      "Antimicrobial\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores across all folds\n",
    "avg_accuracy = np.mean(accuracy_list)\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_f1 = np.mean(f1_list)\n",
    "avg_roc_auc = np.mean(roc_auc_list)\n",
    "\n",
    "# Display average performance metrics\n",
    "print(f'Average Accuracy: {avg_accuracy}')\n",
    "print(f'Average Precision: {avg_precision}')\n",
    "print(f'Average Recall: {avg_recall}')\n",
    "print(f'Average F1 Score: {avg_f1}')\n",
    "print(f'Average ROC AUC: {avg_roc_auc}')\n",
    "\n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "pos_sequences = posDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "neg_sequences = negDF['Bioactive sequence'].str.upper().apply(list).tolist()\n",
    "all_sequences = pos_sequences + neg_sequences\n",
    "w2v_model = Word2Vec(sentences=all_sequences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Generate sequence vectors\n",
    "def sequence_vector(seq, model):\n",
    "    return np.mean([model.wv[char] for char in seq if char in model.wv], axis=0)\n",
    "\n",
    "# Load BERT model and tokenizer\n",
    "print(\"Loading BioBERT model and tokenizer...\")\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embeddings1(sequence, tokenizer, model):\n",
    "    inputs = tokenizer(sequence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Function to predict antimicrobial property using deep learning model\n",
    "def predict_method3(sequence, nn_model, scaler, model, tokenizer, w2v_model):\n",
    "    print(f\"Predicting for sequence: {sequence}\")\n",
    "    properties = calculate_all_properties(sequence)\n",
    "   \n",
    "    prop_values = scaler.transform([list(properties.values())])\n",
    "    \n",
    "    sequence1 = sequence.upper().ljust(22, '#')[:22]\n",
    "    integer_encoded = [char_to_int[char] for char in sequence1]\n",
    "    onehot_encoded = np.ravel([[0 if i != val else 1 for i in range(len(char_to_int))] for val in integer_encoded])\n",
    "    \n",
    "    embedding = get_bert_embeddings1(sequence, tokenizer, model)\n",
    "    \n",
    "    sequence = sequence.upper()\n",
    "    vector = sequence_vector(list(sequence), w2v_model)\n",
    "    \n",
    "    total = np.concatenate((vector, embedding.flatten(), prop_values.flatten(), onehot_encoded))\n",
    "    total = total.reshape(1, -1)\n",
    "    \n",
    "    prediction = nn_model.predict(total)\n",
    "    result = 'Antimicrobial' if prediction[0] >= 0.5 else 'Non-antimicrobial'\n",
    "    \n",
    "    return result, embedding\n",
    "\n",
    "# Prediction\n",
    "sequence = 'GLWSKIKEVGKEAAKAAAKAAG'\n",
    "predictions, embed = predict_method3(sequence, nn_model, scaler, bert_model, tokenizer, w2v_model)\n",
    "print(\"\\nPredictions using Combined technique:\")\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e77e64-9b68-4a79-9ae2-95dccff8c5de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
